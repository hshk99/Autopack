"""GLM (Zhipu AI) Builder and Auditor implementations

GLM uses OpenAI-compatible API format, so we use the OpenAI SDK
but configured with GLM-specific credentials and base URL.

Environment variables:
- GLM_API_KEY: API key for Zhipu AI GLM
- GLM_API_BASE: Base URL for GLM API (defaults to https://open.bigmodel.cn/api/paas/v4)
"""

import json
import logging
import os
from typing import Dict, List, Optional

from openai import OpenAI

from .llm_client import AuditorResult, BuilderResult

logger = logging.getLogger(__name__)

# Default GLM API base URL
DEFAULT_GLM_API_BASE = "https://open.bigmodel.cn/api/paas/v4"


def get_glm_client() -> Optional[OpenAI]:
    """Create an OpenAI client configured for GLM API.

    Returns:
        OpenAI client configured for GLM, or None if credentials not available
    """
    api_key = os.getenv("GLM_API_KEY")
    if not api_key:
        return None

    api_base = os.getenv("GLM_API_BASE", DEFAULT_GLM_API_BASE)

    return OpenAI(api_key=api_key, base_url=api_base)


class GLMBuilderClient:
    """Builder implementation using GLM (Zhipu AI) API

    Generates code patches from phase specifications.
    Uses GLM-4.5 for code generation via OpenAI-compatible API.
    """

    def __init__(self, api_key: Optional[str] = None, api_base: Optional[str] = None):
        """Initialize GLM client

        Args:
            api_key: GLM API key (defaults to GLM_API_KEY env var)
            api_base: GLM API base URL (defaults to GLM_API_BASE env var or default URL)
        """
        self.api_key = api_key or os.getenv("GLM_API_KEY")
        self.api_base = api_base or os.getenv("GLM_API_BASE", DEFAULT_GLM_API_BASE)

        if not self.api_key:
            raise ValueError("GLM_API_KEY environment variable is required for GLM client")

        self.client = OpenAI(api_key=self.api_key, base_url=self.api_base)

    def execute_phase(
        self,
        phase_spec: Dict,
        file_context: Optional[Dict] = None,
        max_tokens: Optional[int] = None,
        model: str = "glm-4.7",
        project_rules: Optional[List] = None,
        run_hints: Optional[List] = None,
    ) -> BuilderResult:
        """Execute a phase and generate code patch

        Args:
            phase_spec: Phase specification with fields:
                - phase_id: str
                - task_category: str
                - complexity: str
                - description: str
                - acceptance_criteria: List[str]
            file_context: Current repo files (optional, for context)
            max_tokens: Token budget limit for this call
            model: GLM model to use
            project_rules: Persistent project learned rules (Stage 0B)
            run_hints: Within-run hints from earlier phases (Stage 0A)

        Returns:
            BuilderResult with patch_content and metadata
        """
        try:
            # Build system prompt for Builder
            system_prompt = self._build_system_prompt()

            # Build user prompt with phase details
            user_prompt = self._build_user_prompt(
                phase_spec, file_context, project_rules, run_hints
            )

            # Call GLM API - NO JSON mode (raw diff output)
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                max_tokens=max_tokens or 128000,
                temperature=0.2,
            )

            # Extract content
            content = response.choices[0].message.content

            # Extract tokens used
            tokens_used = response.usage.total_tokens if response.usage else 0

            # Extract patch from raw text
            patch_content = self._extract_diff_from_text(content)

            if not patch_content:
                error_msg = "LLM output invalid format - no git diff markers found. Output must start with 'diff --git'"
                logger.error(f"{error_msg}\nFirst 500 chars: {content[:500]}")
                return BuilderResult(
                    success=False,
                    patch_content="",
                    builder_messages=[error_msg],
                    tokens_used=tokens_used,
                    model_used=model,
                    error=error_msg,
                )

            logger.debug(
                f"GLM Builder completed: {tokens_used} tokens, patch length: {len(patch_content)}"
            )

            return BuilderResult(
                success=True,
                patch_content=patch_content,
                builder_messages=["Generated by GLM Builder"],
                tokens_used=tokens_used,
                model_used=model,
            )

        except Exception as e:
            logger.error(f"GLM Builder execution failed: {str(e)}")
            return BuilderResult(
                success=False,
                patch_content="",
                builder_messages=[f"GLM Builder error: {str(e)}"],
                tokens_used=0,
                model_used=model,
                error=str(e),
            )

    def _extract_diff_from_text(self, text: str) -> str:
        """Extract git diff content from text that may contain explanations."""
        import re

        lines = text.split("\n")
        diff_lines = []
        in_diff = False

        for line in lines:
            if line.startswith("diff --git"):
                in_diff = True
                diff_lines.append(line)
            elif in_diff:
                # Clean up malformed hunk headers (remove trailing context)
                if line.startswith("@@"):
                    # Extract the valid hunk header part only
                    match = re.match(r"^(@@\s+-\d+,\d+\s+\+\d+,\d+\s+@@)", line)
                    if match:
                        # Use only the valid hunk header, discard anything after
                        clean_line = match.group(1)
                        diff_lines.append(clean_line)
                    else:
                        # Malformed hunk header, skip it
                        logger.warning(f"Skipping malformed hunk header: {line[:80]}")
                        continue
                elif (
                    line.startswith(("index ", "---", "+++", "+", "-", " "))
                    or line.startswith("new file mode")
                    or line.startswith("deleted file mode")
                    or line.startswith("similarity index")
                    or line.startswith("rename from")
                    or line.startswith("rename to")
                    or line == ""
                ):
                    diff_lines.append(line)
                elif line.startswith("diff --git"):
                    diff_lines.append(line)
                else:
                    if line.startswith("```") or line.startswith("#"):
                        break

        return "\n".join(diff_lines) if diff_lines else ""

    def _build_system_prompt(self) -> str:
        """Build system prompt for Builder"""
        return """You are an expert software engineer working as the Builder in an autonomous build system.

Your role:
1. Read the phase specification carefully
2. Generate clean, working code that implements the requirements
3. Return a unified git diff/patch format
4. Ensure code follows best practices and is production-ready

CRITICAL REQUIREMENTS:
1. Output ONLY a raw git diff format patch
2. Do NOT wrap it in JSON, markdown code blocks, or any other format
3. Do NOT add explanatory text before or after the patch
4. Start directly with: diff --git a/path/to/file.py b/path/to/file.py
5. NEVER use "..." or any abbreviation - show COMPLETE code
6. NEVER truncate or abbreviate ANY part of the diff
7. Show the ENTIRE file content - do NOT use ellipsis (...) ANYWHERE

GIT DIFF FORMAT RULES:
- Each file change MUST start with: diff --git a/PATH b/PATH
- Followed by: index HASH..HASH (use 0000000 placeholders if unknown)
- Then: --- a/PATH and +++ b/PATH
- Then ONE hunk header per contiguous change: @@ -START,COUNT +START,COUNT @@
- CRITICAL: Each @@ hunk header MUST be UNIQUE - never repeat the same line numbers
- CRITICAL: The COUNT in @@ -START,COUNT must EXACTLY match the number of context/removed lines
- CRITICAL: The COUNT in @@ +START,COUNT must EXACTLY match the number of context/added lines
- Then the actual changes with +/- prefixes
- Use COMPLETE file paths from repository root
- Do NOT use relative or partial paths
- Do NOT abbreviate variable names, function names, or ANY code

HUNK HEADER EXAMPLE:
For modifying lines 10-15 of a file (removing 2 lines, adding 3):
@@ -10,6 +10,7 @@
 context line (unchanged)
-removed line 1
-removed line 2
+added line 1
+added line 2
+added line 3
 context line (unchanged)

COMMON ERRORS TO AVOID:
- Do NOT generate multiple @@ headers with the same -START value
- Do NOT mismatch the line counts in hunk headers
- Do NOT include duplicate hunks for the same code region

Guidelines:
- Write idiomatic code for the language/framework
- Include error handling where appropriate
- Add docstrings/comments for complex logic
- Follow existing code style in the repository
- Don't over-engineer - keep it simple and focused
- Output ONLY the raw git diff format patch"""

    def _build_user_prompt(
        self,
        phase_spec: Dict,
        file_context: Optional[Dict],
        project_rules: Optional[List] = None,
        run_hints: Optional[List] = None,
    ) -> str:
        """Build user prompt with phase details"""
        prompt_parts = []

        # Stage 0A + 0B: Inject learned rules and hints
        if project_rules or run_hints:
            from .learned_rules import (format_hints_for_prompt,
                                        format_rules_for_prompt)

            if project_rules:
                rules_section = format_rules_for_prompt(project_rules)
                if rules_section:
                    prompt_parts.append(rules_section)
                    prompt_parts.append("\n")

            if run_hints:
                hints_section = format_hints_for_prompt(run_hints)
                if hints_section:
                    prompt_parts.append(hints_section)
                    prompt_parts.append("\n")

        # Milestone 2: Inject intention anchor (canonical project goal)
        if run_id := phase_spec.get("run_id"):
            from .intention_anchor import load_and_render_for_builder

            anchor_section = load_and_render_for_builder(
                run_id=run_id,
                phase_id=phase_spec.get("phase_id", "unknown"),
                base_dir=".",  # Use current directory (.autonomous_runs/<run_id>/)
            )
            if anchor_section:
                prompt_parts.append(anchor_section)
                prompt_parts.append("\n")

        # Add phase details
        prompt_parts.append("## Phase Specification\n")
        prompt_parts.append(f"**Phase ID:** {phase_spec.get('phase_id')}\n")
        prompt_parts.append(f"**Task Category:** {phase_spec.get('task_category')}\n")
        prompt_parts.append(f"**Complexity:** {phase_spec.get('complexity')}\n")
        prompt_parts.append(f"**Description:** {phase_spec.get('description')}\n")

        if acceptance_criteria := phase_spec.get("acceptance_criteria"):
            prompt_parts.append("\n**Acceptance Criteria:**\n")
            for idx, criterion in enumerate(acceptance_criteria, 1):
                prompt_parts.append(f"{idx}. {criterion}\n")

        if file_context:
            prompt_parts.append("\n## Repository Context\n")
            if existing_files := file_context.get("existing_files"):
                prompt_parts.append("**Existing Files:**\n")
                for file_path, content in existing_files.items():
                    prompt_parts.append(f"\n### {file_path}\n```\n{content}\n```\n")

        prompt_parts.append("\n## Instructions\n")
        prompt_parts.append("Generate a complete implementation as a unified git diff/patch.")

        return "\n".join(prompt_parts)


class GLMAuditorClient:
    """Auditor implementation using GLM (Zhipu AI) API

    Reviews code patches and finds issues.
    Uses GLM-4.5 for code review and analysis.
    """

    def __init__(self, api_key: Optional[str] = None, api_base: Optional[str] = None):
        """Initialize GLM client

        Args:
            api_key: GLM API key (defaults to GLM_API_KEY env var)
            api_base: GLM API base URL (defaults to GLM_API_BASE env var or default URL)
        """
        self.api_key = api_key or os.getenv("GLM_API_KEY")
        self.api_base = api_base or os.getenv("GLM_API_BASE", DEFAULT_GLM_API_BASE)

        if not self.api_key:
            raise ValueError("GLM_API_KEY environment variable is required for GLM client")

        self.client = OpenAI(api_key=self.api_key, base_url=self.api_base)

    def review_patch(
        self,
        patch_content: str,
        phase_spec: Dict,
        max_tokens: Optional[int] = None,
        model: str = "glm-4.7",
        project_rules: Optional[List] = None,
        run_hints: Optional[List] = None,
    ) -> AuditorResult:
        """Review a patch and find issues

        Args:
            patch_content: Git diff/patch to review
            phase_spec: Phase specification for context
            max_tokens: Token budget limit for this call
            model: GLM model to use
            project_rules: Persistent project learned rules (Stage 0B)
            run_hints: Within-run hints from earlier phases (Stage 0A)

        Returns:
            AuditorResult with issues_found and metadata
        """
        try:
            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(
                patch_content, phase_spec, project_rules, run_hints
            )

            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                max_tokens=max_tokens or 8192,  # Higher limit for complex reviews
                response_format={"type": "json_object"},
                temperature=0.1,
            )

            result_json = json.loads(response.choices[0].message.content)
            tokens_used = response.usage.total_tokens if response.usage else 0

            issues = result_json.get("issues", [])
            has_major_issues = any(issue.get("severity") == "major" for issue in issues)
            approved = not has_major_issues

            return AuditorResult(
                approved=approved,
                issues_found=issues,
                auditor_messages=result_json.get("messages", []),
                tokens_used=tokens_used,
                model_used=model,
            )

        except Exception as e:
            return AuditorResult(
                approved=False,
                issues_found=[
                    {
                        "severity": "major",
                        "category": "auditor_error",
                        "description": f"GLM Auditor error: {str(e)}",
                        "location": "unknown",
                    }
                ],
                auditor_messages=[f"GLM Auditor error: {str(e)}"],
                tokens_used=0,
                model_used=model,
                error=str(e),
            )

    def _build_system_prompt(self) -> str:
        """Build system prompt for Auditor"""
        return """You are an expert code reviewer working as the Auditor in an autonomous build system.

Your role:
1. Review code patches for issues
2. Check for security vulnerabilities, bugs, code quality problems
3. Classify issues by severity (minor/major)
4. Approve patches with no major issues

Output format (JSON):
{
  "approved": true/false,
  "issues": [
    {
      "severity": "minor|major",
      "category": "security|bug|quality|style|test|documentation",
      "description": "Clear description of the issue",
      "location": "file:line or general area",
      "suggestion": "How to fix (optional)"
    }
  ],
  "messages": ["list of review comments"],
  "tests_verified": true/false
}

Severity guidelines:
- **major**: Security vulnerabilities, critical bugs, missing error handling, broken functionality
- **minor**: Style issues, minor improvements, missing comments, test coverage gaps

Be thorough but fair. Approve patches that work correctly even if they have minor style issues."""

    def _build_user_prompt(
        self,
        patch_content: str,
        phase_spec: Dict,
        project_rules: Optional[List] = None,
        run_hints: Optional[List] = None,
    ) -> str:
        """Build user prompt with patch and context"""
        prompt_parts = []

        if project_rules or run_hints:
            from .learned_rules import (format_hints_for_prompt,
                                        format_rules_for_prompt)

            if project_rules:
                rules_section = format_rules_for_prompt(project_rules)
                if rules_section:
                    prompt_parts.append(rules_section)
                    prompt_parts.append("\n")

            if run_hints:
                hints_section = format_hints_for_prompt(run_hints)
                if hints_section:
                    prompt_parts.append(hints_section)
                    prompt_parts.append("\n")

        # Milestone 2: Inject intention anchor (for validation context)
        if run_id := phase_spec.get("run_id"):
            from .intention_anchor import load_and_render_for_auditor

            anchor_section = load_and_render_for_auditor(
                run_id=run_id,
                base_dir=".",  # Use current directory (.autonomous_runs/<run_id>/)
            )
            if anchor_section:
                prompt_parts.append(anchor_section)
                prompt_parts.append("\n")

        prompt_parts.append("## Phase Context\n")
        prompt_parts.append(f"**Task Category:** {phase_spec.get('task_category')}\n")
        prompt_parts.append(f"**Complexity:** {phase_spec.get('complexity')}\n")
        prompt_parts.append(f"**Description:** {phase_spec.get('description')}\n")

        prompt_parts.append(f"\n## Patch to Review\n```diff\n{patch_content}\n```\n")

        prompt_parts.append("\n## Review Instructions\n")
        prompt_parts.append("Review this patch carefully for:")
        prompt_parts.append("1. Security vulnerabilities (SQL injection, XSS, etc.)")
        prompt_parts.append("2. Bugs and logic errors")
        prompt_parts.append("3. Code quality and best practices")
        prompt_parts.append("4. Test coverage (if this is a test phase)")
        prompt_parts.append("5. Documentation clarity (if this is a docs phase)")
        prompt_parts.append("\nReturn your review in the specified JSON format.")

        return "\n".join(prompt_parts)
