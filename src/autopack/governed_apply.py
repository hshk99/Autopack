"""
Governed Apply System for Autopack

Safely applies code patches generated by the Builder to the filesystem.
Uses git apply for patch application with proper error handling.

Enhanced with self-troubleshoot capabilities:
- Post-application file validation (syntax check)
- File integrity checks before/after fallback operations
- Automatic restoration on corruption detection

Per GPT_RESPONSE18: Added symbol preservation and structural similarity validation.
"""

import subprocess
import logging
import re
import hashlib
import ast
from difflib import SequenceMatcher
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Set

from .repair_helpers import YamlRepairHelper, save_repair_debug

logger = logging.getLogger(__name__)


# =============================================================================
# VALIDATION FUNCTIONS (per GPT_RESPONSE18 Q5/Q6)
# =============================================================================

def extract_python_symbols(source: str) -> Set[str]:
    """
    Extract top-level symbols from Python source using AST.
    
    Per GPT_RESPONSE18 Q5: Extract function and class definitions,
    plus uppercase module-level constants.
    
    Args:
        source: Python source code
        
    Returns:
        Set of symbol names (functions, classes, CONSTANTS)
    """
    try:
        tree = ast.parse(source)
        names: Set[str] = set()
        for node in tree.body:
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                names.add(node.name)
            elif isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name) and target.id.isupper():
                        names.add(target.id)
        return names
    except SyntaxError:
        return set()


def check_symbol_preservation(
    old_content: str,
    new_content: str,
    max_lost_ratio: float
) -> Tuple[bool, str]:
    """
    Check if too many symbols were lost in the patch.
    
    Per GPT_RESPONSE18 Q5: Reject if >30% of symbols are lost (configurable).
    
    Args:
        old_content: Original file content
        new_content: New file content after patch
        max_lost_ratio: Maximum ratio of symbols that can be lost (e.g., 0.3)
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    old_symbols = extract_python_symbols(old_content)
    new_symbols = extract_python_symbols(new_content)
    lost = old_symbols - new_symbols
    
    if old_symbols:
        lost_ratio = len(lost) / len(old_symbols)
        if lost_ratio > max_lost_ratio:
            lost_names = ", ".join(sorted(lost)[:10])
            if len(lost) > 10:
                lost_names += f"... (+{len(lost) - 10} more)"
            return False, (
                f"symbol_preservation_violation: Lost {len(lost)}/{len(old_symbols)} symbols "
                f"({lost_ratio:.1%} > {max_lost_ratio:.0%} threshold). "
                f"Lost: [{lost_names}]"
            )
    
    return True, ""


def check_structural_similarity(
    old_content: str,
    new_content: str,
    min_ratio: float
) -> Tuple[bool, str]:
    """
    Check if file was drastically rewritten unexpectedly.
    
    Per GPT_RESPONSE18 Q6: Reject if structural similarity is <60% (configurable)
    for files >=300 lines.
    
    Args:
        old_content: Original file content
        new_content: New file content after patch
        min_ratio: Minimum similarity ratio required (e.g., 0.6)
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    ratio = SequenceMatcher(None, old_content, new_content).ratio()
    if ratio < min_ratio:
        return False, (
            f"structural_similarity_violation: Similarity {ratio:.2f} below threshold {min_ratio}. "
            f"File appears to have been drastically rewritten."
        )
    
    return True, ""


class PatchApplyError(Exception):
    """Raised when patch application fails"""
    pass


class GovernedApplyPath:
    """
    Safely applies patches to the filesystem using git apply.

    This class provides:
    - Safe patch application with validation
    - Automatic cleanup of temporary files
    - Detailed error reporting
    - File verification
    - Workspace isolation (protected paths)
    """

    # Protected paths that Builder should never modify
    # These are Autopack's own source/config directories
    PROTECTED_PATHS = [
        "src/autopack/",      # Autopack core modules
        "config/",            # Configuration files
        ".autonomous_runs/",  # Run state and logs
        ".git/",              # Git internals
    ]

    # Paths that are always allowed (can override protection if needed)
    ALLOWED_PATHS = [
        # Core maintenance paths that Autopack may update in self-repair runs
        "src/autopack/learned_rules.py",
        "src/autopack/llm_service.py",
        "src/autopack/openai_clients.py",
        "src/autopack/gemini_clients.py",
        "src/autopack/glm_clients.py",
        "config/models.yaml",
    ]

    # Run types that support internal mode
    MAINTENANCE_RUN_TYPES = ["autopack_maintenance", "autopack_upgrade", "self_repair"]

    def __init__(
        self,
        workspace: Path,
        allowed_paths: List[str] = None,
        protected_paths: List[str] = None,
        autopack_internal_mode: bool = False,
        run_type: str = "project_build",
        scope_paths: List[str] = None,
    ):
        """
        Initialize GovernedApplyPath.

        Args:
            workspace: Path to the workspace root directory
            allowed_paths: Additional paths to allow (overrides protection)
            protected_paths: Additional paths to protect (extends defaults)
            autopack_internal_mode: If True, allows writes to src/autopack/ (requires maintenance run_type)
            run_type: Type of run - "project_build" (default) or "autopack_maintenance"
            scope_paths: Optional list of allowed file paths (scope enforcement - Option C Layer 2)

        Raises:
            ValueError: If autopack_internal_mode=True but run_type is not a maintenance type

        Note on workspace isolation (per GPT_RESPONSE6 recommendations):
        - Normal project runs (project_build): PROTECTED_PATHS enforced as-is
        - Maintenance runs (autopack_maintenance): autopack_internal_mode unlocks src/autopack/
          but still protects .autonomous_runs/, .git/ unless explicitly overridden

        Note on scope enforcement (per GPT_RESPONSE - Option C Layer 2):
        - If scope_paths is provided, ONLY those paths can be modified
        - This is the second validation layer (after context loading)
        """
        if isinstance(workspace, str):
            workspace = Path(workspace)
        self.workspace = workspace
        self._file_backups: Dict[str, Tuple[str, str]] = {}  # path -> (hash, content)
        self.run_type = run_type
        self.autopack_internal_mode = autopack_internal_mode
        self.scope_paths = scope_paths or []  # NEW: Store scope paths for validation

        # [Q7 Implementation] Validate autopack_internal_mode is only used with maintenance runs
        if autopack_internal_mode and run_type not in self.MAINTENANCE_RUN_TYPES:
            raise ValueError(
                f"autopack_internal_mode=True only allowed for maintenance runs "
                f"(run_type must be one of {self.MAINTENANCE_RUN_TYPES}, got '{run_type}')"
            )

        # Merge default protected paths with any additional ones
        self.protected_paths = list(self.PROTECTED_PATHS)
        if protected_paths:
            self.protected_paths.extend(protected_paths)

        # [Q7 Implementation] In internal mode, unlock src/autopack/ but keep critical paths protected
        if autopack_internal_mode:
            logger.info("[Isolation] autopack_internal_mode enabled - unlocking src/autopack/ for maintenance")
            # Remove src/autopack/ from protection, keep others
            self.protected_paths = [p for p in self.protected_paths if p != "src/autopack/"]

        # Merge default allowed paths with any additional ones
        self.allowed_paths = list(self.ALLOWED_PATHS)
        if allowed_paths:
            for path in allowed_paths:
                normalized = path.replace("\\", "/")
                if not normalized:
                    continue
                target = normalized
                if not target.endswith("/"):
                    suffix = Path(target).suffix
                    if not suffix:  # Treat as directory prefix
                        target += "/"
                self.allowed_paths.append(target)

    # =========================================================================
    # WORKSPACE ISOLATION METHODS
    # =========================================================================

    def _is_path_protected(self, file_path: str) -> bool:
        """
        Check if a file path is protected from modification.

        Args:
            file_path: Relative file path to check

        Returns:
            True if path is protected, False otherwise
        """
        # Normalize path separators
        normalized_path = file_path.replace('\\', '/')

        # Check if path is explicitly allowed (overrides protection)
        for allowed in self.allowed_paths:
            if normalized_path.startswith(allowed.replace('\\', '/')):
                return False

        # Check if path matches any protected prefix
        for protected in self.protected_paths:
            if normalized_path.startswith(protected.replace('\\', '/')):
                return True

        return False

    def _validate_patch_paths(self, files: List[str]) -> Tuple[bool, List[str]]:
        """
        Validate that patch does not touch protected directories or violate scope.

        This is a critical workspace isolation check that prevents Builder
        from corrupting Autopack's own source code.

        NEW (Option C - Layer 2): Also enforces scope configuration if present.

        Args:
            files: List of file paths from the patch

        Returns:
            Tuple of (is_valid, list of violations)
        """
        violations = []

        # Check 1: Protected paths (existing)
        for file_path in files:
            if self._is_path_protected(file_path):
                violations.append(f"Protected path: {file_path}")
                logger.warning(f"[Isolation] BLOCKED: Patch attempts to modify protected path: {file_path}")

        # Check 2: Scope enforcement (NEW - Option C Layer 2)
        if self.scope_paths:
            # Normalize scope paths for comparison
            normalized_scope = set()
            for path in self.scope_paths:
                normalized_scope.add(path.replace('\\', '/'))

            for file_path in files:
                normalized_file = file_path.replace('\\', '/')
                if normalized_file not in normalized_scope:
                    violations.append(f"Outside scope: {file_path}")
                    logger.warning(f"[Scope] BLOCKED: Patch attempts to modify file outside scope: {file_path}")

            if len(violations) > len([v for v in violations if v.startswith("Protected")]):
                logger.error(f"[Scope] Patch rejected - {len([v for v in violations if v.startswith('Outside')])} files outside scope")

        if violations:
            logger.error(f"[Isolation] Patch rejected - {len(violations)} violations (protected paths + scope)")
            return False, violations

        return True, []

    # =========================================================================
    # FILE VALIDATION AND INTEGRITY METHODS (Self-Troubleshoot Enhancement)
    # =========================================================================

    def _compute_file_hash(self, file_path: Path) -> Optional[str]:
        """Compute SHA256 hash of a file for integrity checking."""
        try:
            if file_path.exists():
                with open(file_path, 'rb') as f:
                    return hashlib.sha256(f.read()).hexdigest()
        except Exception as e:
            logger.warning(f"Failed to compute hash for {file_path}: {e}")
        return None

    def _backup_files(self, file_paths: List[str]) -> Dict[str, Tuple[str, str]]:
        """
        Create in-memory backups of files before modification.

        Args:
            file_paths: List of relative file paths to backup

        Returns:
            Dict mapping file path to (hash, content) tuple
        """
        backups = {}
        for rel_path in file_paths:
            full_path = self.workspace / rel_path
            if full_path.exists():
                try:
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    file_hash = hashlib.sha256(content.encode()).hexdigest()
                    backups[rel_path] = (file_hash, content)
                    logger.debug(f"Backed up {rel_path} (hash: {file_hash[:12]}...)")
                except Exception as e:
                    logger.warning(f"Failed to backup {rel_path}: {e}")
        return backups

    def _restore_file(self, rel_path: str, backup: Tuple[str, str]) -> bool:
        """
        Restore a file from backup.

        Args:
            rel_path: Relative file path
            backup: Tuple of (hash, content)

        Returns:
            True if restoration succeeded
        """
        file_hash, content = backup
        full_path = self.workspace / rel_path
        try:
            with open(full_path, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"[Integrity] Restored {rel_path} from backup")
            return True
        except Exception as e:
            logger.error(f"[Integrity] Failed to restore {rel_path}: {e}")
            return False

    def _validate_python_syntax(self, file_path: Path) -> Tuple[bool, Optional[str]]:
        """
        Validate Python file syntax by attempting to compile it.

        Args:
            file_path: Path to Python file

        Returns:
            Tuple of (is_valid, error_message)
        """
        if not file_path.suffix == '.py':
            return True, None

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                source = f.read()
            compile(source, str(file_path), 'exec')
            return True, None
        except SyntaxError as e:
            error_msg = f"Line {e.lineno}: {e.msg}"
            return False, error_msg
        except Exception as e:
            return False, str(e)

    def _check_merge_conflict_markers(self, file_path: Path) -> Tuple[bool, Optional[str]]:
        """
        Check if a file contains git merge conflict markers.

        These markers can be left behind by 3-way merge (-3) fallback when patches
        don't apply cleanly. They cause syntax errors and must be detected early.

        Note: We only check for '<<<<<<<' and '>>>>>>>' as these are unique to
        merge conflicts. '=======' alone is commonly used as a section divider
        in code comments (e.g., # =========) and would cause false positives.

        Args:
            file_path: Path to file to check

        Returns:
            Tuple of (has_conflicts, error_message)
        """
        # Only check for unique conflict markers, not '=======' which is used in comments
        conflict_markers = ['<<<<<<<', '>>>>>>>']
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line_num, line in enumerate(f, 1):
                    for marker in conflict_markers:
                        if marker in line:
                            return True, f"Line {line_num}: merge conflict marker '{marker}' found"
            return False, None
        except Exception as e:
            logger.warning(f"Failed to check merge conflicts in {file_path}: {e}")
            return False, None

    def _validate_applied_files(self, files_modified: List[str]) -> Tuple[bool, List[str]]:
        """
        Verify files are syntactically valid after patch application.

        This is a critical self-troubleshoot check that detects corruption
        immediately after any file modification.

        Args:
            files_modified: List of relative file paths that were modified

        Returns:
            Tuple of (all_valid, list_of_corrupted_files)
        """
        corrupted_files = []

        for rel_path in files_modified:
            full_path = self.workspace / rel_path

            if not full_path.exists():
                logger.warning(f"[Validation] File does not exist after patch: {rel_path}")
                continue

            # Check for merge conflict markers (critical - prevents API crashes)
            has_conflicts, conflict_error = self._check_merge_conflict_markers(full_path)
            if has_conflicts:
                logger.error(f"[Validation] MERGE CONFLICTS: {rel_path} - {conflict_error}")
                corrupted_files.append(rel_path)
                continue  # Skip other validations - file is definitely corrupted

            # Validate Python files
            if full_path.suffix == '.py':
                is_valid, error = self._validate_python_syntax(full_path)
                if not is_valid:
                    logger.error(f"[Validation] CORRUPTED: {rel_path} - {error}")
                    corrupted_files.append(rel_path)
                else:
                    logger.debug(f"[Validation] OK: {rel_path}")

            # Validate JSON files
            elif full_path.suffix == '.json':
                try:
                    import json
                    with open(full_path, 'r', encoding='utf-8') as f:
                        json.load(f)
                    logger.debug(f"[Validation] OK: {rel_path}")
                except json.JSONDecodeError as e:
                    logger.error(f"[Validation] CORRUPTED: {rel_path} - Invalid JSON: {e}")
                    corrupted_files.append(rel_path)

            # Validate YAML files
            elif full_path.suffix in ['.yaml', '.yml']:
                try:
                    import yaml
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    # Allow leading comments without explicit document start by prepending '---'
                    stripped = content.lstrip()
                    if stripped.startswith("#") and not stripped.startswith("---"):
                        content = "---\n" + content
                    yaml.safe_load(content)
                    logger.debug(f"[Validation] OK: {rel_path}")
                except yaml.YAMLError as e:
                    logger.error(f"[Validation] CORRUPTED: {rel_path} - Invalid YAML: {e}")
                    corrupted_files.append(rel_path)

        if corrupted_files:
            logger.error(f"[Validation] {len(corrupted_files)} files corrupted after patch application")
            return False, corrupted_files

        logger.info(f"[Validation] All {len(files_modified)} modified files validated successfully")
        return True, []

    def _validate_content_changes(
        self,
        files_modified: List[str],
        backups: Dict[str, Tuple[str, str]],
        validation_config: Optional[Dict] = None
    ) -> Tuple[bool, List[str]]:
        """
        Validate content changes using symbol preservation and structural similarity.
        
        Per GPT_RESPONSE18 Q5/Q6: Post-apply validation that checks:
        - Python files: symbol preservation (≤30% loss allowed)
        - Large files (≥300 lines): structural similarity (≥60% required)
        
        Args:
            files_modified: List of relative file paths that were modified
            backups: Dict mapping file path to (hash, content) tuple
            validation_config: Optional config dict with thresholds
            
        Returns:
            Tuple of (all_valid, list of files with issues)
        """
        # Load validation config from models.yaml or use defaults
        if validation_config is None:
            try:
                import yaml
                config_path = Path(__file__).parent.parent.parent / "config" / "models.yaml"
                if config_path.exists():
                    with open(config_path) as f:
                        models_config = yaml.safe_load(f)
                        validation_config = models_config.get("validation", {})
                else:
                    validation_config = {}
            except Exception as e:
                logger.debug(f"[Validation] Could not load validation config: {e}")
                validation_config = {}
        
        # Get thresholds from config
        symbol_config = validation_config.get("symbol_preservation", {})
        symbol_enabled = symbol_config.get("enabled", True)
        max_lost_ratio = symbol_config.get("max_lost_ratio", 0.3)
        
        similarity_config = validation_config.get("structural_similarity", {})
        similarity_enabled = similarity_config.get("enabled", True)
        min_ratio = similarity_config.get("min_ratio", 0.6)
        min_lines_for_check = similarity_config.get("min_lines_for_check", 300)
        
        problem_files = []
        
        for rel_path in files_modified:
            full_path = self.workspace / rel_path
            
            # Skip if file doesn't exist (was deleted) or no backup
            if not full_path.exists() or rel_path not in backups:
                continue
            
            # Get old content from backup
            _, old_content = backups[rel_path]
            
            # Read new content
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    new_content = f.read()
            except Exception as e:
                logger.warning(f"[Validation] Failed to read {rel_path}: {e}")
                continue
            
            old_line_count = old_content.count('\n') + 1
            
            # Check 1: Symbol preservation for Python files
            if symbol_enabled and full_path.suffix == '.py':
                is_valid, error = check_symbol_preservation(
                    old_content, new_content, max_lost_ratio
                )
                if not is_valid:
                    logger.warning(f"[Validation] SYMBOL_LOSS: {rel_path} - {error}")
                    problem_files.append(rel_path)
                    continue  # Skip further checks for this file
            
            # Check 2: Structural similarity for large files
            if similarity_enabled and old_line_count >= min_lines_for_check:
                is_valid, error = check_structural_similarity(
                    old_content, new_content, min_ratio
                )
                if not is_valid:
                    logger.warning(f"[Validation] SIMILARITY_LOW: {rel_path} - {error}")
                    problem_files.append(rel_path)
                    continue
        
        if problem_files:
            logger.warning(
                f"[Validation] {len(problem_files)} files have content validation issues: "
                f"{', '.join(problem_files[:5])}"
                + (f" (+{len(problem_files) - 5} more)" if len(problem_files) > 5 else "")
            )
            return False, problem_files
        
        return True, []

    def _restore_corrupted_files(
        self,
        corrupted_files: List[str],
        backups: Dict[str, Tuple[str, str]]
    ) -> Tuple[int, int]:
        """
        Attempt to restore corrupted files from backup.

        Args:
            corrupted_files: List of corrupted file paths
            backups: Dict of backups from _backup_files()

        Returns:
            Tuple of (restored_count, failed_count)
        """
        restored = 0
        failed = 0

        for rel_path in corrupted_files:
            if rel_path in backups:
                if self._restore_file(rel_path, backups[rel_path]):
                    restored += 1
                else:
                    failed += 1
            else:
                # No backup available - file was new, just delete it
                full_path = self.workspace / rel_path
                try:
                    full_path.unlink()
                    logger.info(f"[Integrity] Removed corrupted new file: {rel_path}")
                    restored += 1
                except Exception as e:
                    logger.error(f"[Integrity] Failed to remove {rel_path}: {e}")
                    failed += 1

        return restored, failed

    # =========================================================================
    # PATCH REPAIR METHODS
    # =========================================================================

    def _repair_hunk_headers(self, patch_content: str) -> str:
        """
        Repair @@ hunk headers with incorrect line numbers and counts.

        LLMs often generate patches with wrong line numbers and counts. This function:
        1. For existing files, finds actual line number by matching context
        2. Recounts actual additions/deletions/context lines
        3. Rewrites the @@ headers with correct values

        Args:
            patch_content: Patch content that may have incorrect headers

        Returns:
            Patch with repaired headers
        """
        result_lines = []
        current_file = None
        current_file_content = None
        is_new_file = False

        lines = patch_content.split('\n')
        i = 0
        while i < len(lines):
            line = lines[i]

            # Track which file we're patching
            if line.startswith('--- a/'):
                file_path = line[6:]
                current_file = self.workspace / file_path
                is_new_file = False
                if current_file.exists():
                    try:
                        current_file_content = current_file.read_text(encoding='utf-8', errors='ignore').split('\n')
                    except Exception:
                        current_file_content = None
                else:
                    current_file_content = None
                result_lines.append(line)
                i += 1
                continue

            # For new files, track that it's a new file
            if line.startswith('--- /dev/null'):
                is_new_file = True
                current_file_content = None
                result_lines.append(line)
                i += 1
                continue

            # Repair @@ headers
            if line.startswith('@@'):
                # Parse the hunk header: @@ -OLD_START,OLD_COUNT +NEW_START,NEW_COUNT @@
                match = re.match(r'@@ -(\d+),?(\d*) \+(\d+),?(\d*) @@(.*)', line)
                if match:
                    suffix = match.group(5)

                    # Collect all hunk content lines first
                    hunk_content = []
                    j = i + 1
                    while j < len(lines) and not lines[j].startswith('@@') and not lines[j].startswith('diff --git'):
                        hunk_content.append(lines[j])
                        j += 1

                    # Remove trailing empty lines from hunk (common LLM artifact)
                    while hunk_content and (hunk_content[-1] == '' or hunk_content[-1] == ' '):
                        hunk_content.pop()

                    # Count actual lines in the hunk
                    additions = 0
                    deletions = 0
                    context = 0
                    for hunk_line in hunk_content:
                        if hunk_line.startswith('+') and not hunk_line.startswith('+++'):
                            additions += 1
                        elif hunk_line.startswith('-') and not hunk_line.startswith('---'):
                            deletions += 1
                        elif hunk_line.startswith(' '):
                            context += 1
                        elif hunk_line.startswith('\\ No newline'):
                            pass  # Don't count this

                    if is_new_file:
                        # New file: old is 0,0, new is 1,additions
                        new_count = additions
                        repaired_line = f'@@ -0,0 +1,{new_count} @@{suffix}'
                    elif current_file_content is not None:
                        # Existing file - try to find context position
                        old_count = deletions + context
                        new_count = additions + context

                        # Try to find actual start line by matching context
                        context_lines = []
                        k = i + 1
                        while k < len(lines) and not lines[k].startswith('@@') and not lines[k].startswith('diff --git'):
                            hunk_line = lines[k]
                            if hunk_line.startswith(' ') or hunk_line.startswith('-'):
                                context_lines.append(hunk_line[1:] if len(hunk_line) > 1 else '')
                            k += 1

                        actual_start = 1  # Default
                        if context_lines:
                            first_context = context_lines[0]
                            for line_num, file_line in enumerate(current_file_content, 1):
                                if file_line.strip() == first_context.strip():
                                    actual_start = line_num
                                    break

                        repaired_line = f'@@ -{actual_start},{old_count} +{actual_start},{new_count} @@{suffix}'
                    else:
                        # Can't determine file content, use counted values
                        old_start = int(match.group(1))
                        old_count = deletions + context
                        new_count = additions + context
                        repaired_line = f'@@ -{old_start},{old_count} +{old_start},{new_count} @@{suffix}'

                    if repaired_line != line:
                        logger.debug(f"Repaired hunk header: {line} -> {repaired_line}")
                    result_lines.append(repaired_line)
                    i += 1
                    continue

            result_lines.append(line)
            i += 1

        return '\n'.join(result_lines)

    def _fix_empty_file_diffs(self, patch_content: str) -> str:
        """
        Fix incomplete diff headers for empty new files.

        LLMs often generate incomplete diffs for empty __init__.py files like:
            diff --git a/path/__init__.py b/path/__init__.py
            new file mode 100644
            index 0000000..e69de29
            diff --git ...  (next file)

        This is missing the --- /dev/null and +++ b/path lines.

        Args:
            patch_content: Patch content that may have incomplete empty file diffs

        Returns:
            Patch with fixed empty file headers
        """
        lines = patch_content.split('\n')
        result = []
        i = 0

        while i < len(lines):
            line = lines[i]

            # Check for incomplete empty file pattern
            if line.startswith('index ') and 'e69de29' in line:
                # e69de29 is the git hash for empty content
                result.append(line)
                # Check if next line is another diff (missing --- and +++)
                if i + 1 < len(lines) and lines[i + 1].startswith('diff --git'):
                    # Find the file path from the previous diff --git line
                    for j in range(len(result) - 1, -1, -1):
                        if result[j].startswith('diff --git'):
                            # Extract file path: diff --git a/path b/path
                            parts = result[j].split()
                            if len(parts) >= 4:
                                file_path = parts[3]  # b/path
                                # Insert missing headers
                                result.append('--- /dev/null')
                                result.append(f'+++ {file_path}')
                                logger.debug(f"Fixed empty file diff for {file_path}")
                            break
                i += 1
                continue

            result.append(line)
            i += 1

        return '\n'.join(result)

    def _remove_existing_files_for_new_patches(self, patch_content: str) -> str:
        """
        Remove existing files that patches try to create as 'new file'.

        When a patch marks a file as 'new file mode' but the file already exists,
        delete the existing file so the patch can create it fresh.

        Args:
            patch_content: Patch content

        Returns:
            Unchanged patch content (side effect: removes conflicting files)
        """
        lines = patch_content.split('\n')
        i = 0

        while i < len(lines):
            line = lines[i]

            if line.startswith('diff --git'):
                # Extract file path: diff --git a/path b/path
                parts = line.split()
                if len(parts) >= 4:
                    file_path = parts[3][2:]  # Remove 'b/' prefix
                    full_path = self.workspace / file_path

                    # Check if next lines indicate new file mode
                    if i + 1 < len(lines) and lines[i + 1].startswith('new file mode'):
                        if full_path.exists():
                            # File exists but patch wants to create it - remove it
                            logger.info(f"Removing existing file for new file patch: {file_path}")
                            try:
                                full_path.unlink()
                            except Exception as e:
                                logger.warning(f"Failed to remove {file_path}: {e}")

            i += 1

        return patch_content

    def _validate_patch_quality(self, patch_content: str) -> List[str]:
        """
        Validate patch quality to detect LLM truncation/abbreviation issues.

        Returns:
            List of validation error messages (empty if valid)
        """
        import re

        errors = []
        lines = patch_content.split('\n')

        # Check for ellipsis/truncation markers (CRITICAL: LLMs use these when hitting token limits)
        # Be careful NOT to flag legitimate code like logger.info("...") or f-strings
        truncation_patterns = [
            r'^\+\s*\.\.\.\s*$',              # Line that is ONLY "..."
            r'^\+\s*#\s*\.\.\.\s*$',          # Comment line that is only "# ..."
            r'^\+.*\.\.\.\s*more\s+code',     # "... more code" pattern
            r'^\+.*\.\.\.\s*rest\s+of',       # "... rest of" pattern
            r'^\+.*\.\.\.\s*continues',       # "... continues" pattern
            r'^\+.*\.\.\.\s*etc',             # "... etc" pattern
            r'^\+.*code\s+omitted\s*\.\.\.',  # "code omitted..." pattern
        ]

        for i, line in enumerate(lines, 1):
            # Skip comment lines, docstrings, and strings (... is ok there)
            stripped = line.strip()
            if stripped.startswith(('#', '"""', "'''")):
                continue
            # Skip lines with ... inside strings (legitimate code)
            if '("' in line or "('" in line or 'f"' in line or "f'" in line:
                continue

            for pattern in truncation_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    errors.append(f"Line {i} contains truncation/ellipsis '...': {line[:80]}")
                    break

        # Check for malformed hunk headers (common LLM error)
        hunk_header_pattern = re.compile(r'^@@\s+-(\d+),(\d+)\s+\+(\d+),(\d+)\s+@@')
        for i, line in enumerate(lines, 1):
            if line.startswith('@@'):
                match = hunk_header_pattern.match(line)
                if not match:
                    errors.append(f"Line {i} has malformed hunk header: {line[:80]}")
                else:
                    # Validate line counts make sense
                    old_start, old_count, new_start, new_count = map(int, match.groups())
                    if old_count == 0 and new_count == 0:
                        errors.append(f"Line {i} has zero-length hunk (invalid): {line[:80]}")

        # Check for incomplete diff structure
        if 'diff --git' in patch_content:
            has_index = 'index ' in patch_content
            has_minus = '---' in patch_content
            has_plus = '+++' in patch_content

            if not (has_index and has_minus and has_plus):
                errors.append("Incomplete diff structure (missing index/---/+++ lines)")

        # Check for truncated file content (common LLM issue - output cut off mid-file)
        # This detects files that end abruptly with unclosed strings or incomplete YAML
        truncation_errors = self._detect_truncated_content(patch_content)
        errors.extend(truncation_errors)

        return errors

    def _validate_pack_schema_in_patch(self, patch_content: str) -> List[str]:
        """
        If the patch touches pack YAML files, validate them against the pack schema.
        """
        errors: List[str] = []
        import re

        # Find files in patch
        file_paths = []
        for line in patch_content.splitlines():
            if line.startswith("diff --git"):
                parts = line.split()
                if len(parts) >= 4:
                    file_path = parts[3][2:]  # strip leading b/
                    file_paths.append(file_path)

        pack_paths = [
            p for p in file_paths
            if p.endswith((".yaml", ".yml"))
            and "backend/packs/" in p
        ]

        if not pack_paths:
            return errors

        # Extract new content for each pack and validate
        for path in pack_paths:
            new_content = self._extract_new_content_from_patch(patch_content, path)
            if not new_content:
                continue
            errors.extend(self._validate_pack_schema(path, new_content))

        return errors

    def _validate_pack_schema(self, file_path: str, content: str) -> List[str]:
        """
        Validate a country pack YAML against a minimal schema:
        - Required top-level keys: name, description, version, country, domain, categories, checklists, official_sources
        - categories: non-empty list, no duplicate names, each with name/description/examples
        - checklists: non-empty list, each with name/required_documents
        - official_sources: non-empty list
        """
        import yaml

        required_keys = ["name", "description", "version", "country", "domain", "categories", "checklists", "official_sources"]
        errors: List[str] = []

        try:
            content_to_load = content
            stripped = content.lstrip()
            # Allow leading comments without explicit document start by prepending '---'
            if stripped.startswith("#") and not stripped.startswith("---"):
                content_to_load = "---\n" + content
            data = yaml.safe_load(content_to_load)
        except Exception as e:
            errors.append(f"Pack schema: YAML parse failed for {file_path}: {e}")
            return errors

        if not isinstance(data, dict):
            errors.append(f"Pack schema: Expected mapping at top level for {file_path}")
            return errors

        for key in required_keys:
            if key not in data or data.get(key) in ("", None):
                errors.append(f"Pack schema: Missing required key '{key}' in {file_path}")

        categories = data.get("categories", [])
        if not isinstance(categories, list) or not categories:
            errors.append(f"Pack schema: 'categories' must be a non-empty list in {file_path}")
        else:
            seen = set()
            for cat in categories:
                if not isinstance(cat, dict):
                    errors.append(f"Pack schema: category entry is not a mapping in {file_path}")
                    continue
                name = cat.get("name")
                if not name or not isinstance(name, str):
                    errors.append(f"Pack schema: category missing 'name' in {file_path}")
                elif name in seen:
                    errors.append(f"Pack schema: duplicate category name '{name}' in {file_path}")
                else:
                    seen.add(name)
                if not cat.get("description"):
                    errors.append(f"Pack schema: category '{name or '?'}' missing description in {file_path}")
                examples = cat.get("examples", [])
                if not isinstance(examples, list) or not examples:
                    errors.append(f"Pack schema: category '{name or '?'}' missing examples list in {file_path}")

        checklists = data.get("checklists", [])
        if not isinstance(checklists, list) or not checklists:
            errors.append(f"Pack schema: 'checklists' must be a non-empty list in {file_path}")
        else:
            for cl in checklists:
                if not isinstance(cl, dict):
                    errors.append(f"Pack schema: checklist entry is not a mapping in {file_path}")
                    continue
                if not cl.get("name"):
                    errors.append(f"Pack schema: checklist missing 'name' in {file_path}")
                reqs = cl.get("required_documents", [])
                if not isinstance(reqs, list) or not reqs:
                    errors.append(f"Pack schema: checklist '{cl.get('name','?')}' missing required_documents list in {file_path}")

        sources = data.get("official_sources", [])
        if not isinstance(sources, list) or not sources:
            errors.append(f"Pack schema: 'official_sources' must be a non-empty list in {file_path}")

        return errors

    def _detect_truncated_content(self, patch_content: str) -> List[str]:
        """
        Detect truncated file content in patches - catches LLM output that was cut off.

        Common patterns:
        - File ends with unclosed quote (started " or ' but never closed)
        - YAML file ends mid-list without proper structure
        - File ends with "No newline at end of file" after incomplete content

        Returns:
            List of truncation error messages
        """
        errors = []
        lines = patch_content.split('\n')

        # Track files being patched and their new content
        current_file = None
        new_file_lines = []
        in_new_file = False

        for i, line in enumerate(lines):
            if line.startswith('diff --git'):
                # Check previous file for truncation before moving to next
                if current_file and new_file_lines:
                    file_errors = self._check_file_truncation(current_file, new_file_lines)
                    errors.extend(file_errors)

                # Extract new file path
                match = re.search(r'diff --git a/.+ b/(.+)', line)
                if match:
                    current_file = match.group(1)
                new_file_lines = []
                in_new_file = False

            elif line.startswith('--- /dev/null'):
                in_new_file = True

            elif line.startswith('+') and not line.startswith('+++'):
                # Collect added lines for new files
                new_file_lines.append(line[1:])  # Remove + prefix

            elif line.startswith('\\ No newline at end of file'):
                # This marker after minimal content is suspicious
                if len(new_file_lines) < 5:
                    errors.append(f"File '{current_file}' appears truncated (only {len(new_file_lines)} lines before 'No newline')")

        # Check last file
        if current_file and new_file_lines:
            file_errors = self._check_file_truncation(current_file, new_file_lines)
            errors.extend(file_errors)

        return errors

    def _check_file_truncation(self, file_path: str, content_lines: List[str]) -> List[str]:
        """Check a single file's content for truncation indicators."""
        errors = []
        content = '\n'.join(content_lines)

        # Check for unclosed quotes at end of file
        last_lines = content_lines[-3:] if len(content_lines) >= 3 else content_lines
        last_content = '\n'.join(last_lines)

        # Pattern: line ending with unclosed quote (started " but not closed)
        unclosed_quote_pattern = re.compile(r'^\s*-?\s*"[^"]*$', re.MULTILINE)
        if content_lines:
            last_line = content_lines[-1].rstrip()
            # Check if last line has unclosed double quote
            if last_line.count('"') % 2 == 1:
                errors.append(f"File '{file_path}' ends with unclosed quote: '{last_line[-50:]}'")
            # Check if last line has unclosed single quote (but not apostrophes)
            if "'" in last_line and last_line.count("'") % 2 == 1:
                # Filter out common apostrophe usage
                if not re.search(r"\w'\w", last_line):  # e.g., "don't", "it's"
                    errors.append(f"File '{file_path}' may end with unclosed quote: '{last_line[-50:]}'")

        # For YAML files, check for incomplete structure
        if file_path.endswith(('.yaml', '.yml')):
            yaml_errors = self._check_yaml_truncation(file_path, content_lines)
            errors.extend(yaml_errors)

        return errors

    def _check_yaml_truncation(self, file_path: str, content_lines: List[str]) -> List[str]:
        """Check YAML content for truncation indicators."""
        errors = []

        if not content_lines:
            return errors

        # Check if file ends abruptly mid-list item
        last_line = content_lines[-1].rstrip()
        if last_line.strip().startswith('-') and last_line.strip() == '-':
            errors.append(f"YAML file '{file_path}' ends with empty list marker")

        # Check for incomplete list item (just "- " with nothing after)
        if re.match(r'^\s*-\s*$', last_line):
            errors.append(f"YAML file '{file_path}' ends with incomplete list item")

        # Check for unclosed multi-line string indicator
        for i, line in enumerate(content_lines[-5:], len(content_lines) - 4):
            if line.rstrip().endswith('|') or line.rstrip().endswith('>'):
                # Multi-line string started but file ends soon after
                remaining = len(content_lines) - i - 1
                if remaining < 2:
                    errors.append(f"YAML file '{file_path}' ends shortly after multi-line string indicator")

        # Try to parse as YAML to catch structural issues
        try:
            import yaml
            content = '\n'.join(content_lines)
            # Lenient handling: if YAML starts with comments and no document marker, prepend '---'
            stripped = content.lstrip()
            if stripped.startswith("#") and not stripped.startswith("---"):
                content = "---\n" + content
            yaml.safe_load(content)
        except yaml.YAMLError as e:
            # Only report if it looks like truncation (not just any YAML error)
            error_str = str(e).lower()
            if 'end of stream' in error_str or 'expected' in error_str:
                errors.append(f"YAML file '{file_path}' has incomplete structure: {str(e)[:100]}")

        return errors

    def _attempt_yaml_repair_in_patch(
        self,
        patch_content: str,
        validation_errors: List[str]
    ) -> Tuple[Optional[str], str]:
        """
        Attempt to repair YAML content within a patch.

        Extracts YAML files from the patch, attempts repair using YamlRepairHelper,
        and reconstructs the patch with repaired content.

        Args:
            patch_content: The full patch content
            validation_errors: List of validation errors (used to identify which files need repair)

        Returns:
            Tuple of (repaired_patch, repair_method) or (None, "failed")
        """
        yaml_repair = YamlRepairHelper()

        # Extract YAML file paths from validation errors
        yaml_files_to_repair = set()
        for error in validation_errors:
            # Extract file path from error message like "YAML file 'path/to/file.yaml' ..."
            match = re.search(r"YAML file '([^']+)'", error)
            if match:
                yaml_files_to_repair.add(match.group(1))

        if not yaml_files_to_repair:
            return None, "no_yaml_files_identified"

        logger.info(f"[YamlRepair] Attempting repair of {len(yaml_files_to_repair)} YAML files: {yaml_files_to_repair}")

        # Parse the patch to extract file contents
        repaired_files = {}
        for file_path in yaml_files_to_repair:
            new_content = self._extract_new_content_from_patch(patch_content, file_path)
            if not new_content:
                logger.warning(f"[YamlRepair] Could not extract content for {file_path}")
                continue

            # Get old content if file exists
            full_path = self.workspace / file_path
            old_content = ""
            if full_path.exists():
                try:
                    old_content = full_path.read_text(encoding='utf-8')
                except Exception as e:
                    logger.warning(f"[YamlRepair] Could not read existing {file_path}: {e}")

            # Attempt repair
            error_msg = f"YAML validation failed for {file_path}"
            repaired, method = yaml_repair.attempt_repair(old_content, new_content, error_msg, file_path)

            if repaired:
                repaired_files[file_path] = (new_content, repaired, method)
                # Save debug info
                save_repair_debug(
                    file_path=file_path,
                    original=old_content,
                    attempted=new_content,
                    repaired=repaired,
                    error=error_msg,
                    method=method
                )
            else:
                logger.warning(f"[YamlRepair] Could not repair {file_path}")
                save_repair_debug(
                    file_path=file_path,
                    original=old_content,
                    attempted=new_content,
                    repaired=None,
                    error=error_msg,
                    method=method
                )

        if not repaired_files:
            return None, "all_repairs_failed"

        # Reconstruct the patch with repaired content
        new_patch = patch_content
        for file_path, (old_new, repaired_content, method) in repaired_files.items():
            new_patch = self._replace_file_content_in_patch(new_patch, file_path, old_new, repaired_content)

        repair_methods = "+".join(m for _, _, m in repaired_files.values())
        return new_patch, f"yaml_repair:{repair_methods}"

    def _extract_new_content_from_patch(self, patch_content: str, file_path: str) -> Optional[str]:
        """Extract the new file content for a specific file from a patch."""
        lines = patch_content.split('\n')
        content_lines = []
        in_target_file = False
        in_hunk = False

        for i, line in enumerate(lines):
            if line.startswith('diff --git'):
                # Check if this is our target file
                if f'b/{file_path}' in line or line.endswith(f' b/{file_path}'):
                    in_target_file = True
                    in_hunk = False
                    content_lines = []
                else:
                    # If we were in target file, we're done
                    if in_target_file and content_lines:
                        break
                    in_target_file = False
                continue

            if not in_target_file:
                continue

            if line.startswith('@@'):
                in_hunk = True
                continue

            if in_hunk:
                # Collect added lines (strip the '+' prefix)
                if line.startswith('+') and not line.startswith('+++'):
                    content_lines.append(line[1:])
                # For context lines in new file mode, also include them
                elif line.startswith(' '):
                    content_lines.append(line[1:])

        if content_lines:
            return '\n'.join(content_lines)
        return None

    def _replace_file_content_in_patch(
        self,
        patch_content: str,
        file_path: str,
        old_content: str,
        new_content: str
    ) -> str:
        """Replace the content of a specific file in a patch with repaired content."""
        lines = patch_content.split('\n')
        result = []
        in_target_file = False
        skip_until_next_diff = False

        i = 0
        while i < len(lines):
            line = lines[i]

            if line.startswith('diff --git'):
                # Check if this is our target file
                if f'b/{file_path}' in line or line.endswith(f' b/{file_path}'):
                    in_target_file = True
                    skip_until_next_diff = False
                    result.append(line)
                    i += 1
                    # Collect metadata lines until hunk
                    while i < len(lines) and not lines[i].startswith('@@') and not lines[i].startswith('diff --git'):
                        result.append(lines[i])
                        i += 1
                    # Now write the repaired content as a new hunk
                    if i < len(lines) and lines[i].startswith('@@'):
                        # Create proper hunk header for the repaired content
                        new_lines = new_content.split('\n')
                        result.append(f'@@ -0,0 +1,{len(new_lines)} @@')
                        for content_line in new_lines:
                            result.append(f'+{content_line}')
                        # Skip the old hunk content
                        i += 1
                        while i < len(lines) and not lines[i].startswith('diff --git'):
                            i += 1
                    continue
                else:
                    in_target_file = False
                    skip_until_next_diff = False

            if not skip_until_next_diff:
                result.append(line)
            i += 1

        return '\n'.join(result)

    def _sanitize_patch(self, patch_content: str) -> str:
        """
        Sanitize a patch to fix common formatting issues from LLM output.

        Common issues:
        - Lines in new file content missing the '+' prefix
        - Context lines missing the leading space
        - Hunk headers with incorrect line counts

        Args:
            patch_content: Raw patch content

        Returns:
            Sanitized patch content
        """
        # First fix empty file diffs
        patch_content = self._fix_empty_file_diffs(patch_content)

        lines = patch_content.split('\n')
        sanitized = []
        in_hunk = False
        in_new_file = False

        i = 0
        while i < len(lines):
            line = lines[i]

            # Track diff headers
            if line.startswith('diff --git'):
                sanitized.append(line)
                in_hunk = False
                in_new_file = False
                i += 1
                continue

            # Track new file mode
            if line.startswith('new file mode'):
                in_new_file = True
                sanitized.append(line)
                i += 1
                continue

            # Standard diff metadata lines
            if line.startswith(('index ', '---', '+++', 'similarity', 'rename ', 'deleted file')):
                sanitized.append(line)
                i += 1
                continue

            # Hunk header - we're now in a hunk
            if line.startswith('@@'):
                in_hunk = True
                sanitized.append(line)
                i += 1
                continue

            # Inside a hunk - content lines should start with +, -, or space
            if in_hunk:
                # Already properly formatted
                if line.startswith(('+', '-', ' ')):
                    sanitized.append(line)
                elif line == '':
                    # Blank lines inside hunks must carry a context prefix
                    sanitized.append(' ')
                    logger.debug("[PatchSanitize] Added context prefix to blank line inside hunk")
                elif line.isspace():
                    sanitized.append(' ')
                    logger.debug("[PatchSanitize] Normalized whitespace-only line inside hunk")
                # No newline at end of file marker
                elif line.startswith('\\ No newline'):
                    sanitized.append(line)
                # Line missing prefix - for new files, add +, otherwise add space (context)
                elif in_new_file or line.strip():
                    # For new files being created, all content lines should be additions
                    sanitized.append('+' + line)
                    logger.debug(f"Sanitized line (added +): {line[:50]}...")
                else:
                    sanitized.append(line)
            else:
                sanitized.append(line)

            i += 1

        return '\n'.join(sanitized)

    def apply_patch(self, patch_content: str, *, full_file_mode: bool = False) -> Tuple[bool, Optional[str]]:
        """
        Apply a patch to the filesystem.

        Args:
            patch_content: The patch content to apply (git diff format)
            full_file_mode: If True, allows direct write fallback for complete file contents.
                           If False (diff mode), skips direct write fallback and fails fast.
                           Per GPT_RESPONSE15: direct write only works for full-file mode.

        Returns:
            Tuple of (success: bool, error_message: Optional[str])
            - (True, None) if patch applied successfully
            - (False, error_message) if patch failed with error details
        """
        if not patch_content or not patch_content.strip():
            logger.warning("Empty patch content provided")
            return True, None  # Empty patch is technically successful

        try:
            # Sanitize patch to fix common LLM output issues
            patch_content = self._sanitize_patch(patch_content)
            # Remove existing files that conflict with new file patches
            patch_content = self._remove_existing_files_for_new_patches(patch_content)
            # Repair incorrect line numbers and counts in hunk headers
            patch_content = self._repair_hunk_headers(patch_content)
            # Normalize line endings to LF and ensure trailing newline
            patch_content = patch_content.replace('\r\n', '\n').replace('\r', '\n')
            if not patch_content.endswith('\n'):
                patch_content += '\n'

            # [Self-Troubleshoot] Backup files before modification
            files_to_modify = self._extract_files_from_patch(patch_content)

            # [Workspace Isolation] Check for protected path violations BEFORE applying
            is_valid, violations = self._validate_patch_paths(files_to_modify)
            if not is_valid:
                error_msg = f"Patch rejected - protected path violations: {', '.join(violations)}"
                logger.error(f"[Isolation] {error_msg}")
                return False, error_msg

            backups = self._backup_files(files_to_modify)
            logger.debug(f"[Integrity] Backed up {len(backups)} existing files before patch")

            # Validate patch for common LLM truncation issues
            validation_errors = self._validate_patch_quality(patch_content)
            if validation_errors:
                # Check if any errors are YAML-related - attempt repair
                yaml_errors = [e for e in validation_errors if 'YAML' in e or 'yaml' in e]
                if yaml_errors and full_file_mode:
                    logger.info(f"[YamlRepair] Detected {len(yaml_errors)} YAML validation errors, attempting repair...")
                    repaired_patch, repair_method = self._attempt_yaml_repair_in_patch(patch_content, validation_errors)
                    if repaired_patch:
                        logger.info(f"[YamlRepair] Repair succeeded via {repair_method}")
                        patch_content = repaired_patch
                        # Re-validate after repair
                        validation_errors = self._validate_patch_quality(patch_content)
                    # Pack schema validation for country packs (after structural validation passes)
                    if not validation_errors:
                        validation_errors.extend(self._validate_pack_schema_in_patch(patch_content))
                        if not validation_errors:
                            logger.info("[YamlRepair] Repaired patch passed validation")

                if validation_errors:
                    error_details = "\n".join(f"  - {err}" for err in validation_errors)
                    error_msg = f"Patch validation failed - LLM generated incomplete/truncated patch:\n{error_details}"
                    logger.error(error_msg)
                    logger.error(f"Patch content:\n{patch_content[:500]}...")
                    raise PatchApplyError(error_msg)

            # Write patch to a temporary file
            patch_file = self.workspace / "temp_patch.diff"
            logger.info(f"Writing patch to {patch_file}")

            with open(patch_file, 'w', encoding='utf-8') as f:
                f.write(patch_content)

            # Also save a debug copy
            debug_patch_file = self.workspace / "last_patch_debug.diff"
            with open(debug_patch_file, 'w', encoding='utf-8') as f:
                f.write(patch_content)

            # First, try strict apply (dry run)
            logger.info("Checking if patch can be applied (dry run)...")
            check_result = subprocess.run(
                ["git", "apply", "--check", "temp_patch.diff"],
                cwd=self.workspace,
                capture_output=True,
                text=True
            )

            use_lenient_mode = False
            use_three_way = False
            if check_result.returncode != 0:
                error_msg = check_result.stderr.strip()
                logger.warning(f"Strict patch check failed: {error_msg}")

                # Try with lenient options that handle common LLM issues
                logger.info("Retrying with lenient mode (--ignore-whitespace -C1)...")
                lenient_check = subprocess.run(
                    ["git", "apply", "--check", "--ignore-whitespace", "-C1", "temp_patch.diff"],
                    cwd=self.workspace,
                    capture_output=True,
                    text=True
                )
                if lenient_check.returncode == 0:
                    use_lenient_mode = True
                    logger.info("Lenient mode check passed")
                else:
                    # Try 3-way merge which handles line number mismatches
                    logger.warning(f"Lenient mode also failed: {lenient_check.stderr.strip()}")
                    logger.info("Retrying with 3-way merge mode (-3)...")
                    three_way_check = subprocess.run(
                        ["git", "apply", "--check", "-3", "temp_patch.diff"],
                        cwd=self.workspace,
                        capture_output=True,
                        text=True
                    )
                    if three_way_check.returncode == 0:
                        use_three_way = True
                        logger.info("3-way merge mode check passed")
                    else:
                        # All git apply modes failed
                        # Per GPT_RESPONSE15: Only use direct write fallback for full-file mode
                        if not full_file_mode:
                            logger.error("All git apply modes failed for diff-mode patch. Direct write fallback skipped (only works for full-file mode).")
                            if patch_file.exists():
                                patch_file.unlink()
                            return False, "diff_mode_patch_failed: All git apply modes failed and direct write is not available for diff patches"
                        
                        # Try direct file write as last resort (only for full-file mode)
                        logger.warning("All git apply modes failed, attempting direct file write fallback (full-file mode only)...")
                        success, files_written = self._apply_patch_directly(patch_content)
                        if success:
                            logger.info(f"Direct file write succeeded - {len(files_written)} files written")
                            for f in files_written:
                                logger.info(f"  - {f}")

                            # [Self-Troubleshoot] Validate files after direct write
                            all_valid, corrupted = self._validate_applied_files(files_written)
                            if not all_valid:
                                logger.error(f"[Integrity] Direct write corrupted {len(corrupted)} files - restoring")
                                restored, failed = self._restore_corrupted_files(corrupted, backups)
                                if patch_file.exists():
                                    patch_file.unlink()
                                return False, f"Direct file write corrupted {len(corrupted)} files (restored {restored})"

                            # [GPT_RESPONSE18] Validate content changes (symbol preservation, structural similarity)
                            # Note: For new files, backups will be empty, so validation will skip them (expected)
                            content_valid, problem_files = self._validate_content_changes(files_written, backups)
                            if not content_valid:
                                logger.warning(
                                    f"[Validation] Content validation issues in {len(problem_files)} files. "
                                    "Patch applied but may have unintended changes."
                                )

                            if patch_file.exists():
                                patch_file.unlink()
                            return True, None
                        else:
                            logger.error(f"Direct file write also failed: {three_way_check.stderr.strip()}")
                            logger.error(f"Patch content:\n{patch_content[:500]}...")
                            if patch_file.exists():
                                patch_file.unlink()
                            return False, error_msg

            # Apply patch using git
            logger.info("Applying patch to filesystem...")
            if use_three_way:
                result = subprocess.run(
                    ["git", "apply", "-3", "temp_patch.diff"],
                    cwd=self.workspace,
                    capture_output=True,
                    text=True
                )
            elif use_lenient_mode:
                result = subprocess.run(
                    ["git", "apply", "--ignore-whitespace", "-C1", "temp_patch.diff"],
                    cwd=self.workspace,
                    capture_output=True,
                    text=True
                )
            else:
                result = subprocess.run(
                    ["git", "apply", "temp_patch.diff"],
                    cwd=self.workspace,
                    capture_output=True,
                    text=True
                )

            # Clean up temp file
            if patch_file.exists():
                patch_file.unlink()

            if result.returncode != 0:
                error_msg = result.stderr.strip()
                logger.error(f"Failed to apply patch: {error_msg}")
                return False, error_msg

            # Extract files that were modified
            files_changed = self._extract_files_from_patch(patch_content)
            logger.info(f"Patch applied successfully - {len(files_changed)} files modified")
            for file_path in files_changed:
                logger.info(f"  - {file_path}")

            # [Self-Troubleshoot] Validate files after git apply
            all_valid, corrupted = self._validate_applied_files(files_changed)
            if not all_valid:
                logger.error(f"[Integrity] Git apply corrupted {len(corrupted)} files - restoring")
                restored, failed = self._restore_corrupted_files(corrupted, backups)
                return False, f"Patch corrupted {len(corrupted)} files (restored {restored}, failed {failed})"

            # [GPT_RESPONSE18] Validate content changes (symbol preservation, structural similarity)
            content_valid, problem_files = self._validate_content_changes(files_changed, backups)
            if not content_valid:
                logger.warning(
                    f"[Validation] Content validation issues in {len(problem_files)} files. "
                    "Patch applied but may have unintended changes."
                )
                # Note: We don't fail the patch here, just log warnings.
                # The caller can check logs for DATA_INTEGRITY warnings.
                # Per GPT_RESPONSE18: These are advisory checks in Phase 1.

            return True, None

        except Exception as e:
            error_msg = str(e)
            logger.error(f"Exception during patch application: {error_msg}")
            # Clean up temp file if it exists
            patch_file = self.workspace / "temp_patch.diff"
            if patch_file.exists():
                patch_file.unlink()
            return False, error_msg

    def _apply_patch_directly(self, patch_content: str) -> Tuple[bool, List[str]]:
        """
        Apply patch by directly writing files - fallback when git apply fails.

        This extracts new file content from patches and writes them directly.
        ONLY works for new files (where --- /dev/null) - partial patches for
        existing files cannot be safely applied this way.

        Args:
            patch_content: Patch content

        Returns:
            Tuple of (success, list of files written)
        """
        files_written = []
        lines = patch_content.split('\n')
        i = 0

        while i < len(lines):
            line = lines[i]

            # Look for new file diffs
            if line.startswith('diff --git'):
                parts = line.split()
                if len(parts) >= 4:
                    file_path = parts[3][2:]  # Remove 'b/' prefix

                    # Check if this is a new file (has '--- /dev/null')
                    is_new_file = False
                    hunk_start = -1
                    j = i + 1
                    while j < len(lines) and not lines[j].startswith('diff --git'):
                        if lines[j].startswith('new file mode') or lines[j] == '--- /dev/null':
                            is_new_file = True
                        if lines[j].startswith('@@'):
                            hunk_start = j
                            break
                        j += 1

                    # Only process new files - for existing files, we can't safely
                    # apply partial patches without the original file content
                    if is_new_file and hunk_start >= 0:
                        content_lines = []

                        # Handle malformed hunk header where content is on same line
                        hunk_line = lines[hunk_start]
                        hunk_header_end = hunk_line.rfind('@@')
                        if hunk_header_end > 2:
                            after_header = hunk_line[hunk_header_end + 2:].lstrip()
                            if after_header:
                                content_lines.append(after_header)

                        k = hunk_start + 1
                        while k < len(lines) and not lines[k].startswith('diff --git'):
                            line_k = lines[k]
                            # Skip additional hunk headers
                            if line_k.startswith('@@'):
                                # Handle inline content after @@
                                hunk_end = line_k.rfind('@@')
                                if hunk_end > 2:
                                    after_hunk = line_k[hunk_end + 2:].lstrip()
                                    if after_hunk:
                                        content_lines.append(after_hunk)
                                k += 1
                                continue
                            # Extract added lines (for new files, everything after + is content)
                            if line_k.startswith('+') and not line_k.startswith('+++'):
                                content_lines.append(line_k[1:])
                            k += 1

                        if content_lines:
                            full_path = self.workspace / file_path
                            try:
                                full_path.parent.mkdir(parents=True, exist_ok=True)
                                with open(full_path, 'w', encoding='utf-8') as f:
                                    f.write('\n'.join(content_lines))
                                    if not content_lines[-1] == '':
                                        f.write('\n')
                                files_written.append(file_path)
                                logger.info(f"Directly wrote file: {file_path}")
                            except Exception as e:
                                logger.error(f"Failed to write {file_path}: {e}")
                    elif not is_new_file:
                        logger.warning(f"Skipping {file_path} - cannot apply partial patch to existing file via direct write")

            i += 1

        return len(files_written) > 0, files_written

    def _extract_files_from_patch(self, patch_content: str) -> List[str]:
        """
        Extract list of files modified from patch content.

        Args:
            patch_content: Git diff/patch content

        Returns:
            List of file paths that were modified
        """
        files = []
        for line in patch_content.split('\n'):
            # Look for diff --git a/path b/path lines
            if line.startswith('diff --git'):
                parts = line.split()
                if len(parts) >= 4:
                    # Extract file path from 'a/path/to/file'
                    file_path = parts[2][2:]  # Remove 'a/' prefix
                    files.append(file_path)
            # Also look for +++ b/path lines as backup
            elif line.startswith('+++') and not line.startswith('+++ /dev/null'):
                file_path = line[6:].strip()  # Remove '+++ b/'
                if file_path and file_path not in files:
                    files.append(file_path)

        return files

    def parse_patch_stats(self, patch_content: str) -> Tuple[List[str], int, int]:
        """
        Parse patch to extract statistics.

        Args:
            patch_content: Git diff/patch content

        Returns:
            Tuple of (files_changed, lines_added, lines_removed)
        """
        files_changed = self._extract_files_from_patch(patch_content)

        lines_added = 0
        lines_removed = 0

        for line in patch_content.split('\n'):
            # Count additions (lines starting with + but not +++)
            if line.startswith('+') and not line.startswith('+++'):
                lines_added += 1
            # Count removals (lines starting with - but not ---)
            elif line.startswith('-') and not line.startswith('---'):
                lines_removed += 1

        return files_changed, lines_added, lines_removed
