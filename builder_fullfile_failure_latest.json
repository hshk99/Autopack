```json
{
  "summary": "Implement diagnostics iteration loop enhancements for Cursor-like steering with evidence requests and human response ingestion",
  "files": [
    {
      "path": "src/autopack/diagnostics/evidence_requests.py",
      "mode": "create",
      "new_content": "\"\"\"Evidence Request System for Diagnostics Iteration Loop.\n\nProvides mechanisms for Autopack to explicitly request missing evidence\nwithout token blowups, enabling Cursor-like iterative steering.\n\"\"\"\n\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Any\nfrom enum import Enum\nfrom datetime import datetime\n\n\nclass EvidenceType(Enum):\n    \"\"\"Types of evidence that can be requested.\"\"\"\n    FILE_CONTENT = \"file_content\"\n    COMMAND_OUTPUT = \"command_output\"\n    LOG_EXCERPT = \"log_excerpt\"\n    ENVIRONMENT_VAR = \"environment_var\"\n    DEPENDENCY_VERSION = \"dependency_version\"\n    GIT_STATE = \"git_state\"\n    TEST_RESULT = \"test_result\"\n    CONFIGURATION = \"configuration\"\n\n\nclass EvidencePriority(Enum):\n    \"\"\"Priority levels for evidence requests.\"\"\"\n    CRITICAL = \"critical\"  # Blocking - cannot proceed without this\n    HIGH = \"high\"  # Important - strongly recommended\n    MEDIUM = \"medium\"  # Helpful - would improve diagnosis\n    LOW = \"low\"  # Optional - nice to have\n\n\n@dataclass\nclass EvidenceRequest:\n    \"\"\"A single evidence request with context and constraints.\"\"\"\n    \n    evidence_type: EvidenceType\n    priority: EvidencePriority\n    description: str\n    rationale: str\n    \n    # Type-specific parameters\n    file_path: Optional[str] = None\n    command: Optional[str] = None\n    log_path: Optional[str] = None\n    line_range: Optional[tuple[int, int]] = None\n    env_var_name: Optional[str] = None\n    package_name: Optional[str] = None\n    \n    # Constraints to prevent token blowup\n    max_lines: int = 100\n    max_bytes: int = 10000\n    \n    # Metadata\n    request_id: str = field(default_factory=lambda: f\"req_{datetime.utcnow().timestamp()}\")\n    created_at: datetime = field(default_factory=datetime.utcnow)\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"request_id\": self.request_id,\n            \"evidence_type\": self.evidence_type.value,\n            \"priority\": self.priority.value,\n            \"description\": self.description,\n            \"rationale\": self.rationale,\n            \"file_path\": self.file_path,\n            \"command\": self.command,\n            \"log_path\": self.log_path,\n            \"line_range\": self.line_range,\n            \"env_var_name\": self.env_var_name,\n            \"package_name\": self.package_name,\n            \"max_lines\": self.max_lines,\n            \"max_bytes\": self.max_bytes,\n            \"created_at\": self.created_at.isoformat()\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"EvidenceRequest\":\n        \"\"\"Create from dictionary.\"\"\"\n        return cls(\n            evidence_type=EvidenceType(data[\"evidence_type\"]),\n            priority=EvidencePriority(data[\"priority\"]),\n            description=data[\"description\"],\n            rationale=data[\"rationale\"],\n            file_path=data.get(\"file_path\"),\n            command=data.get(\"command\"),\n            log_path=data.get(\"log_path\"),\n            line_range=tuple(data[\"line_range\"]) if data.get(\"line_range\") else None,\n            env_var_name=data.get(\"env_var_name\"),\n            package_name=data.get(\"package_name\"),\n            max_lines=data.get(\"max_lines\", 100),\n            max_bytes=data.get(\"max_bytes\", 10000),\n            request_id=data.get(\"request_id\", f\"req_{datetime.utcnow().timestamp()}\"),\n            created_at=datetime.fromisoformat(data[\"created_at\"]) if \"created_at\" in data else datetime.utcnow()\n        )\n\n\n@dataclass\nclass EvidenceRequestBatch:\n    \"\"\"A batch of evidence requests for a diagnostic phase.\"\"\"\n    \n    phase_id: str\n    run_id: str\n    requests: List[EvidenceRequest] = field(default_factory=list)\n    context: str = \"\"\n    created_at: datetime = field(default_factory=datetime.utcnow)\n    \n    def add_request(self, request: EvidenceRequest) -> None:\n        \"\"\"Add a request to the batch.\"\"\"\n        self.requests.append(request)\n    \n    def get_critical_requests(self) -> List[EvidenceRequest]:\n        \"\"\"Get all critical priority requests.\"\"\"\n        return [r for r in self.requests if r.priority == EvidencePriority.CRITICAL]\n    \n    def get_high_priority_requests(self) -> List[EvidenceRequest]:\n        \"\"\"Get all high priority requests.\"\"\"\n        return [r for r in self.requests if r.priority == EvidencePriority.HIGH]\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"phase_id\": self.phase_id,\n            \"run_id\": self.run_id,\n            \"context\": self.context,\n            \"created_at\": self.created_at.isoformat(),\n            \"requests\": [r.to_dict() for r in self.requests]\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"EvidenceRequestBatch\":\n        \"\"\"Create from dictionary.\"\"\"\n        batch = cls(\n            phase_id=data[\"phase_id\"],\n            run_id=data[\"run_id\"],\n            context=data.get(\"context\", \"\"),\n            created_at=datetime.fromisoformat(data[\"created_at\"]) if \"created_at\" in data else datetime.utcnow()\n        )\n        batch.requests = [EvidenceRequest.from_dict(r) for r in data.get(\"requests\", [])]\n        return batch\n    \n    def to_json(self) -> str:\n        \"\"\"Serialize to JSON string.\"\"\"\n        return json.dumps(self.to_dict(), indent=2)\n    \n    @classmethod\n    def from_json(cls, json_str: str) -> \"EvidenceRequestBatch\":\n        \"\"\"Deserialize from JSON string.\"\"\"\n        return cls.from_dict(json.loads(json_str))\n\n\nclass EvidenceRequestBuilder:\n    \"\"\"Builder for creating evidence requests with fluent API.\"\"\"\n    \n    def __init__(self):\n        self._requests: List[EvidenceRequest] = []\n    \n    def request_file(\n        self,\n        file_path: str,\n        description: str,\n        rationale: str,\n        priority: EvidencePriority = EvidencePriority.HIGH,\n        line_range: Optional[tuple[int, int]] = None,\n        max_lines: int = 100\n    ) -> \"EvidenceRequestBuilder\":\n        \"\"\"Request file content.\"\"\"\n        self._requests.append(EvidenceRequest(\n            evidence_type=EvidenceType.FILE_CONTENT,\n            priority=priority,\n            description=description,\n            rationale=rationale,\n            file_path=file_path,\n            line_range=line_range,\n            max_lines=max_lines\n        ))\n        return self\n    \n    def request_command(\n        self,\n        command: str,\n        description: str,\n        rationale: str,\n        priority: EvidencePriority = EvidencePriority.HIGH,\n        max_lines: int = 50\n    ) -> \"EvidenceRequestBuilder\":\n        \"\"\"Request command output.\"\"\"\n        self._requests.append(EvidenceRequest(\n            evidence_type=EvidenceType.COMMAND_OUTPUT,\n            priority=priority,\n            description=description,\n            rationale=rationale,\n            command=command,\n            max_lines=max_lines\n        ))\n        return self\n    \n    def request_log(\n        self,\n        log_path: str,\n        description: str,\n        rationale: str,\n        priority: EvidencePriority = EvidencePriority.MEDIUM,\n        line_range: Optional[tuple[int, int]] = None,\n        max_lines: int = 100\n    ) -> \"EvidenceRequestBuilder\":\n        \"\"\"Request log excerpt.\"\"\"\n        self._requests.append(EvidenceRequest(\n            evidence_type=EvidenceType.LOG_EXCERPT,\n            priority=priority,\n            description=description,\n            rationale=rationale,\n            log_path=log_path,\n            line_range=line_range,\n            max_lines=max_lines\n        ))\n        return self\n    \n    def request_env_var(\n        self,\n        env_var_name: str,\n        description: str,\n        rationale: str,\n        priority: EvidencePriority = EvidencePriority.MEDIUM\n    ) -> \"EvidenceRequestBuilder\":\n        \"\"\"Request environment variable value.\"\"\"\n        self._requests.append(EvidenceRequest(\n            evidence_type=EvidenceType.ENVIRONMENT_VAR,\n            priority=priority,\n            description=description,\n            rationale=rationale,\n            env_var_name=env_var_name,\n            max_lines=1,\n            max_bytes=1000\n        ))\n        return self\n    \n    def request_dependency(\n        self,\n        package_name: str,\n        description: str,\n        rationale: str,\n        priority: EvidencePriority = EvidencePriority.MEDIUM\n    ) -> \"EvidenceRequestBuilder\":\n        \"\"\"Request dependency version.\"\"\"\n        self._requests.append(EvidenceRequest(\n            evidence_type=EvidenceType.DEPENDENCY_VERSION,\n            priority=priority,\n            description=description,\n            rationale=rationale,\n            package_name=package_name,\n            max_lines=1,\n            max_bytes=500\n        ))\n        return self\n    \n    def build(self, phase_id: str, run_id: str, context: str = \"\") -> EvidenceRequestBatch:\n        \"\"\"Build the evidence request batch.\"\"\"\n        batch = EvidenceRequestBatch(\n            phase_id=phase_id,\n            run_id=run_id,\n            context=context\n        )\n        batch.requests = self._requests\n        return batch\n\n\ndef format_evidence_request_prompt(batch: EvidenceRequestBatch) -> str:\n    \"\"\"Format evidence request batch as a compact prompt section.\n    \n    This generates a token-efficient prompt that can be included in\n    diagnostic iterations without causing token blowup.\n    \"\"\"\n    lines = [\n        \"## Missing Evidence Requests\",\n        \"\",\n        f\"Phase: {batch.phase_id}\",\n        f\"Context: {batch.context}\",\n        \"\"\n    ]\n    \n    # Group by priority\n    critical = batch.get_critical_requests()\n    high = batch.get_high_priority_requests()\n    other = [r for r in batch.requests if r.priority not in (EvidencePriority.CRITICAL, EvidencePriority.HIGH)]\n    \n    if critical:\n        lines.append(\"### CRITICAL (blocking):\")\n        for req in critical:\n            lines.append(f\"- [{req.evidence_type.value}] {req.description}\")\n            lines.append(f\"  Rationale: {req.rationale}\")\n            if req.file_path:\n                lines.append(f\"  File: {req.file_path}\")\n            if req.command:\n                lines.append(f\"  Command: {req.command}\")\n        lines.append(\"\")\n    \n    if high:\n        lines.append(\"### HIGH (strongly recommended):\")\n        for req in high:\n            lines.append(f\"- [{req.evidence_type.value}] {req.description}\")\n            lines.append(f\"  Rationale: {req.rationale}\")\n        lines.append(\"\")\n    \n    if other:\n        lines.append(\"### OPTIONAL (helpful):\")\n        for req in other:\n            lines.append(f\"- [{req.evidence_type.value}] {req.description}\")\n        lines.append(\"\")\n    \n    lines.append(\"Please provide the requested evidence to continue diagnosis.\")\n    \n    return \"\\n\".join(lines)\n"
    },
    {
      "path": "src/autopack/diagnostics/human_response_parser.py",
      "mode": "create",
      "new_content": "\"\"\"Human Response Parser for Diagnostics Iteration Loop.\n\nProvides compact ingestion mechanism for human-provided evidence\nin response to evidence requests.\n\"\"\"\n\nimport json\nimport re\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Any\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom .evidence_requests import EvidenceType, EvidenceRequest\n\n\n@dataclass\nclass EvidenceResponse:\n    \"\"\"A single evidence response from human.\"\"\"\n    \n    request_id: str\n    evidence_type: EvidenceType\n    content: str\n    truncated: bool = False\n    error: Optional[str] = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    provided_at: datetime = field(default_factory=datetime.utcnow)\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"request_id\": self.request_id,\n            \"evidence_type\": self.evidence_type.value,\n            \"content\": self.content,\n            \"truncated\": self.truncated,\n            \"error\": self.error,\n            \"metadata\": self.metadata,\n            \"provided_at\": self.provided_at.isoformat()\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"EvidenceResponse\":\n        \"\"\"Create from dictionary.\"\"\"\n        return cls(\n            request_id=data[\"request_id\"],\n            evidence_type=EvidenceType(data[\"evidence_type\"]),\n            content=data[\"content\"],\n            truncated=data.get(\"truncated\", False),\n            error=data.get(\"error\"),\n            metadata=data.get(\"metadata\", {}),\n            provided_at=datetime.fromisoformat(data[\"provided_at\"]) if \"provided_at\" in data else datetime.utcnow()\n        )\n\n\n@dataclass\nclass HumanResponseBatch:\n    \"\"\"A batch of human-provided evidence responses.\"\"\"\n    \n    phase_id: str\n    run_id: str\n    responses: List[EvidenceResponse] = field(default_factory=list)\n    notes: str = \"\"\n    provided_at: datetime = field(default_factory=datetime.utcnow)\n    \n    def add_response(self, response: EvidenceResponse) -> None:\n        \"\"\"Add a response to the batch.\"\"\"\n        self.responses.append(response)\n    \n    def get_response(self, request_id: str) -> Optional[EvidenceResponse]:\n        \"\"\"Get response by request ID.\"\"\"\n        for response in self.responses:\n            if response.request_id == request_id:\n                return response\n        return None\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"phase_id\": self.phase_id,\n            \"run_id\": self.run_id,\n            \"notes\": self.notes,\n            \"provided_at\": self.provided_at.isoformat(),\n            \"responses\": [r.to_dict() for r in self.responses]\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"HumanResponseBatch\":\n        \"\"\"Create from dictionary.\"\"\"\n        batch = cls(\n            phase_id=data[\"phase_id\"],\n            run_id=data[\"run_id\"],\n            notes=data.get(\"notes\", \"\"),\n            provided_at=datetime.fromisoformat(data[\"provided_at\"]) if \"provided_at\" in data else datetime.utcnow()\n        )\n        batch.responses = [EvidenceResponse.from_dict(r) for r in data.get(\"responses\", [])]\n        return batch\n    \n    def to_json(self) -> str:\n        \"\"\"Serialize to JSON string.\"\"\"\n        return json.dumps(self.to_dict(), indent=2)\n    \n    @classmethod\n    def from_json(cls, json_str: str) -> \"HumanResponseBatch\":\n        \"\"\"Deserialize from JSON string.\"\"\"\n        return cls.from_dict(json.loads(json_str))\n\n\nclass HumanResponseParser:\n    \"\"\"Parser for human-provided evidence responses.\n    \n    Supports multiple input formats:\n    1. JSON format (structured)\n    2. Markdown format (human-friendly)\n    3. Plain text with markers\n    \"\"\"\n    \n    def __init__(self, max_content_bytes: int = 10000):\n        self.max_content_bytes = max_content_bytes\n    \n    def parse_json(self, json_str: str) -> HumanResponseBatch:\n        \"\"\"Parse JSON format response.\"\"\"\n        return HumanResponseBatch.from_json(json_str)\n    \n    def parse_markdown(self, markdown: str, phase_id: str, run_id: str) -> HumanResponseBatch:\n        \"\"\"Parse markdown format response.\n        \n        Expected format:\n        ```\n        # Evidence Response\n        \n        ## Request: req_123456\n        Type: file_content\n        \n        ```\n        file content here\n        ```\n        \n        ## Request: req_789012\n        Type: command_output\n        \n        ```\n        command output here\n        ```\n        \n        ## Notes\n        Additional context...\n        ```\n        \"\"\"\n        batch = HumanResponseBatch(phase_id=phase_id, run_id=run_id)\n        \n        # Extract notes section\n        notes_match = re.search(r'## Notes\\s*\\n(.+?)(?=##|$)', markdown, re.DOTALL)\n        if notes_match:\n            batch.notes = notes_match.group(1).strip()\n        \n        # Extract evidence blocks\n        request_pattern = r'## Request: (\\S+)\\s*\\nType: (\\S+)\\s*\\n```(?:\\w+)?\\n(.+?)\\n```'\n        for match in re.finditer(request_pattern, markdown, re.DOTALL):\n            request_id = match.group(1)\n            evidence_type_str = match.group(2)\n            content = match.group(3)\n            \n            # Truncate if needed\n            truncated = False\n            if len(content.encode('utf-8')) > self.max_content_bytes:\n                content = content[:self.max_content_bytes]\n                truncated = True\n            \n            try:\n                evidence_type = EvidenceType(evidence_type_str)\n            except ValueError:\n                # Unknown type, skip\n                continue\n            \n            response = EvidenceResponse(\n                request_id=request_id,\n                evidence_type=evidence_type,\n                content=content,\n                truncated=truncated\n            )\n            batch.add_response(response)\n        \n        return batch\n    \n    def parse_plain_text(self, text: str, phase_id: str, run_id: str) -> HumanResponseBatch:\n        \"\"\"Parse plain text format with markers.\n        \n        Expected format:\n        ```\n        REQUEST: req_123456\n        TYPE: file_content\n        ---\n        file content here\n        ---\n        \n        REQUEST: req_789012\n        TYPE: command_output\n        ---\n        command output here\n        ---\n        \n        NOTES:\n        Additional context...\n        ```\n        \"\"\"\n        batch = HumanResponseBatch(phase_id=phase_id, run_id=run_id)\n        \n        # Extract notes\n        notes_match = re.search(r'NOTES:\\s*\\n(.+?)$', text, re.DOTALL)\n        if notes_match:\n            batch.notes = notes_match.group(1).strip()\n        \n        # Extract evidence blocks\n        request_pattern = r'REQUEST: (\\S+)\\s*\\nTYPE: (\\S+)\\s*\\n---\\s*\\n(.+?)\\n---'\n        for match in re.finditer(request_pattern, text, re.DOTALL):\n            request_id = match.group(1)\n            evidence_type_str = match.group(2)\n            content = match.group(3).strip()\n            \n            # Truncate if needed\n            truncated = False\n            if len(content.encode('utf-8')) > self.max_content_bytes:\n                content = content[:self.max_content_bytes]\n                truncated = True\n            \n            try:\n                evidence_type = EvidenceType(evidence_type_str)\n            except ValueError:\n                continue\n            \n            response = EvidenceResponse(\n                request_id=request_id,\n                evidence_type=evidence_type,\n                content=content,\n                truncated=truncated\n            )\n            batch.add_response(response)\n        \n        return batch\n    \n    def parse_file(self, file_path: Path, phase_id: str, run_id: str) -> HumanResponseBatch:\n        \"\"\"Parse response from file (auto-detect format).\"\"\"\n        content = file_path.read_text(encoding='utf-8')\n        \n        # Try JSON first\n        if file_path.suffix == '.json':\n            return self.parse_json(content)\n        \n        # Try markdown\n        if file_path.suffix in ('.md', '.markdown'):\n            return self.parse_markdown(content, phase_id, run_id)\n        \n        # Fall back to plain text\n        return self.parse_plain_text(content, phase_id, run_id)\n    \n    def auto_parse(self, content: str, phase_id: str, run_id: str) -> HumanResponseBatch:\n        \"\"\"Auto-detect format and parse.\"\"\"\n        content = content.strip()\n        \n        # Try JSON\n        if content.startswith('{'):\n            try:\n                return self.parse_json(content)\n            except json.JSONDecodeError:\n                pass\n        \n        # Try markdown\n        if '## Request:' in content:\n            return self.parse_markdown(content, phase_id, run_id)\n        \n        # Fall back to plain text\n        return self.parse_plain_text(content, phase_id, run_id)\n\n\ndef format_response_template(requests: List[EvidenceRequest]) -> str:\n    \"\"\"Generate a markdown template for human to fill in.\n    \n    This makes it easy for humans to provide evidence in the expected format.\n    \"\"\"\n    lines = [\n        \"# Evidence Response\",\n        \"\",\n        \"Please fill in the requested evidence below.\",\n        \"\"\n    ]\n    \n    for req in requests:\n        lines.append(f\"## Request: {req.request_id}\")\n        lines.append(f\"Type: {req.evidence_type.value}\")\n        lines.append(f\"Description: {req.description}\")\n        lines.append(\"\")\n        lines.append(\"```\")\n        lines.append(\"# Paste evidence here\")\n        if req.file_path:\n            lines.append(f\"# File: {req.file_path}\")\n        if req.command:\n            lines.append(f\"# Command: {req.command}\")\n        if req.log_path:\n            lines.append(f\"# Log: {req.log_path}\")\n        lines.append(\"```\")\n        lines.append(\"\")\n    \n    lines.append(\"## Notes\")\n    lines.append(\"\")\n    lines.append(\"# Add any additional context or observations here\")\n    lines.append(\"\")\n    \n    return \"\\n\".join(lines)\n\n\ndef format_compact_summary(batch: HumanResponseBatch) -> str:\n    \"\"\"Format response batch as compact summary for prompt injection.\n    \n    This generates a token-efficient summary that can be included in\n    subsequent diagnostic iterations.\n    \"\"\"\n    lines = [\n        \"## Human-Provided Evidence\",\n        \"\"\n    ]\n    \n    for response in batch.responses:\n        lines.append(f\"### {response.evidence_type.value} ({response.request_id})\")\n        \n        if response.error:\n            lines.append(f\"Error: {response.error}\")\n        else:\n            # Truncate content for summary\n            content = response.content\n            if len(content) > 500:\n                content = content[:500] + \"... [truncated]\"\n            lines.append(f\"```\\n{content}\\n```\")\n        \n        if response.truncated:\n            lines.append(\"(Content was truncated to fit token budget)\")\n        \n        lines.append(\"\")\n    \n    if batch.notes:\n        lines.append(\"### Additional Notes\")\n        lines.append(batch.notes)\n        lines.append(\"\")\n    \n    return \"\\n\".join(lines)\n"
    },
    {
      "path": "tests/autopack/diagnostics/test_evidence_requests.py",
      "mode": "create",
      "new_content": "\"\"\"Tests for evidence request system.\"\"\"\n\nimport json\nimport pytest\nfrom datetime import datetime\n\nfrom src.autopack.diagnostics.evidence_requests import (\n    EvidenceType,\n    EvidencePriority,\n    EvidenceRequest,\n    EvidenceRequestBatch,\n    EvidenceRequestBuilder,\n    format_evidence_request_prompt\n)\n\n\nclass TestEvidenceRequest:\n    \"\"\"Tests for EvidenceRequest dataclass.\"\"\"\n    \n    def test_create_file_request(self):\n        \"\"\"Test creating a file content request.\"\"\"\n        req = EvidenceRequest(\n            evidence_type=EvidenceType.FILE_CONTENT,\n            priority=EvidencePriority.HIGH,\n            description=\"Need config file\",\n            rationale=\"To verify settings\",\n            file_path=\"config/settings.yaml\",\n            max_lines=50\n        )\n        \n        assert req.evidence_type == EvidenceType.FILE_CONTENT\n        assert req.priority == EvidencePriority.HIGH\n        assert req.file_path == \"config/settings.yaml\"\n        assert req.max_lines == 50\n    \n    def test_create_command_request(self):\n        \"\"\"Test creating a command output request.\"\"\"\n        req = EvidenceRequest(\n            evidence_type=EvidenceType.COMMAND_OUTPUT,\n            priority=EvidencePriority.CRITICAL,\n            description=\"Check Python version\",\n            rationale=\"Verify compatibility\",\n            command=\"python --version\"\n        )\n        \n        assert req.evidence_type == EvidenceType.COMMAND_OUTPUT\n        assert req.command == \"python --version\"\n    \n    def test_to_dict(self):\n        \"\"\"Test serialization to dictionary.\"\"\"\n        req = EvidenceRequest(\n            evidence_type=EvidenceType.LOG_EXCERPT,\n            priority=EvidencePriority.MEDIUM,\n            description=\"Check error logs\",\n            rationale=\"Find root cause\",\n            log_path=\"logs/error.log\",\n            line_range=(100, 200)\n        )\n        \n        data = req.to_dict()\n        \n        assert data[\"evidence_type\"] == \"log_excerpt\"\n        assert data[\"priority\"] == \"medium\"\n        assert data[\"log_path\"] == \"logs/error.log\"\n        assert data[\"line_range\"] == (100, 200)\n    \n    def test_from_dict(self):\n        \"\"\"Test deserialization from dictionary.\"\"\"\n        data = {\n            \"request_id\": \"req_123\",\n            \"evidence_type\": \"file_content\",\n            \"priority\": \"high\",\n            \"description\": \"Test\",\n            \"rationale\": \"Testing\",\n            \"file_path\": \"test.py\",\n            \"max_lines\": 100,\n            \"max_bytes\": 10000,\n            \"created_at\": datetime.utcnow().isoformat()\n        }\n        \n        req = EvidenceRequest.from_dict(data)\n        \n        assert req.request_id == \"req_123\"\n        assert req.evidence_type == EvidenceType.FILE_CONTENT\n        assert req.file_path == \"test.py\"\n\n\nclass TestEvidenceRequestBatch:\n    \"\"\"Tests for EvidenceRequestBatch.\"\"\"\n    \n    def test_create_batch(self):\n        \"\"\"Test creating a batch.\"\"\"\n        batch = EvidenceRequestBatch(\n            phase_id=\"test-phase\",\n            run_id=\"test-run\",\n            context=\"Testing context\"\n        )\n        \n        assert batch.phase_id == \"test-phase\"\n        assert batch.run_id == \"test-run\"\n        assert len(batch.requests) == 0\n    \n    def test_add_requests(self):\n        \"\"\"Test adding requests to batch.\"\"\"\n        batch = EvidenceRequestBatch(\n            phase_id=\"test-phase\",\n            run_id=\"test-run\"\n        )\n        \n        req1 = EvidenceRequest(\n            evidence_type=EvidenceType.FILE_CONTENT,\n            priority=EvidencePriority.CRITICAL,\n            description=\"File 1\",\n            rationale=\"Reason 1\"\n        )\n        req2 = EvidenceRequest(\n            evidence_type=EvidenceType.COMMAND_OUTPUT,\n            priority=EvidencePriority.HIGH,\n            description=\"Command 1\",\n            rationale=\"Reason 2\"\n        )\n        \n        batch.add_request(req1)\n        batch.add_request(req2)\n        \n        assert len(batch.requests) == 2\n    \n    def test_get_critical_requests(self):\n        \"\"\"Test filtering critical requests.\"\"\"\n        batch = EvidenceRequestBatch(\n            phase_id=\"test-phase\",\n            run_id=\"test-run\"\n        )\n        \n        batch.add_request(EvidenceRequest(\n            evidence_type=EvidenceType.FILE_CONTENT,\n            priority=EvidencePriority.CRITICAL,\n            description=\"Critical\",\n            rationale=\"Blocking\"\n        ))\n        batch.add_request(EvidenceRequest(\n            evidence_type=EvidenceType.COMMAND_OUTPUT,\n            priority=EvidencePriority.HIGH,\n            description=\"High\",\n            rationale=\"Important\"\n        ))\n        batch.add_request(EvidenceRequest(\n            evidence_type=EvidenceType.LOG_EXCERPT,\n            priority=EvidencePriority.CRITICAL,\n            description=\"Critical 2\",\n            rationale=\"Blocking 2\"\n        ))\n        \n        critical = batch.get_critical_requests()\n        assert len(critical) == 2\n        assert all(r.priority == EvidencePriority.CRITICAL for r in critical)\n    \n    def test_serialization(self):\n        \"\"\"Test JSON serialization round-trip.\"\"\"\n        batch = EvidenceRequestBatch(\n            phase_id=\"test-phase\",\n            run_id=\"test-run\",\n            context=\"Test context\"\n        )\n        \n        batch.add_request(EvidenceRequest(\n            evidence_type=EvidenceType.FILE_CONTENT,\n            priority=EvidencePriority.HIGH,\n            description=\"Test file\",\n            rationale=\"Testing\",\n            file_path=\"test.py\"\n        ))\n        \n        # Serialize\n        json_str = batch.to_json()\n        \n        # Deserialize\n        batch2 = EvidenceRequestBatch.from_json(json_str)\n        \n        assert batch2.phase_id == batch.phase_id\n        assert batch2.run_id == batch.run_id\n        assert len(batch2.requests) == len(batch.requests)\n        assert batch2.requests[0].file_path == \"test.py\"\n\n\nclass TestEvidenceRequestBuilder:\n    \"\"\"Tests for EvidenceRequestBuilder.\"\"\"\n    \n    def test_build_file_request(self):\n        \"\"\"Test building file request.\"\"\"\n        builder = EvidenceRequestBuilder()\n        batch = builder.request_file(\n            file_path=\"config.yaml\",\n            description=\"Config file\",\n            rationale=\"Check settings\"\n        ).build(phase_id=\"test\", run_id=\"run1\")\n        \n        assert len(batch.requests) == 1\n        assert batch.requests[0].evidence_type == EvidenceType.FILE_CONTENT\n        assert batch.requests[0].file_path == \"config.yaml\"\n    \n    def test_build_command_request(self):\n        \"\"\"Test building command request.\"\"\"\n        builder = EvidenceRequestBuilder()\n        batch = builder.request_command(\n            command=\"pytest --version\",\n            description=\"Check pytest\",\n            rationale=\"Verify installation\"\n        ).build(phase_id=\"test\", run_id=\"run1\")\n        \n        assert len(batch.requests) == 1\n        assert batch.requests[0].evidence_type == EvidenceType.COMMAND_OUTPUT\n        assert batch.requests[0].command == \"pytest --version\"\n    \n    def test_build_multiple_requests(self):\n        \"\"\"Test building multiple requests.\"\"\"\n        builder = EvidenceRequestBuilder()\n        batch = (builder\n            .request_file(\"file1.py\", \"File 1\", \"Reason 1\")\n            .request_command(\"cmd1\", \"Command 1\", \"Reason 2\")\n            .request_log(\"log.txt\", \"Log\", \"Reason 3\")\n            .build(phase_id=\"test\", run_id=\"run1\"))\n        \n        assert len(batch.requests) == 3\n        assert batch.requests[0].evidence_type == EvidenceType.FILE_CONTENT\n        assert batch.requests[1].evidence_type == EvidenceType.COMMAND_OUTPUT\n        assert batch.requests[2].evidence_type == EvidenceType.LOG_EXCERPT\n    \n    def test_build_env_var_request(self):\n        \"\"\"Test building environment variable request.\"\"\"\n        builder = EvidenceRequestBuilder()\n        batch = builder.request_env_var(\n            env_var_name=\"DATABASE_URL\",\n            description=\"DB connection\",\n            rationale=\"Check config\"\n        ).build(phase_id=\"test\", run_id=\"run1\")\n        \n        assert len(batch.requests) == 1\n        assert batch.requests[0].evidence_type == EvidenceType.ENVIRONMENT_VAR\n        assert batch.requests[0].env_var_name == \"DATABASE_URL\"\n    \n    def test_build_dependency_request(self):\n        \"\"\"Test building dependency version request.\"\"\"\n        builder = EvidenceRequestBuilder()\n        batch = builder.request_dependency(\n            package_name=\"pytest\",\n            description=\"Pytest version\",\n            rationale=\"Check compatibility\"\n        ).build(phase_id=\"test\", run_id=\"run1\")\n        \n        assert len(batch.requests) == 1\n        assert batch.requests[0].evidence_type == EvidenceType.DEPENDENCY_VERSION\n        assert batch.requests[0].package_name == \"pytest\"\n\n\nclass TestFormatEvidenceRequestPrompt:\n    \"\"\"Tests for prompt formatting.\"\"\"\n    \n    def test_format_empty_batch(self):\n        \"\"\"Test formatting empty batch.\"\"\"\n        batch = EvidenceRequestBatch(\n            phase_id=\"test\",\n            run_id=\"run1\",\n            context=\"Test context\"\n        )\n        \n        prompt = format_evidence_request_prompt(batch)\n        \n        assert \"Missing Evidence Requests\" in prompt\n        assert \"Test context\" in prompt\n    \n    def test_format_critical_requests(self):\n        \"\"\"Test formatting critical requests.\"\"\"\n        batch = EvidenceRequestBatch(\n            phase_id=\"test\",\n            run_id=\"run1\"\n        )\n        \n        batch.add_request(EvidenceRequest(\n            evidence_type=EvidenceType.FILE_CONTENT,\n            priority=EvidencePriority.CRITICAL,\n            description=\"Critical file\",\n            rationale=\"Blocking issue\",\n            file_path=\"critical.py\"\n        ))\n        \n        prompt = format_evidence_request_prompt(batch)\n        \n        assert \"CRITICAL (blocking)\" in prompt\n        assert \"Critical file\" in prompt\n        assert \"critical.py\" in prompt\n    \n    def test_format_mixed_priorities(self):\n        \"\"\"Test formatting requests with mixed priorities.\"\"\"\n        batch = EvidenceRequestBatch(\n            phase_id=\"test\",\n            run_id=\"run1\"\n        )\n        \n        batch.add_request(EvidenceRequest(\n            evidence_type=EvidenceType.FILE_CONTENT,\n            priority=EvidencePriority.CRITICAL,\n            description=\"Critical\",\n            rationale=\"Blocking\"\n        ))\n        batch.add_request(EvidenceRequest(\n            evidence_type=EvidenceType.COMMAND_OUTPUT,\n            priority=EvidencePriority.HIGH,\n            description=\"High priority\",\n            rationale=\"Important\"\n        ))\n        batch.add_request(EvidenceRequest(\n            evidence_type=EvidenceType.LOG_EXCERPT,\n            priority=EvidencePriority.MEDIUM,\n            description=\"Medium priority\",\n            rationale=\"Helpful\"\n        ))\n        \n        prompt = format_evidence_request_prompt(batch)\n        \n        assert \"CRITICAL (blocking)\" in prompt\n        assert \"HIGH (strongly recommended)\" in prompt\n        assert \"OPTIONAL (helpful)\" in prompt\n"
    },
    {
      "path": "tests/autopack/diagnostics/test_human_response_parser.py",
      "mode": "create",
      "new_content": "\"\"\"Tests for human response parser.\"\"\"\n\nimport json\nimport pytest\nfrom pathlib import Path\nfrom datetime import datetime\n\nfrom src.autopack.diagnostics.human_response_parser import (\n    EvidenceResponse,\n    HumanResponseBatch,\n    HumanResponseParser,\n    format_response_template,\n    format_compact_summary\n)\nfrom src.autopack.diagnostics.evidence_requests import (\n    EvidenceType,\n    EvidencePriority,\n    EvidenceRequest\n)\n\n\nclass TestEvidenceResponse:\n    \"\"\"Tests for EvidenceResponse dataclass.\"\"\"\n    \n    def test_create_response(self):\n        \"\"\"Test creating a response.\"\"\"\n        resp = EvidenceResponse(\n            request_id=\"req_123\",\n            evidence_type=EvidenceType.FILE_CONTENT,\n            content=\"file content here\"\n        )\n        \n        assert resp.request_id == \"req_123\"\n        assert resp.evidence_type == EvidenceType.FILE_CONTENT\n        assert resp.content == \"file content here\"\n        assert not resp.truncated\n        assert resp.error is None\n    \n    def test_response_with_error(self):\n        \"\"\"Test response with error.\"\"\"\n        resp = EvidenceResponse(\n            request_id=\"req_123\",\n            evidence_type=EvidenceType.COMMAND_OUTPUT,\n            content=\"\",\n            error=\"Command not found\"\n        )\n        \n        assert resp.error == \"Command not found\"\n    \n    def test_serialization(self):\n        \"\"\"Test JSON serialization.\"\"\"\n        resp = EvidenceResponse(\n            request_id=\"req_123\",\n            evidence_type=EvidenceType.FILE_CONTENT,\n            content=\"test content\",\n            truncated=True\n        )\n        \n        data = resp.to_dict()\n        resp2 = EvidenceResponse.from_dict(data)\n        \n        assert resp2.request_id == resp.request_id\n        assert resp2.evidence_type == resp.evidence_type\n        assert resp2.content == resp.content\n        assert resp2.truncated == resp.truncated\n\n\nclass TestHumanResponseBatch:\n    \"\"\"Tests for HumanResponseBatch.\"\"\"\n    \n    def test_create_batch(self):\n        \"\"\"Test creating a batch.\"\"\"\n        batch = HumanResponseBatch(\n            phase_id=\"test-phase\",\n            run_id=\"test-run\",\n            notes=\"Test notes\"\n        )\n        \n        assert batch.phase_id == \"test-phase\"\n        assert batch.run_id == \"test-run\"\n        assert batch.notes == \"Test notes\"\n        assert len(batch.responses) == 0\n    \n    def test_add_response(self):\n        \"\"\"Test adding responses.\"\"\"\n        batch = HumanResponseBatch(\n            phase_id=\"test-phase\",\n            run_id=\"test-run\"\n        )\n        \n        resp = EvidenceResponse(\n            request_id=\"req_123\",\n            evidence_type=EvidenceType.FILE_CONTENT,\n            content=\"content\"\n        )\n        \n        batch.add_response(resp)\n        assert len(batch.responses) == 1\n    \n    def test_get_response(self):\n        \"\"\"Test retrieving response by ID.\"\"\"\n        batch = HumanResponseBatch(\n            phase_id=\"test-phase\",\n            run_id=\"test-run\"\n        )\n        \n        resp1 = EvidenceResponse(\n            request_id=\"req_123\",\n            evidence_type=EvidenceType.FILE_CONTENT,\n            content=\"content1\"\n        )\n        resp2 = EvidenceResponse(\n            request_id=\"req_456\",\n            evidence_type=EvidenceType.COMMAND_OUTPUT,\n            content=\"content2\"\n        )\n        \n        batch.add_response(resp1)\n        batch.add_response(resp2)\n        \n        found = batch.get_response(\"req_456\")\n        assert found is not None\n        assert found.content == \"content2\"\n        \n        not_found = batch.get_response(\"req_999\")\n        assert not_found is None\n    \n    def test_serialization(self):\n        \"\"\"Test JSON serialization.\"\"\"\n        batch = HumanResponseBatch(\n            phase_id=\"test-phase\",\n            run_id=\"test-run\",\n            notes=\"Test notes\"\n        )\n        \n        batch.add_response(EvidenceResponse(\n            request_id=\"req_123\",\n            evidence_type=EvidenceType.FILE_CONTENT,\n            content=\"content\"\n        ))\n        \n        json_str = batch.to_json()\n        batch2 = HumanResponseBatch.from_json(json_str)\n        \n        assert batch2.phase_id == batch.phase_id\n        assert batch2.notes == batch.notes\n        assert len(batch2.responses) == 1\n\n\nclass TestHumanResponseParser:\n    \"\"\"Tests for HumanResponseParser.\"\"\"\n    \n    def test_parse_json(self):\n        \"\"\"Test parsing JSON format.\"\"\"\n        parser = HumanResponseParser()\n        \n        json_data = {\n            \"phase_id\": \"test-phase\",\n            \"run_id\": \"test-run\",\n            \"notes\": \"Test notes\",\n            \"provided_at\": datetime.utcnow().isoformat(),\n            \"responses\": [\n                {\n                    \"request_id\": \"req_123\",\n                    \"evidence_type\": \"file_content\",\n                    \"content\": \"file content\",\n                    \"truncated\": False,\n                    \"error\": None,\n                    \"metadata\": {},\n                    \"provided_at\": datetime.utcnow().isoformat()\n                }\n            ]\n        }\n        \n        batch = parser.parse_json(json.dumps(json_data))\n        \n        assert batch.phase_id == \"test-phase\"\n        assert len(batch.responses) == 1\n        assert batch.responses[0].request_id == \"req_123\"\n    \n    def test_parse_markdown(self):\n        \"\"\"Test parsing markdown format.\"\"\"\n        parser = HumanResponseParser()\n        \n        markdown = \"\"\"\n# Evidence Response\n\n## Request: req_123\nType: file_content\n\n```python\ndef test():\n    pass\n```\n\n## Request: req_456\nType: command_output\n\n```\npytest --version\npytest 7.4.0\n```\n\n## Notes\n\nAdditional context here.\n\"\"\"\n        \n        batch = parser.parse_markdown(markdown, \"test-phase\", \"test-run\")\n        \n        assert batch.phase_id == \"test-phase\"\n        assert len(batch.responses) == 2\n        assert batch.responses[0].request_id == \"req_123\"\n        assert batch.responses[1].request_id == \"req_456\"\n        assert \"Additional context\" in batch.notes\n    \n    def test_parse_plain_text(self):\n        \"\"\"Test parsing plain text format.\"\"\"\n        parser = HumanResponseParser()\n        \n        text = \"\"\"\nREQUEST: req_123\nTYPE: file_content\n---\ndef test():\n    pass\n---\n\nREQUEST: req_456\nTYPE: command_output\n---\npytest --version\n---\n\nNOTES:\nSome additional notes.\n\"\"\"\n        \n        batch = parser.parse_plain_text(text, \"test-phase\", \"test-run\")\n        \n        assert batch.phase_id == \"test-phase\"\n        assert len(batch.responses) == 2\n        assert \"Some additional notes\" in batch.notes\n    \n    def test_truncation(self):\n        \"\"\"Test content truncation.\"\"\"\n        parser = HumanResponseParser(max_content_bytes=100)\n        \n        markdown = f\"\"\"\n## Request: req_123\nType: file_content\n\n```\n{'x' * 200}\n```\n\"\"\"\n        \n        batch = parser.parse_markdown(markdown, \"test-phase\", \"test-run\")\n        \n        assert len(batch.responses) == 1\n        assert batch.responses[0].truncated\n        assert len(batch.responses[0].content) <= 100\n    \n    def test_auto_parse_json(self):\n        \"\"\"Test auto-detection of JSON format.\"\"\"\n        parser = HumanResponseParser()\n        \n        json_data = {\n            \"phase_id\": \"test-phase\",\n            \"run_id\": \"test-run\",\n            \"responses\": []\n        }\n        \n        batch = parser.auto_parse(json.dumps(json_data), \"test-phase\", \"test-run\")\n        \n        assert batch.phase_id == \"test-phase\"\n    \n    def test_auto_parse_markdown(self):\n        \"\"\"Test auto-detection of markdown format.\"\"\"\n        parser = HumanResponseParser()\n        \n        markdown = \"\"\"\n## Request: req_123\nType: file_content\n\n```\ncontent\n```\n\"\"\"\n        \n        batch = parser.auto_parse(markdown, \"test-phase\", \"test-run\")\n        \n        assert len(batch.responses) == 1\n\n\nclass TestFormatResponseTemplate:\n    \"\"\"Tests for response template formatting.\"\"\"\n    \n    def test_format_template(self):\n        \"\"\"Test generating response template.\"\"\"\n        requests = [\n            EvidenceRequest(\n                evidence_type=EvidenceType.FILE_CONTENT,\n                priority=EvidencePriority.HIGH,\n                description=\"Config file\",\n                rationale=\"Check settings\",\n                file_path=\"config.yaml\"\n            ),\n            EvidenceRequest(\n                evidence_type=EvidenceType.COMMAND_OUTPUT,\n                priority=EvidencePriority.MEDIUM,\n                description=\"Version check\",\n                rationale=\"Verify installation\",\n                command=\"python --version\"\n            )\n        ]\n        \n        template = format_response_template(requests)\n        \n        assert \"Evidence Response\" in template\n        assert \"config.yaml\" in template\n        assert \"python --version\" in template\n        assert \"## Notes\" in template\n\n\nclass TestFormatCompactSummary:\n    \"\"\"Tests for compact summary formatting.\"\"\"\n    \n    def test_format_summary(self):\n        \"\"\"Test formatting compact summary.\"\"\"\n        batch = HumanResponseBatch(\n            phase_id=\"test-phase\",\n            run_id=\"test-run\",\n            notes=\"Additional context\"\n        )\n        \n        batch.add_response(EvidenceResponse(\n            request_id=\"req_123\",\n            evidence_type=EvidenceType.FILE_CONTENT,\n            content=\"short content\"\n        ))\n        \n        summary = format_compact_summary(batch)\n        \n        assert \"Human-Provided Evidence\" in summary\n        assert \"short content\" in summary\n        assert \"Additional context\" in summary\n    \n    def test_format_summary_with_truncation(self):\n        \"\"\"Test summary with truncated content.\"\"\"\n        batch = HumanResponseBatch(\n            phase_id=\"test-phase\",\n            run_id=\"test-run\"\n        )\n        \n        batch.add_response(EvidenceResponse(\n            request_id=\"req_123\",\n            evidence_type=EvidenceType.FILE_CONTENT,\n            content=\"x\" * 1000,\n            truncated=True\n        ))\n        \n        summary = format_compact_summary(batch)\n        \n        assert \"[truncated]\" in summary\n        assert \"Content was truncated\" in summary\n    \n    def test_format_summary_with_error(self):\n        \"\"\"Test summary with error response.\"\"\"\n        batch = HumanResponseBatch(\n            phase_id=\"test-phase\",\n            run_id=\"test-run\"\n        )\n        \n        batch.add_response(EvidenceResponse(\n            request_id=\"req_123\",\n            evidence_type=EvidenceType.COMMAND_OUTPUT,\n            content=\"\",\n            error=\"Command not found\"\n        ))\n        \n        summary = format_compact_summary(batch)\n        \n        assert \"Error: Command not found\" in summary\n"
    },
    {
      "path": "docs/autopack/diagnostics_iteration_loop.md",
      "mode": "create",
      "new_content": "# Diagnostics Iteration Loop: Cursor-Like Steering\n\n**Status**: Implemented (Stage 2B)\n**Date**: 2025-12-20\n**Category**: Diagnostics Enhancement\n\n## Overview\n\nThis document describes the diagnostics iteration loop enhancements that enable Autopack to request missing evidence explicitly and ingest human responses compactly, providing Cursor-like iterative steering without token blowups.\n\n## Problem Statement\n\nPrevious diagnostics implementations suffered from:\n\n1. **Token Blowup**: Loading entire files/logs into prompts consumed excessive tokens\n2. **Missing Evidence**: No mechanism to request specific evidence from humans\n3. **Poor Iteration**: No structured way to incorporate human feedback into next diagnostic cycle\n4. **Context Loss**: Human-provided evidence not efficiently summarized for subsequent iterations\n\n## Solution: Evidence Request System\n\n### Architecture\n\nThe system consists of two main components:\n\n1. **Evidence Requests** (`evidence_requests.py`)\n   - Structured requests for specific evidence\n   - Priority levels (CRITICAL, HIGH, MEDIUM, LOW)\n   - Token budget constraints (max_lines, max_bytes)\n   - Multiple evidence types (files, commands, logs, env vars, etc.)\n\n2. **Human Response Parser** (`human_response_parser.py`)\n   - Parses human-provided evidence in multiple formats (JSON, Markdown, Plain Text)\n   - Truncates content to fit token budgets\n   - Generates compact summaries for prompt injection\n\n### Evidence Types\n\n```python\nclass EvidenceType(Enum):\n    FILE_CONTENT = \"file_content\"          # Specific file content\n    COMMAND_OUTPUT = \"command_output\"      # Shell command output\n    LOG_EXCERPT = \"log_excerpt\"            # Log file excerpt\n    ENVIRONMENT_VAR = \"environment_var\"    # Environment variable value\n    DEPENDENCY_VERSION = \"dependency_version\"  # Package version\n    GIT_STATE = \"git_state\"                # Git status/diff\n    TEST_RESULT = \"test_result\"            # Test execution output\n    CONFIGURATION = \"configuration\"        # Config file/setting\n```\n\n### Priority Levels\n\n```python\nclass EvidencePriority(Enum):\n    CRITICAL = \"critical\"  # Blocking - cannot proceed without this\n    HIGH = \"high\"          # Important - strongly recommended\n    MEDIUM = \"medium\"      # Helpful - would improve diagnosis\n    LOW = \"low\"            # Optional - nice to have\n```\n\n## Usage Examples\n\n### Creating Evidence Requests\n\n```python\nfrom src.autopack.diagnostics.evidence_requests import (\n    EvidenceRequestBuilder,\n    EvidencePriority\n)\n\n# Build a batch of evidence requests\nbuilder = EvidenceRequestBuilder()\nbatch = (builder\n    .request_file(\n        file_path=\"src/autopack/config.py\",\n        description=\"Configuration file content\",\n        rationale=\"Need to verify DATABASE_URL setting\",\n        priority=EvidencePriority.CRITICAL,\n        max_lines=50\n    )\n    .request_command(\n        command=\"pip list | grep pytest\",\n        description=\"Pytest version\",\n        rationale=\"Check if pytest is installed and version\",\n        priority=EvidencePriority.HIGH\n    )\n    .request_log(\n        log_path=\"logs/executor.log\",\n        description=\"Recent executor errors\",\n        rationale=\"Find root cause of phase failure\",\n        priority=EvidencePriority.HIGH,\n        line_range=(100, 200),\n        max_lines=100\n    )\n    .build(phase_id=\"diagnostics-phase-1\", run_id=\"test-run-123\"))\n\n# Save to file for human\nbatch_json = batch.to_json()\nwith open(\".autonomous_runs/test-run-123/evidence_requests.json\", \"w\") as f:\n    f.write(batch_json)\n```\n\n### Formatting Requests for Humans\n\n```python\nfrom src.autopack.diagnostics.evidence_requests import format_evidence_request_prompt\n\n# Generate human-readable prompt\nprompt = format_evidence_request_prompt(batch)\nprint(prompt)\n```\n\nOutput:\n```markdown\n## Missing Evidence Requests\n\nPhase: diagnostics-phase-1\nContext: Investigating phase failure\n\n### CRITICAL (blocking):\n- [file_content] Configuration file content\n  Rationale: Need to verify DATABASE_URL setting\n  File: src/autopack/config.py\n\n### HIGH (strongly recommended):\n- [command_output] Pytest version\n  Rationale: Check if pytest is installed and version\n  Command: pip list | grep pytest\n\n- [log_excerpt] Recent executor errors\n  Rationale: Find root cause of phase failure\n  Log: logs/executor.log\n\nPlease provide the requested evidence to continue diagnosis.\n```\n\n### Generating Response Template\n\n```python\nfrom src.autopack.diagnostics.human_response_parser import format_response_template\n\n# Generate template for human to fill in\ntemplate = format_response_template(batch.requests)\nwith open(\".autonomous_runs/test-run-123/evidence_response_template.md\", \"w\") as f:\n    f.write(template)\n```\n\nOutput:\n```markdown\n# Evidence Response\n\nPlease fill in the requested evidence below.\n\n## Request: req_1234567890.123\nType: file_content\nDescription: Configuration file content\n\n```\n# Paste evidence here\n# File: src/autopack/config.py\n```\n\n## Request: req_1234567890.456\nType: command_output\nDescription: Pytest version\n\n```\n# Paste evidence here\n# Command: pip list | grep pytest\n```\n\n## Notes\n\n# Add any additional context or observations here\n```\n\n### Parsing Human Responses\n\n```python\nfrom src.autopack.diagnostics.human_response_parser import HumanResponseParser\n\nparser = HumanResponseParser(max_content_bytes=10000)\n\n# Auto-detect format and parse\nwith open(\".autonomous_runs/test-run-123/evidence_response.md\", \"r\") as f:\n    content = f.read()\n\nresponse_batch = parser.auto_parse(\n    content,\n    phase_id=\"diagnostics-phase-1\",\n    run_id=\"test-run-123\"\n)\n\n# Access responses\nfor response in response_batch.responses:\n    print(f\"Request: {response.request_id}\")\n    print(f\"Type: {response.evidence_type.value}\")\n    print(f\"Content: {response.content[:100]}...\")\n    if response.truncated:\n        print(\"(Content was truncated)\")\n```\n\n### Generating Compact Summary\n\n```python\nfrom src.autopack.diagnostics.human_response_parser import format_compact_summary\n\n# Generate token-efficient summary for next iteration\nsummary = format_compact_summary(response_batch)\n\n# Inject into next diagnostic prompt\ndiagnostic_prompt = f\"\"\"\nYou are diagnosing a phase failure.\n\n{summary}\n\nBased on the evidence above, what is the root cause?\n\"\"\"\n```\n\nOutput:\n```markdown\n## Human-Provided Evidence\n\n### file_content (req_1234567890.123)\n```\nDATABASE_URL = \"postgresql://user:pass@localhost:5432/autopack\"\nSECRET_KEY = \"...\"\n```\n\n### command_output (req_1234567890.456)\n```\npytest                    7.4.0\npytest-cov                4.1.0\n```\n\n### Additional Notes\nDatabase connection is configured correctly. Pytest is installed.\n```\n\n## Integration with Diagnostics Runner\n\nThe evidence request system integrates with the existing diagnostics runner:\n\n```python