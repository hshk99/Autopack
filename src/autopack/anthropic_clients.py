"""Anthropic Claude-based Builder and Auditor implementations

Per models.yaml configuration:
- Claude Opus 4.5 for high-risk auditing
- Claude Sonnet 4.5 for progressive strategy auditing
- Complementary to OpenAI models for dual auditing

This module provides Anthropic API integration for when
ModelRouter selects Claude models based on category/quota.
"""

import os
import json
from typing import Dict, List, Optional

try:
    from anthropic import Anthropic
except ImportError:
    # Graceful degradation if anthropic package not installed
    Anthropic = None

from .llm_client import BuilderResult, AuditorResult


class AnthropicBuilderClient:
    """Builder implementation using Anthropic Claude API

    Currently used for:
    - Test generation (claude-sonnet-4-5 per models.yaml)
    - Escalation scenarios when OpenAI quota exhausted
    """

    def __init__(self, api_key: Optional[str] = None):
        """Initialize Anthropic client

        Args:
            api_key: Anthropic API key (defaults to ANTHROPIC_API_KEY env var)
        """
        if Anthropic is None:
            raise ImportError(
                "anthropic package not installed. "
                "Install with: pip install anthropic"
            )

        self.client = Anthropic(api_key=api_key or os.getenv("ANTHROPIC_API_KEY"))

    def execute_phase(
        self,
        phase_spec: Dict,
        file_context: Optional[Dict] = None,
        max_tokens: Optional[int] = None,
        model: str = "claude-sonnet-4-5",
        project_rules: Optional[List] = None,
        run_hints: Optional[List] = None
    ) -> BuilderResult:
        """Execute a phase using Claude

        Args:
            phase_spec: Phase specification
            file_context: Repository file context
            max_tokens: Token budget
            model: Claude model (claude-opus-4-5, claude-sonnet-4-5, etc.)
            project_rules: Persistent learned rules
            run_hints: Within-run hints

        Returns:
            BuilderResult with patch and metadata
        """
        try:
            # Build system prompt
            system_prompt = self._build_system_prompt()

            # Build user prompt
            user_prompt = self._build_user_prompt(
                phase_spec, file_context, project_rules, run_hints
            )

            # Call Anthropic API
            response = self.client.messages.create(
                model=model,
                max_tokens=max_tokens or 4096,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}],
                temperature=0.2
            )

            # Extract content
            content = response.content[0].text

            # Parse JSON response
            try:
                result_json = json.loads(content)
            except json.JSONDecodeError:
                # Fallback: treat as plain text patch
                result_json = {
                    "patch_content": content,
                    "summary": "Generated by Claude",
                    "files_changed": []
                }

            return BuilderResult(
                patch_content=result_json.get("patch_content", ""),
                summary=result_json.get("summary", ""),
                files_changed=result_json.get("files_changed", []),
                model_used=model,
                total_tokens=response.usage.input_tokens + response.usage.output_tokens,
                confidence_score=0.8,  # Default for Claude
                metadata={
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens,
                    "stop_reason": response.stop_reason
                }
            )

        except Exception as e:
            # Return error result
            return BuilderResult(
                patch_content="",
                summary=f"Builder error: {str(e)}",
                files_changed=[],
                model_used=model,
                total_tokens=0,
                confidence_score=0.0,
                metadata={"error": str(e)}
            )

    def _build_system_prompt(self) -> str:
        """Build system prompt for Claude Builder"""
        return """You are an expert software engineer working on an autonomous build system.

Your task is to generate code changes (patches) based on phase specifications.

Output Format (JSON):
{
  "patch_content": "git diff format patch with all changes",
  "summary": "Brief summary of changes made",
  "files_changed": ["path/to/file1.py", "path/to/file2.py"]
}

Requirements:
- Generate clean, production-quality code
- Follow best practices (type hints, docstrings, tests)
- Apply learned rules from project history
- Create git diff format patches"""

    def _build_user_prompt(
        self,
        phase_spec: Dict,
        file_context: Optional[Dict],
        project_rules: Optional[List],
        run_hints: Optional[List]
    ) -> str:
        """Build user prompt with phase details"""
        prompt_parts = [
            "# Phase Specification",
            f"Description: {phase_spec.get('description', '')}",
            f"Category: {phase_spec.get('task_category', 'general')}",
            f"Complexity: {phase_spec.get('complexity', 'medium')}",
        ]

        if phase_spec.get("acceptance_criteria"):
            prompt_parts.append("\nAcceptance Criteria:")
            for criteria in phase_spec["acceptance_criteria"]:
                prompt_parts.append(f"- {criteria}")

        if project_rules:
            prompt_parts.append("\n# Learned Rules (must follow):")
            for rule in project_rules[:10]:  # Top 10 rules
                prompt_parts.append(f"- {rule.get('rule_text', '')}")

        if run_hints:
            prompt_parts.append("\n# Hints from earlier phases:")
            for hint in run_hints[:5]:  # Recent hints
                prompt_parts.append(f"- {hint.get('hint_text', '')}")

        if file_context:
            prompt_parts.append("\n# Repository Context:")
            for file_path, content in list(file_context.items())[:5]:
                prompt_parts.append(f"\n## {file_path}")
                prompt_parts.append(f"```\n{content[:500]}...\n```")

        prompt_parts.append("\nGenerate the code changes as a JSON response.")

        return "\n".join(prompt_parts)


class AnthropicAuditorClient:
    """Auditor implementation using Anthropic Claude API

    Used for:
    - Primary auditing in progressive strategies (claude-sonnet-4-5)
    - High-risk auditing in best_first strategies (claude-opus-4-5)
    - Dual auditing partner with OpenAI
    """

    def __init__(self, api_key: Optional[str] = None):
        """Initialize Anthropic client

        Args:
            api_key: Anthropic API key (defaults to ANTHROPIC_API_KEY env var)
        """
        if Anthropic is None:
            raise ImportError(
                "anthropic package not installed. "
                "Install with: pip install anthropic"
            )

        self.client = Anthropic(api_key=api_key or os.getenv("ANTHROPIC_API_KEY"))

    def review_patch(
        self,
        patch_content: str,
        phase_spec: Dict,
        file_context: Optional[Dict] = None,
        max_tokens: Optional[int] = None,
        model: str = "claude-sonnet-4-5",
        project_rules: Optional[List] = None
    ) -> AuditorResult:
        """Review patch using Claude

        Args:
            patch_content: Git diff format patch
            phase_spec: Phase specification
            file_context: Repository context
            max_tokens: Token budget
            model: Claude model
            project_rules: Learned rules

        Returns:
            AuditorResult with issues found
        """
        try:
            # Build system prompt
            system_prompt = self._build_system_prompt()

            # Build user prompt
            user_prompt = self._build_user_prompt(
                patch_content, phase_spec, file_context, project_rules
            )

            # Call Anthropic API
            response = self.client.messages.create(
                model=model,
                max_tokens=max_tokens or 2048,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}],
                temperature=0.1  # Low temperature for consistent auditing
            )

            # Extract content
            content = response.content[0].text

            # Parse JSON response
            try:
                result_json = json.loads(content)
            except json.JSONDecodeError:
                # Fallback: treat as text review
                result_json = {
                    "approved": False,
                    "issues": [{"severity": "major", "description": content}],
                    "summary": "Review completed"
                }

            return AuditorResult(
                approved=result_json.get("approved", False),
                issues=result_json.get("issues", []),
                summary=result_json.get("summary", ""),
                model_used=model,
                total_tokens=response.usage.input_tokens + response.usage.output_tokens,
                metadata={
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens,
                    "stop_reason": response.stop_reason
                }
            )

        except Exception as e:
            # Return error result
            return AuditorResult(
                approved=False,
                issues=[{
                    "severity": "critical",
                    "category": "auditor_error",
                    "description": f"Auditor error: {str(e)}"
                }],
                summary=f"Auditor failed: {str(e)}",
                model_used=model,
                total_tokens=0,
                metadata={"error": str(e)}
            )

    def _build_system_prompt(self) -> str:
        """Build system prompt for Claude Auditor"""
        return """You are an expert code reviewer for an autonomous build system.

Your task is to review code patches for:
- Security vulnerabilities (OWASP Top 10)
- Type safety issues
- Edge cases and error handling
- Performance problems
- Best practice violations

Output Format (JSON):
{
  "approved": true/false,
  "issues": [
    {
      "severity": "minor|major|critical",
      "category": "security|types|logic|performance|style",
      "description": "Detailed issue description",
      "file_path": "path/to/file.py",
      "line_number": 42
    }
  ],
  "summary": "Brief review summary"
}

Approval Criteria:
- REJECT if any critical or major issues
- APPROVE if only minor issues or no issues"""

    def _build_user_prompt(
        self,
        patch_content: str,
        phase_spec: Dict,
        file_context: Optional[Dict],
        project_rules: Optional[List]
    ) -> str:
        """Build user prompt with patch to review"""
        prompt_parts = [
            "# Patch to Review",
            f"```diff\n{patch_content}\n```",
            f"\n# Phase Context",
            f"Category: {phase_spec.get('task_category', 'general')}",
            f"Description: {phase_spec.get('description', '')}",
        ]

        if project_rules:
            prompt_parts.append("\n# Project Rules (check compliance):")
            for rule in project_rules[:10]:
                prompt_parts.append(f"- {rule.get('rule_text', '')}")

        prompt_parts.append("\nProvide your review as a JSON response.")

        return "\n".join(prompt_parts)
