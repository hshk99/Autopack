"""
Autonomous Plan Analyzer

Analyzes any unorganized implementation plan and generates:
1. Feasibility assessment (CAN/RISKY/MANUAL classification)
2. Quality gates and validation criteria
3. Governance scope (allowed paths, approval requirements)
4. Risk classification and blockers

This meta-layer runs BEFORE autonomous execution to ensure safe,
structured implementation.
"""

import json
import re
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from enum import Enum

from autopack.llm_service import LlmService


class FeasibilityLevel(str, Enum):
    """Feasibility classification levels"""
    CAN_IMPLEMENT = "CAN_IMPLEMENT"  # 75-90% confidence
    RISKY = "RISKY"  # 45-65% confidence
    MANUAL_REQUIRED = "MANUAL_REQUIRED"  # 20-40% confidence


class RiskLevel(str, Enum):
    """Risk levels for governance"""
    LOW = "LOW"
    MEDIUM = "MEDIUM"
    HIGH = "HIGH"
    VERY_HIGH = "VERY_HIGH"


class DecisionCategory(str, Enum):
    """BUILD-113 decision categories"""
    CLEAR_FIX = "CLEAR_FIX"  # Auto-apply
    THRESHOLD = "THRESHOLD"  # Manual review
    RISKY = "RISKY"  # Manual approval required
    AMBIGUOUS = "AMBIGUOUS"  # Clarification needed


@dataclass
class PhaseAnalysis:
    """Analysis results for a single phase"""
    phase_id: str
    phase_name: str
    feasibility: FeasibilityLevel
    confidence: float  # 0.0 - 1.0
    risk_level: RiskLevel
    decision_category: DecisionCategory
    auto_apply: bool

    # Scope analysis
    estimated_files_modified: int
    core_files_affected: List[str]  # Critical files that will be modified
    allowed_paths: List[str]  # Governance allowlist
    readonly_context: List[str]  # Files to read but not modify

    # Blockers and risks
    blockers: List[str]  # Critical blockers (must resolve before execution)
    risks: List[str]  # Non-blocking risks (proceed with caution)
    dependencies: List[str]  # Other phases that must complete first

    # Quality gates
    success_criteria: List[str]
    validation_tests: List[str]
    metrics: Dict[str, str]  # e.g., {"token_reduction": ">=40%"}

    # Estimates
    estimated_duration_days: float
    complexity_score: int  # 1-10


@dataclass
class PlanAnalysisResult:
    """Complete analysis of an implementation plan"""
    run_id: str
    total_phases: int

    # Feasibility breakdown
    can_implement_count: int
    risky_count: int
    manual_required_count: int

    # Phase analyses
    phases: List[PhaseAnalysis]

    # Overall assessment
    overall_feasibility: FeasibilityLevel
    overall_confidence: float
    estimated_total_duration_days: float

    # Critical findings
    critical_blockers: List[str]  # Must resolve before starting
    infrastructure_requirements: List[str]  # Dependencies to install

    # Governance
    global_allowed_paths: List[str]  # Paths allowed across all phases
    protected_paths: List[str]  # Paths that should never be modified

    # Recommendations
    recommended_execution_order: List[str]  # Phase IDs in recommended order
    phases_requiring_manual_implementation: List[str]

    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization"""
        return asdict(self)


class PlanAnalyzer:
    """
    Analyzes implementation plans and generates structured execution configs.

    Uses LLM to:
    1. Assess feasibility of each phase
    2. Identify files that will be modified
    3. Classify risk levels
    4. Generate quality gates
    5. Detect blockers and dependencies
    """

    def __init__(
        self,
        # Phase D compatibility (ManifestGenerator passes these)
        repo_scanner: Any = None,
        pattern_matcher: Any = None,
        llm_service: Optional[LlmService] = None,
        workspace: Optional[Path] = None,
    ):
        self.repo_scanner = repo_scanner
        self.pattern_matcher = pattern_matcher
        # NOTE: LlmService in this repo typically requires a DB session; we keep
        # llm initialization lazy so legacy imports don't explode.
        self.llm = llm_service
        self.workspace = workspace or Path.cwd()

    async def analyze_phase(
        self,
        phase_spec: Dict,
        context: Optional[str] = None,
    ):
        """
        BUILD-124 Phase D: Analyze a single phase with grounded context.

        This is intentionally lightweight: Phase D uses this method from
        `ManifestGenerator` and tests patch `autopack.llm_service.LlmService`.
        """
        # Lazy init llm so tests can patch `autopack.llm_service.LlmService` cleanly.
        if self.llm is None:
            # IMPORTANT: do NOT instantiate via the imported name `LlmService` here.
            # The integration test patches `autopack.llm_service.LlmService`, and patching
            # wonâ€™t affect this module-level binding once imported. Import the module and
            # instantiate off it so the patch takes effect.
            from autopack import llm_service as llm_service_module

            try:
                self.llm = llm_service_module.LlmService()  # type: ignore[call-arg]
            except TypeError as e:
                raise RuntimeError(
                    "LlmService requires additional constructor args in this environment. "
                    "Pass `llm_service=` explicitly."
                ) from e

        phase_id = phase_spec.get("phase_id", "unknown")
        goal = phase_spec.get("goal", "")
        description = phase_spec.get("description", "")

        prompt = "\n".join(
            [
                "You are a software architect doing feasibility triage.",
                "",
                f"Phase ID: {phase_id}",
                f"Goal: {goal}",
                f"Description: {description}",
                "",
                context or "No additional context provided.",
            ]
        )

        response = await self.llm.call_llm(
            prompt=prompt,
            model="claude-sonnet-4-5",
            temperature=0.1,
        )

        # Best-effort parse for the integration tests (they don't assert content,
        # but we provide a stable shape for metadata attachment).
        feasible = True
        confidence = 0.0

        m = re.search(r"CONFIDENCE:\s*([0-9]*\.?[0-9]+)", response or "", flags=re.IGNORECASE)
        if m:
            try:
                confidence = float(m.group(1))
            except Exception:
                confidence = 0.0

        m2 = re.search(r"FEASIBILITY:\s*(HIGH|MEDIUM|LOW)", response or "", flags=re.IGNORECASE)
        if m2:
            feasible = m2.group(1).upper() != "LOW"

        # Return a simple object with expected attributes (duck-typed).
        return type(
            "PhaseFeasibilityAnalysis",
            (),
            {
                "feasible": feasible,
                "confidence": confidence,
                "concerns": [],
                "recommendations": [],
                "recommended_scope": [],
            },
        )()

    async def analyze_plan(
        self,
        run_id: str,
        phases: List[Dict],
        context: Optional[str] = None,
    ) -> PlanAnalysisResult:
        """
        Analyze an implementation plan with multiple phases.

        Args:
            run_id: Unique run identifier
            phases: List of phase specifications (can be unstructured)
            context: Optional context about the codebase/project

        Returns:
            PlanAnalysisResult with complete analysis
        """

        # Analyze each phase
        phase_analyses = []
        for phase_spec in phases:
            analysis = await self._analyze_phase(phase_spec, context)
            phase_analyses.append(analysis)

        # Aggregate results
        can_count = sum(1 for p in phase_analyses if p.feasibility == FeasibilityLevel.CAN_IMPLEMENT)
        risky_count = sum(1 for p in phase_analyses if p.feasibility == FeasibilityLevel.RISKY)
        manual_count = sum(1 for p in phase_analyses if p.feasibility == FeasibilityLevel.MANUAL_REQUIRED)

        # Determine overall feasibility
        if manual_count > len(phases) * 0.3:  # >30% manual
            overall_feasibility = FeasibilityLevel.MANUAL_REQUIRED
        elif risky_count > len(phases) * 0.5:  # >50% risky
            overall_feasibility = FeasibilityLevel.RISKY
        else:
            overall_feasibility = FeasibilityLevel.CAN_IMPLEMENT

        # Calculate overall confidence (weighted average)
        total_confidence = sum(p.confidence for p in phase_analyses)
        overall_confidence = total_confidence / len(phase_analyses) if phase_analyses else 0.0

        # Aggregate critical blockers
        critical_blockers = []
        for p in phase_analyses:
            critical_blockers.extend(p.blockers)
        critical_blockers = list(set(critical_blockers))  # Deduplicate

        # Extract infrastructure requirements
        infrastructure_requirements = await self._extract_infrastructure_requirements(phase_analyses)

        # Generate global governance scope
        global_allowed_paths, protected_paths = self._generate_global_governance(phase_analyses)

        # Recommend execution order (topological sort by dependencies)
        execution_order = self._recommend_execution_order(phase_analyses)

        # Identify manual-only phases
        manual_phases = [p.phase_id for p in phase_analyses if p.feasibility == FeasibilityLevel.MANUAL_REQUIRED]

        # Total duration
        total_duration = sum(p.estimated_duration_days for p in phase_analyses)

        return PlanAnalysisResult(
            run_id=run_id,
            total_phases=len(phases),
            can_implement_count=can_count,
            risky_count=risky_count,
            manual_required_count=manual_count,
            phases=phase_analyses,
            overall_feasibility=overall_feasibility,
            overall_confidence=overall_confidence,
            estimated_total_duration_days=total_duration,
            critical_blockers=critical_blockers,
            infrastructure_requirements=infrastructure_requirements,
            global_allowed_paths=global_allowed_paths,
            protected_paths=protected_paths,
            recommended_execution_order=execution_order,
            phases_requiring_manual_implementation=manual_phases,
        )

    async def _analyze_phase(
        self,
        phase_spec: Dict,
        context: Optional[str] = None,
    ) -> PhaseAnalysis:
        """
        Analyze a single phase using LLM.

        Generates:
        - Feasibility assessment
        - File scope analysis
        - Risk classification
        - Quality gates
        - Blockers and dependencies
        """

        # Build analysis prompt
        prompt = self._build_phase_analysis_prompt(phase_spec, context)

        # Call LLM for structured analysis
        response = await self.llm.call_llm(
            prompt=prompt,
            model="claude-sonnet-4-5",  # Use Sonnet 4.5 for analysis (BUILD-124)
            temperature=0.1,  # Low temp for consistent analysis
        )

        # Parse LLM response into structured PhaseAnalysis
        analysis = self._parse_phase_analysis_response(response, phase_spec)

        return analysis

    def _build_phase_analysis_prompt(
        self,
        phase_spec: Dict,
        context: Optional[str] = None,
    ) -> str:
        """Build LLM prompt for phase analysis"""

        phase_goal = phase_spec.get("goal", "")
        phase_name = phase_spec.get("phase_name", phase_spec.get("phase_id", "Unknown"))
        phase_description = phase_spec.get("description", "")

        prompt = f"""You are an expert software architect analyzing an implementation phase for autonomous execution.

# Phase to Analyze

**Phase Name:** {phase_name}
**Goal:** {phase_goal}
**Description:** {phase_description}

# Codebase Context

{context or "No additional context provided."}

# Your Task

Analyze this phase and provide a structured assessment in JSON format with the following fields:

1. **feasibility**: One of ["CAN_IMPLEMENT", "RISKY", "MANUAL_REQUIRED"]
   - CAN_IMPLEMENT: 75-90% confidence, autonomous execution safe
   - RISKY: 45-65% confidence, supervised execution recommended
   - MANUAL_REQUIRED: 20-40% confidence, manual implementation only

2. **confidence**: Float 0.0-1.0 representing confidence level

3. **risk_level**: One of ["LOW", "MEDIUM", "HIGH", "VERY_HIGH"]
   - Consider impact on core functionality, governance model changes, external dependencies

4. **decision_category**: One of ["CLEAR_FIX", "THRESHOLD", "RISKY", "AMBIGUOUS"]
   - CLEAR_FIX: Auto-apply safe (e.g., data files, isolated features)
   - THRESHOLD: Manual review needed (e.g., medium complexity changes)
   - RISKY: Manual approval required (e.g., governance changes, core architecture)
   - AMBIGUOUS: Clarification needed (e.g., unclear requirements)

5. **auto_apply**: Boolean - can patches be auto-applied without approval?

6. **estimated_files_modified**: Integer - estimated number of files to modify

7. **core_files_affected**: List of critical files that will be modified
   - Include file paths if known, or patterns if uncertain

8. **allowed_paths**: List of file paths/patterns allowed for modification
   - Be specific but not overly restrictive

9. **readonly_context**: List of files to read for context but NOT modify

10. **blockers**: List of critical blockers that must be resolved before execution
    - e.g., "Hash embeddings not semantic - Phase 1 requires semantic embeddings"
    - e.g., "External API subscription required (Morph API, $100/month)"

11. **risks**: List of non-blocking risks to be aware of
    - e.g., "Frontend integration complexity may increase duration"
    - e.g., "Symbol preservation may reject valid patches"

12. **dependencies**: List of phase IDs that must complete before this one

13. **success_criteria**: List of validation criteria for successful completion
    - Be specific and measurable

14. **validation_tests**: List of test commands to run for validation
    - e.g., "pytest tests/autopack/test_lovable.py -v"

15. **metrics**: Dictionary of success metrics
    - e.g., {{"token_reduction": ">=40%", "patch_success": ">=85%"}}

16. **estimated_duration_days**: Float - realistic time estimate in days

17. **complexity_score**: Integer 1-10
    - 1-3: Simple (single file, isolated change)
    - 4-6: Moderate (multiple files, some integration)
    - 7-9: Complex (architecture changes, broad impact)
    - 10: Very complex (requires external expertise)

# Output Format

Respond with ONLY a valid JSON object. No markdown, no explanation, just JSON.

Example:
{{
  "feasibility": "CAN_IMPLEMENT",
  "confidence": 0.85,
  "risk_level": "MEDIUM",
  "decision_category": "THRESHOLD",
  "auto_apply": false,
  "estimated_files_modified": 3,
  "core_files_affected": ["src/autopack/memory/embeddings.py"],
  "allowed_paths": ["src/autopack/lovable/", "src/autopack/memory/embeddings.py"],
  "readonly_context": ["src/autopack/memory/memory_service.py"],
  "blockers": [],
  "risks": ["Embedding backend auto-detection may fail in edge cases"],
  "dependencies": [],
  "success_criteria": ["Semantic similarity test passes: cosine_sim(related) > 0.7"],
  "validation_tests": ["pytest tests/autopack/memory/test_embeddings.py -v"],
  "metrics": {{"semantic_similarity_related": ">0.7", "semantic_similarity_unrelated": "<0.5"}},
  "estimated_duration_days": 2.0,
  "complexity_score": 5
}}

Analyze the phase now:
"""

        return prompt

    def _parse_phase_analysis_response(
        self,
        response: str,
        phase_spec: Dict,
    ) -> PhaseAnalysis:
        """Parse LLM response into PhaseAnalysis object"""

        try:
            data = json.loads(response.strip())
        except json.JSONDecodeError:
            # Fallback: extract JSON from markdown if LLM wrapped it
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
                data = json.loads(json_str)
            else:
                raise ValueError(f"Invalid JSON response from LLM: {response}")

        return PhaseAnalysis(
            phase_id=phase_spec.get("phase_id", "unknown"),
            phase_name=phase_spec.get("phase_name", "Unknown Phase"),
            feasibility=FeasibilityLevel(data["feasibility"]),
            confidence=data["confidence"],
            risk_level=RiskLevel(data["risk_level"]),
            decision_category=DecisionCategory(data["decision_category"]),
            auto_apply=data["auto_apply"],
            estimated_files_modified=data["estimated_files_modified"],
            core_files_affected=data["core_files_affected"],
            allowed_paths=data["allowed_paths"],
            readonly_context=data["readonly_context"],
            blockers=data["blockers"],
            risks=data["risks"],
            dependencies=data.get("dependencies", []),
            success_criteria=data["success_criteria"],
            validation_tests=data["validation_tests"],
            metrics=data["metrics"],
            estimated_duration_days=data["estimated_duration_days"],
            complexity_score=data["complexity_score"],
        )
    async def _extract_infrastructure_requirements(
        self,
        phase_analyses: List[PhaseAnalysis],
    ) -> List[str]:
        """Extract infrastructure requirements from blockers and risks"""

        requirements = set()

        for phase in phase_analyses:
            # Extract from blockers
            for blocker in phase.blockers:
                if "sentence-transformers" in blocker.lower():
                    requirements.add("pip install sentence-transformers torch")
                elif "openai" in blocker.lower() and "embedding" in blocker.lower():
                    requirements.add("OpenAI API key (OPENAI_API_KEY) for embeddings")
                elif "morph" in blocker.lower():
                    requirements.add("Morph API subscription ($100/month)")
                elif "postgresql" in blocker.lower():
                    requirements.add("PostgreSQL database")
                elif "qdrant" in blocker.lower():
                    requirements.add("Qdrant vector store (or FAISS fallback)")

        return sorted(list(requirements))

    def _generate_global_governance(
        self,
        phase_analyses: List[PhaseAnalysis],
    ) -> Tuple[List[str], List[str]]:
        """Generate global allowed paths and protected paths"""

        # Aggregate all allowed paths
        allowed_paths = set()
        for phase in phase_analyses:
            allowed_paths.update(phase.allowed_paths)

        # Define protected paths (never modify)
        protected_paths = [
            ".git/",
            ".autonomous_runs/*/gold_set/",  # Never modify gold set
            "venv/",
            "node_modules/",
            ".pytest_cache/",
            "__pycache__/",
        ]

        return sorted(list(allowed_paths)), protected_paths

    def _recommend_execution_order(
        self,
        phase_analyses: List[PhaseAnalysis],
    ) -> List[str]:
        """
        Recommend phase execution order using topological sort.

        Considers:
        - Explicit dependencies
        - Feasibility (CAN_IMPLEMENT first, MANUAL last)
        - Complexity (simpler phases first for quick wins)
        """

        # Build dependency graph
        graph = {p.phase_id: p.dependencies for p in phase_analyses}
        phase_map = {p.phase_id: p for p in phase_analyses}

        # Topological sort (Kahn's algorithm)
        in_degree = {p.phase_id: len(p.dependencies) for p in phase_analyses}
        queue = [p.phase_id for p in phase_analyses if len(p.dependencies) == 0]
        order = []

        while queue:
            # Sort queue by priority: CAN_IMPLEMENT > RISKY > MANUAL, then by complexity
            queue.sort(key=lambda pid: (
                0 if phase_map[pid].feasibility == FeasibilityLevel.CAN_IMPLEMENT else
                1 if phase_map[pid].feasibility == FeasibilityLevel.RISKY else 2,
                phase_map[pid].complexity_score
            ))

            current = queue.pop(0)
            order.append(current)

            # Reduce in-degree for dependent phases
            for phase_id, deps in graph.items():
                if current in deps:
                    in_degree[phase_id] -= 1
                    if in_degree[phase_id] == 0:
                        queue.append(phase_id)

        return order


async def analyze_implementation_plan(
    run_id: str,
    plan_file: Path,
    workspace: Optional[Path] = None,
    output_file: Optional[Path] = None,
) -> PlanAnalysisResult:
    """
    High-level function to analyze an implementation plan from a file.

    Args:
        run_id: Unique run identifier
        plan_file: Path to implementation plan (JSON, YAML, or Markdown)
        workspace: Project workspace directory
        output_file: Optional output file for analysis results

    Returns:
        PlanAnalysisResult
    """

    # Load plan
    if plan_file.suffix == ".json":
        with open(plan_file) as f:
            plan_data = json.load(f)
    elif plan_file.suffix in [".yaml", ".yml"]:
        import yaml
        with open(plan_file) as f:
            plan_data = yaml.safe_load(f)
    else:
        raise ValueError(f"Unsupported plan file format: {plan_file.suffix}")

    # Extract phases and context
    phases = plan_data.get("phases", [])
    context = plan_data.get("context", plan_data.get("description", ""))

    # Analyze
    analyzer = PlanAnalyzer(workspace=workspace)
    result = await analyzer.analyze_plan(run_id, phases, context)

    # Save results if output file specified
    if output_file:
        with open(output_file, "w") as f:
            json.dump(result.to_dict(), f, indent=2)

    return result

