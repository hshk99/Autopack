```json
{
  "summary": "Implement batch upload & processing with background job queue, progress tracking, batch classification, and frontend drag-and-drop UI. Added batch upload service, job queue integration, progress tracking endpoints, batch classification logic, and enhanced frontend with drag-and-drop interface.",
  "files": [
    {
      "path": "fileorganizer/backend/app/services/batch_service.py",
      "mode": "create",
      "new_content": "\"\"\"\nBatch Upload Service - Handle multiple file uploads with validation\n\"\"\"\nimport logging\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nfrom sqlalchemy.orm import Session\nfrom app.models.document import Document, ProcessingStatus\nfrom app.core.config import settings\nfrom app.core.security import validate_filename, sanitize_filename\nimport hashlib\nimport uuid\n\nlogger = logging.getLogger(__name__)\n\n\nclass BatchUploadService:\n    \"\"\"Service for handling batch file uploads\"\"\"\n\n    def __init__(self, db: Session):\n        self.db = db\n        self.upload_dir = Path(\"uploads\")\n        self.temp_dir = Path(\"temp_uploads\")\n        self.upload_dir.mkdir(exist_ok=True)\n        self.temp_dir.mkdir(exist_ok=True)\n\n    def validate_batch(self, files: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        Validate batch of files before processing\n        Returns: {\"valid\": [...], \"invalid\": [{\"filename\": str, \"reason\": str}]}\n        \"\"\"\n        valid_files = []\n        invalid_files = []\n\n        for file_info in files:\n            filename = file_info.get(\"filename\", \"\")\n            file_size = file_info.get(\"size\", 0)\n\n            # Validate filename\n            if not validate_filename(filename):\n                invalid_files.append({\n                    \"filename\": filename,\n                    \"reason\": \"Invalid filename (path traversal detected or too long)\"\n                })\n                continue\n\n            # Validate file size\n            max_size_bytes = settings.MAX_FILE_SIZE_MB * 1024 * 1024\n            if file_size > max_size_bytes:\n                invalid_files.append({\n                    \"filename\": filename,\n                    \"reason\": f\"File too large: {file_size} bytes (max: {max_size_bytes})\"\n                })\n                continue\n\n            # Validate file type\n            file_type = Path(filename).suffix.lower()\n            if file_type not in settings.SUPPORTED_FORMATS:\n                invalid_files.append({\n                    \"filename\": filename,\n                    \"reason\": f\"Unsupported file type: {file_type}\"\n                })\n                continue\n\n            valid_files.append(file_info)\n\n        return {\n            \"valid\": valid_files,\n            \"invalid\": invalid_files,\n            \"total\": len(files),\n            \"valid_count\": len(valid_files),\n            \"invalid_count\": len(invalid_files)\n        }\n\n    def save_temp_file(self, filename: str, file_data: bytes) -> Path:\n        \"\"\"\n        Save file to temporary storage with unique identifier\n        Returns: Path to saved file\n        \"\"\"\n        # Sanitize filename\n        safe_filename = sanitize_filename(filename)\n\n        # Generate unique identifier\n        file_hash = hashlib.md5(file_data).hexdigest()[:8]\n        unique_id = str(uuid.uuid4())[:8]\n        temp_filename = f\"{unique_id}_{file_hash}_{safe_filename}\"\n\n        # Save to temp directory\n        temp_path = self.temp_dir / temp_filename\n        with open(temp_path, \"wb\") as f:\n            f.write(file_data)\n\n        logger.info(f\"Saved temp file: {temp_filename}\")\n        return temp_path\n\n    def move_to_permanent(self, temp_path: Path, final_filename: str) -> Path:\n        \"\"\"\n        Move file from temporary to permanent storage\n        Returns: Path to permanent file\n        \"\"\"\n        safe_filename = sanitize_filename(final_filename)\n        permanent_path = self.upload_dir / safe_filename\n\n        # Handle duplicate filenames\n        counter = 1\n        while permanent_path.exists():\n            name_parts = safe_filename.rsplit(\".\", 1)\n            if len(name_parts) == 2:\n                permanent_path = self.upload_dir / f\"{name_parts[0]}_{counter}.{name_parts[1]}\"\n            else:\n                permanent_path = self.upload_dir / f\"{safe_filename}_{counter}\"\n            counter += 1\n\n        temp_path.rename(permanent_path)\n        logger.info(f\"Moved to permanent storage: {permanent_path}\")\n        return permanent_path\n\n    def cleanup_temp_files(self, temp_paths: List[Path]) -> None:\n        \"\"\"\n        Clean up temporary files after processing\n        \"\"\"\n        for temp_path in temp_paths:\n            try:\n                if temp_path.exists():\n                    temp_path.unlink()\n                    logger.info(f\"Cleaned up temp file: {temp_path}\")\n            except Exception as e:\n                logger.error(f\"Failed to cleanup temp file {temp_path}: {str(e)}\")\n\n    def create_document_records(\n        self,\n        file_paths: List[Path],\n        batch_id: str\n    ) -> List[Document]:\n        \"\"\"\n        Create database records for uploaded files\n        Returns: List of created Document objects\n        \"\"\"\n        documents = []\n\n        for file_path in file_paths:\n            try:\n                file_size = file_path.stat().st_size\n                file_type = file_path.suffix.lower()\n\n                document = Document(\n                    filename=file_path.name,\n                    original_path=str(file_path),\n                    file_size=file_size,\n                    file_type=file_type,\n                    status=ProcessingStatus.PENDING\n                )\n                self.db.add(document)\n                documents.append(document)\n\n            except Exception as e:\n                logger.error(f\"Failed to create document record for {file_path}: {str(e)}\")\n                continue\n\n        try:\n            self.db.commit()\n            for doc in documents:\n                self.db.refresh(doc)\n            logger.info(f\"Created {len(documents)} document records for batch {batch_id}\")\n        except Exception as e:\n            self.db.rollback()\n            logger.error(f\"Failed to commit document records: {str(e)}\")\n            raise\n\n        return documents\n\n    def handle_upload_failure(\n        self,\n        temp_paths: List[Path],\n        document_ids: List[int]\n    ) -> None:\n        \"\"\"\n        Handle cleanup after upload failure\n        \"\"\"\n        # Clean up temp files\n        self.cleanup_temp_files(temp_paths)\n\n        # Mark documents as failed\n        try:\n            for doc_id in document_ids:\n                document = self.db.query(Document).filter(Document.id == doc_id).first()\n                if document:\n                    document.status = ProcessingStatus.FAILED\n            self.db.commit()\n        except Exception as e:\n            logger.error(f\"Failed to mark documents as failed: {str(e)}\")\n            self.db.rollback()\n"
    },
    {
      "path": "fileorganizer/backend/app/services/job_queue_service.py",
      "mode": "create",
      "new_content": "\"\"\"\nJob Queue Service - Background job processing without external dependencies\n\"\"\"\nimport logging\nimport threading\nimport queue\nimport time\nfrom typing import Dict, Any, Callable, Optional\nfrom datetime import datetime\nfrom enum import Enum\nimport uuid\n\nlogger = logging.getLogger(__name__)\n\n\nclass JobStatus(str, Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n\n\nclass Job:\n    \"\"\"Represents a background job\"\"\"\n\n    def __init__(\n        self,\n        job_id: str,\n        job_type: str,\n        task_func: Callable,\n        args: tuple = (),\n        kwargs: dict = None\n    ):\n        self.job_id = job_id\n        self.job_type = job_type\n        self.task_func = task_func\n        self.args = args\n        self.kwargs = kwargs or {}\n        self.status = JobStatus.PENDING\n        self.result = None\n        self.error = None\n        self.created_at = datetime.now()\n        self.started_at: Optional[datetime] = None\n        self.completed_at: Optional[datetime] = None\n        self.progress = 0\n        self.total = 0\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert job to dictionary for API responses\"\"\"\n        return {\n            \"job_id\": self.job_id,\n            \"job_type\": self.job_type,\n            \"status\": self.status,\n            \"progress\": self.progress,\n            \"total\": self.total,\n            \"result\": self.result,\n            \"error\": self.error,\n            \"created_at\": self.created_at.isoformat(),\n            \"started_at\": self.started_at.isoformat() if self.started_at else None,\n            \"completed_at\": self.completed_at.isoformat() if self.completed_at else None\n        }\n\n\nclass JobQueueService:\n    \"\"\"Simple in-memory job queue service\"\"\"\n\n    _instance = None\n    _lock = threading.Lock()\n\n    def __new__(cls):\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = super().__new__(cls)\n                    cls._instance._initialized = False\n        return cls._instance\n\n    def __init__(self):\n        if self._initialized:\n            return\n\n        self.job_queue = queue.Queue()\n        self.jobs: Dict[str, Job] = {}\n        self.workers: list = []\n        self.num_workers = 2\n        self.running = False\n        self._initialized = True\n\n    def start(self) -> None:\n        \"\"\"Start worker threads\"\"\"\n        if self.running:\n            return\n\n        self.running = True\n        for i in range(self.num_workers):\n            worker = threading.Thread(target=self._worker, daemon=True, name=f\"Worker-{i}\")\n            worker.start()\n            self.workers.append(worker)\n\n        logger.info(f\"Started {self.num_workers} worker threads\")\n\n    def stop(self) -> None:\n        \"\"\"Stop worker threads\"\"\"\n        self.running = False\n        logger.info(\"Stopping job queue service\")\n\n    def enqueue(\n        self,\n        job_type: str,\n        task_func: Callable,\n        args: tuple = (),\n        kwargs: dict = None\n    ) -> str:\n        \"\"\"\n        Add job to queue\n        Returns: job_id\n        \"\"\"\n        job_id = str(uuid.uuid4())\n        job = Job(job_id, job_type, task_func, args, kwargs)\n        self.jobs[job_id] = job\n        self.job_queue.put(job)\n\n        logger.info(f\"Enqueued job {job_id} of type {job_type}\")\n        return job_id\n\n    def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Get job status by ID\"\"\"\n        job = self.jobs.get(job_id)\n        if job:\n            return job.to_dict()\n        return None\n\n    def update_progress(self, job_id: str, progress: int, total: int) -> None:\n        \"\"\"Update job progress\"\"\"\n        job = self.jobs.get(job_id)\n        if job:\n            job.progress = progress\n            job.total = total\n\n    def _worker(self) -> None:\n        \"\"\"Worker thread that processes jobs\"\"\"\n        while self.running:\n            try:\n                job = self.job_queue.get(timeout=1)\n            except queue.Empty:\n                continue\n\n            try:\n                logger.info(f\"Processing job {job.job_id}\")\n                job.status = JobStatus.RUNNING\n                job.started_at = datetime.now()\n\n                # Execute task\n                result = job.task_func(*job.args, **job.kwargs)\n\n                job.status = JobStatus.COMPLETED\n                job.result = result\n                job.completed_at = datetime.now()\n                logger.info(f\"Job {job.job_id} completed successfully\")\n\n            except Exception as e:\n                job.status = JobStatus.FAILED\n                job.error = str(e)\n                job.completed_at = datetime.now()\n                logger.error(f\"Job {job.job_id} failed: {str(e)}\")\n\n            finally:\n                self.job_queue.task_done()\n\n    def cleanup_old_jobs(self, max_age_hours: int = 24) -> None:\n        \"\"\"Remove old completed/failed jobs\"\"\"\n        cutoff_time = datetime.now().timestamp() - (max_age_hours * 3600)\n        jobs_to_remove = []\n\n        for job_id, job in self.jobs.items():\n            if job.status in [JobStatus.COMPLETED, JobStatus.FAILED]:\n                if job.completed_at and job.completed_at.timestamp() < cutoff_time:\n                    jobs_to_remove.append(job_id)\n\n        for job_id in jobs_to_remove:\n            del self.jobs[job_id]\n\n        if jobs_to_remove:\n            logger.info(f\"Cleaned up {len(jobs_to_remove)} old jobs\")\n\n\n# Global instance\njob_queue = JobQueueService()\n"
    },
    {
      "path": "fileorganizer/backend/app/services/batch_processor.py",
      "mode": "create",
      "new_content": "\"\"\"\nBatch Processor - Process multiple documents in background\n\"\"\"\nimport logging\nfrom typing import List, Dict, Any\nfrom sqlalchemy.orm import Session\nfrom app.models.document import Document, ProcessingStatus\nfrom app.services.ocr_service import OCRService\nfrom app.services.classification_service import ClassificationService\nfrom app.services.embeddings_service import EmbeddingsService\nfrom app.models.category import Category\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\n\n\nclass BatchProcessor:\n    \"\"\"Process multiple documents with OCR and classification\"\"\"\n\n    def __init__(self, db: Session):\n        self.db = db\n        self.ocr_service = OCRService()\n        self.classification_service = ClassificationService()\n        self.embeddings_service = EmbeddingsService()\n\n    def process_batch(\n        self,\n        document_ids: List[int],\n        pack_id: int,\n        job_id: str = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Process batch of documents with OCR and classification\n        Returns: Summary of processing results\n        \"\"\"\n        from app.services.job_queue_service import job_queue\n\n        results = {\n            \"total\": len(document_ids),\n            \"processed\": 0,\n            \"failed\": 0,\n            \"errors\": []\n        }\n\n        # Get categories for classification\n        categories = self.db.query(Category).filter(\n            Category.scenario_pack_id == pack_id\n        ).all()\n\n        if not categories:\n            logger.warning(f\"No categories found for pack {pack_id}\")\n\n        for idx, doc_id in enumerate(document_ids):\n            try:\n                # Update progress\n                if job_id:\n                    job_queue.update_progress(job_id, idx + 1, len(document_ids))\n\n                # Get document\n                document = self.db.query(Document).filter(\n                    Document.id == doc_id\n                ).first()\n\n                if not document:\n                    results[\"errors\"].append({\n                        \"document_id\": doc_id,\n                        \"error\": \"Document not found\"\n                    })\n                    results[\"failed\"] += 1\n                    continue\n\n                # Process document\n                self._process_single_document(document, categories)\n                results[\"processed\"] += 1\n\n            except Exception as e:\n                logger.error(f\"Failed to process document {doc_id}: {str(e)}\")\n                results[\"errors\"].append({\n                    \"document_id\": doc_id,\n                    \"error\": str(e)\n                })\n                results[\"failed\"] += 1\n\n                # Mark document as failed\n                try:\n                    document = self.db.query(Document).filter(\n                        Document.id == doc_id\n                    ).first()\n                    if document:\n                        document.status = ProcessingStatus.FAILED\n                        self.db.commit()\n                except Exception:\n                    pass\n\n        return results\n\n    def _process_single_document(\n        self,\n        document: Document,\n        categories: List[Category]\n    ) -> None:\n        \"\"\"\n        Process single document: OCR + Classification + Embeddings\n        \"\"\"\n        try:\n            # Update status\n            document.status = ProcessingStatus.PROCESSING\n            self.db.commit()\n\n            # Step 1: OCR\n            file_path = Path(document.original_path)\n            if not file_path.exists():\n                raise FileNotFoundError(f\"File not found: {file_path}\")\n\n            extracted_text, confidence = self.ocr_service.extract_text(\n                file_path,\n                document.file_type\n            )\n\n            document.extracted_text = extracted_text\n            document.ocr_confidence = confidence\n            self.db.commit()\n\n            # Step 2: Classification (if categories available)\n            if categories and extracted_text:\n                category_id, class_confidence = self.classification_service.classify_document(\n                    extracted_text,\n                    categories\n                )\n\n                document.assigned_category_id = category_id\n                document.classification_confidence = class_confidence\n                self.db.commit()\n\n                # Step 3: Generate embeddings\n                embedding = self.embeddings_service.generate_embedding(extracted_text)\n                embedding_str = self.embeddings_service.serialize_embedding(embedding)\n                document.embedding_vector = embedding_str\n                self.db.commit()\n\n            # Mark as completed\n            document.status = ProcessingStatus.COMPLETED\n            self.db.commit()\n\n            logger.info(f\"Successfully processed document {document.id}\")\n\n        except Exception as e:\n            document.status = ProcessingStatus.FAILED\n            self.db.commit()\n            raise\n"
    },
    {
      "path": "fileorganizer/backend/app/routers/batch.py",
      "mode": "create",
      "new_content": "\"\"\"\nBatch Upload API endpoints\n\"\"\"\nfrom fastapi import APIRouter, Depends, UploadFile, File, HTTPException, BackgroundTasks\nfrom sqlalchemy.orm import Session\nfrom app.db.session import get_db\nfrom app.services.batch_service import BatchUploadService\nfrom app.services.batch_processor import BatchProcessor\nfrom app.services.job_queue_service import job_queue, JobStatus\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any\nimport uuid\nimport logging\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter()\n\n# Start job queue on module load\njob_queue.start()\n\n\nclass BatchUploadResponse(BaseModel):\n    batch_id: str\n    job_id: str\n    total_files: int\n    valid_files: int\n    invalid_files: int\n    invalid_details: List[Dict[str, str]]\n    message: str\n\n\nclass JobStatusResponse(BaseModel):\n    job_id: str\n    job_type: str\n    status: str\n    progress: int\n    total: int\n    result: Any = None\n    error: str = None\n\n\nclass BatchProcessRequest(BaseModel):\n    document_ids: List[int]\n    pack_id: int\n\n\n@router.post(\"/batch/upload\", response_model=BatchUploadResponse)\nasync def batch_upload(\n    files: List[UploadFile] = File(...),\n    pack_id: int = 1,\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    Upload multiple files in batch with validation\n    \"\"\"\n    batch_id = str(uuid.uuid4())\n    batch_service = BatchUploadService(db)\n\n    try:\n        # Prepare file info for validation\n        file_infos = []\n        file_data_map = {}\n\n        for file in files:\n            file_data = await file.read()\n            file_infos.append({\n                \"filename\": file.filename,\n                \"size\": len(file_data)\n            })\n            file_data_map[file.filename] = file_data\n\n        # Validate batch\n        validation_result = batch_service.validate_batch(file_infos)\n\n        if validation_result[\"valid_count\"] == 0:\n            raise HTTPException(\n                status_code=400,\n                detail=\"No valid files in batch\"\n            )\n\n        # Save valid files to temp storage\n        temp_paths = []\n        for file_info in validation_result[\"valid\"]:\n            filename = file_info[\"filename\"]\n            file_data = file_data_map[filename]\n            temp_path = batch_service.save_temp_file(filename, file_data)\n            temp_paths.append(temp_path)\n\n        # Move to permanent storage\n        permanent_paths = []\n        for temp_path in temp_paths:\n            permanent_path = batch_service.move_to_permanent(\n                temp_path,\n                temp_path.name.split(\"_\", 2)[-1]  # Extract original filename\n            )\n            permanent_paths.append(permanent_path)\n\n        # Create document records\n        documents = batch_service.create_document_records(permanent_paths, batch_id)\n        document_ids = [doc.id for doc in documents]\n\n        # Enqueue background processing job\n        def process_batch_job():\n            processor = BatchProcessor(db)\n            return processor.process_batch(document_ids, pack_id)\n\n        job_id = job_queue.enqueue(\n            job_type=\"batch_process\",\n            task_func=process_batch_job\n        )\n\n        return BatchUploadResponse(\n            batch_id=batch_id,\n            job_id=job_id,\n            total_files=validation_result[\"total\"],\n            valid_files=validation_result[\"valid_count\"],\n            invalid_files=validation_result[\"invalid_count\"],\n            invalid_details=validation_result[\"invalid\"],\n            message=f\"Batch upload initiated. Processing {len(document_ids)} documents in background.\"\n        )\n\n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Batch upload failed: {str(e)}\")\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Batch upload failed: {str(e)}\"\n        )\n\n\n@router.get(\"/batch/job/{job_id}\", response_model=JobStatusResponse)\nasync def get_job_status(job_id: str):\n    \"\"\"\n    Get status of background processing job\n    \"\"\"\n    job_status = job_queue.get_job_status(job_id)\n\n    if not job_status:\n        raise HTTPException(\n            status_code=404,\n            detail=\"Job not found\"\n        )\n\n    return JobStatusResponse(**job_status)\n\n\n@router.post(\"/batch/process\")\nasync def process_batch(\n    request: BatchProcessRequest,\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    Process batch of documents (OCR + Classification)\n    \"\"\"\n    try:\n        def process_job():\n            processor = BatchProcessor(db)\n            return processor.process_batch(request.document_ids, request.pack_id)\n\n        job_id = job_queue.enqueue(\n            job_type=\"batch_process\",\n            task_func=process_job\n        )\n\n        return {\n            \"job_id\": job_id,\n            \"message\": f\"Processing {len(request.document_ids)} documents in background\"\n        }\n\n    except Exception as e:\n        logger.error(f\"Batch processing failed: {str(e)}\")\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Batch processing failed: {str(e)}\"\n        )\n"
    },
    {
      "path": "fileorganizer/backend/app/__init__.py",
      "mode": "create",
      "new_content": ""
    },
    {
      "path": "fileorganizer/backend/app/main.py",
      "mode": "create",
      "new_content": "\"\"\"\nMain FastAPI application\n\"\"\"\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom app.core.middleware import error_handler_middleware\nfrom app.core.security import ALLOWED_ORIGINS\nfrom app.routers import health, packs, documents, classification, export, batch\nfrom app.db.session import init_db\nfrom app.core.logging import setup_logging\n\n# Setup logging\nsetup_logging()\n\n# Initialize database\ninit_db()\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"FileOrganizer API\",\n    description=\"Document organization and classification system\",\n    version=\"1.0.0\"\n)\n\n# CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=ALLOWED_ORIGINS,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Error handling middleware\napp.middleware(\"http\")(error_handler_middleware)\n\n# Include routers\napp.include_router(health.router, prefix=\"/api/v1\", tags=[\"health\"])\napp.include_router(packs.router, prefix=\"/api/v1\", tags=[\"packs\"])\napp.include_router(documents.router, prefix=\"/api/v1\", tags=[\"documents\"])\napp.include_router(classification.router, prefix=\"/api/v1\", tags=[\"classification\"])\napp.include_router(export.router, prefix=\"/api/v1\", tags=[\"export\"])\napp.include_router(batch.router, prefix=\"/api/v1\", tags=[\"batch\"])\n\n\n@app.get(\"/\")\nasync def root():\n    return {\n        \"message\": \"FileOrganizer API\",\n        \"version\": \"1.0.0\",\n        \"docs\": \"/docs\"\n    }\n"
    },
    {
      "path": "fileorganizer/frontend/src/components/DragDropUpload.tsx",
      "mode": "create",
      "new_content": "import React, { useState, useRef, DragEvent } from 'react';\nimport ProgressBar from './ProgressBar';\n\ninterface DragDropUploadProps {\n  onFilesSelected: (files: File[]) => void;\n  maxFiles?: number;\n  acceptedTypes?: string[];\n}\n\nconst DragDropUpload: React.FC<DragDropUploadProps> = ({\n  onFilesSelected,\n  maxFiles = 50,\n  acceptedTypes = ['.pdf', '.png', '.jpg', '.jpeg']\n}) => {\n  const [isDragging, setIsDragging] = useState(false);\n  const [selectedFiles, setSelectedFiles] = useState<File[]>([]);\n  const fileInputRef = useRef<HTMLInputElement>(null);\n\n  const handleDragEnter = (e: DragEvent<HTMLDivElement>) => {\n    e.preventDefault();\n    e.stopPropagation();\n    setIsDragging(true);\n  };\n\n  const handleDragLeave = (e: DragEvent<HTMLDivElement>) => {\n    e.preventDefault();\n    e.stopPropagation();\n    setIsDragging(false);\n  };\n\n  const handleDragOver = (e: DragEvent<HTMLDivElement>) => {\n    e.preventDefault();\n    e.stopPropagation();\n  };\n\n  const handleDrop = (e: DragEvent<HTMLDivElement>) => {\n    e.preventDefault();\n    e.stopPropagation();\n    setIsDragging(false);\n\n    const files = Array.from(e.dataTransfer.files);\n    handleFiles(files);\n  };\n\n  const handleFileInput = (e: React.ChangeEvent<HTMLInputElement>) => {\n    if (e.target.files) {\n      const files = Array.from(e.target.files);\n      handleFiles(files);\n    }\n  };\n\n  const handleFiles = (files: File[]) => {\n    // Filter by accepted types\n    const validFiles = files.filter(file => {\n      const extension = '.' + file.name.split('.').pop()?.toLowerCase();\n      return acceptedTypes.includes(extension);\n    });\n\n    // Limit number of files\n    const limitedFiles = validFiles.slice(0, maxFiles - selectedFiles.length);\n\n    const newFiles = [...selectedFiles, ...limitedFiles];\n    setSelectedFiles(newFiles);\n    onFilesSelected(newFiles);\n  };\n\n  const removeFile = (index: number) => {\n    const newFiles = selectedFiles.filter((_, i) => i !== index);\n    setSelectedFiles(newFiles);\n    onFilesSelected(newFiles);\n  };\n\n  const clearAll = () => {\n    setSelectedFiles([]);\n    onFilesSelected([]);\n    if (fileInputRef.current) {\n      fileInputRef.current.value = '';\n    }\n  };\n\n  const formatFileSize = (bytes: number): string => {\n    if (bytes < 1024) return bytes + ' B';\n    if (bytes < 1024 * 1024) return (bytes / 1024).toFixed(2) + ' KB';\n    return (bytes / (1024 * 1024)).toFixed(2) + ' MB';\n  };\n\n  return (\n    <div className=\"w-full\">\n      {/* Drop Zone */}\n      <div\n        onDragEnter={handleDragEnter}\n        onDragOver={handleDragOver}\n        onDragLeave={handleDragLeave}\n        onDrop={handleDrop}\n        onClick={() => fileInputRef.current?.click()}\n        className={`\n          border-2 border-dashed rounded-lg p-12 text-center cursor-pointer\n          transition-all duration-200\n          ${\n            isDragging\n              ? 'border-blue-500 bg-blue-50'\n              : 'border-gray-300 hover:border-blue-400 hover:bg-gray-50'\n          }\n        `}\n      >\n        <input\n          ref={fileInputRef}\n          type=\"file\"\n          multiple\n          accept={acceptedTypes.join(',')}\n          onChange={handleFileInput}\n          className=\"hidden\"\n        />\n\n        <div className=\"flex flex-col items-center\">\n          <svg\n            className={`w-16 h-16 mb-4 ${\n              isDragging ? 'text-blue-500' : 'text-gray-400'\n            }`}\n            fill=\"none\"\n            stroke=\"currentColor\"\n            viewBox=\"0 0 24 24\"\n          >\n            <path\n              strokeLinecap=\"round\"\n              strokeLinejoin=\"round\"\n              strokeWidth={2}\n              d=\"M7 16a4 4 0 01-.88-7.903A5 5 0 1115.9 6L16 6a5 5 0 011 9.9M15 13l-3-3m0 0l-3 3m3-3v12\"\n            />\n          </svg>\n\n          <p className=\"text-lg font-semibold text-gray-700 mb-2\">\n            {isDragging ? 'Drop files here' : 'Drag & drop files here'}\n          </p>\n          <p className=\"text-sm text-gray-500 mb-4\">or click to browse</p>\n          <p className=\"text-xs text-gray-400\">\n            Supported: {acceptedTypes.join(', ')} (max {maxFiles} files)\n          </p>\n        </div>\n      </div>\n\n      {/* Selected Files List */}\n      {selectedFiles.length > 0 && (\n        <div className=\"mt-6\">\n          <div className=\"flex justify-between items-center mb-3\">\n            <h3 className=\"font-semibold text-gray-700\">\n              Selected Files ({selectedFiles.length})\n            </h3>\n            <button\n              onClick={clearAll}\n              className=\"text-sm text-red-600 hover:text-red-700 font-medium\"\n            >\n              Clear All\n            </button>\n          </div>\n\n          <div className=\"space-y-2 max-h-64 overflow-y-auto\">\n            {selectedFiles.map((file, index) => (\n              <div\n                key={index}\n                className=\"flex items-center justify-between bg-gray-50 p-3 rounded-lg border border-gray-200\"\n              >\n                <div className=\"flex items-center flex-1 min-w-0\">\n                  <svg\n                    className=\"w-5 h-5 text-gray-400 mr-3 flex-shrink-0\"\n                    fill=\"none\"\n                    stroke=\"currentColor\"\n                    viewBox=\"0 0 24 24\"\n                  >\n                    <path\n                      strokeLinecap=\"round\"\n                      strokeLinejoin=\"round\"\n                      strokeWidth={2}\n                      d=\"M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z\"\n                    />\n                  </svg>\n                  <div className=\"min-w-0 flex-1\">\n                    <p className=\"text-sm font-medium text-gray-700 truncate\">\n                      {file.name}\n                    </p>\n                    <p className=\"text-xs text-gray-500\">\n                      {formatFileSize(file.size)}\n                    </p>\n                  </div>\n                </div>\n                <button\n                  onClick={() => removeFile(index)}\n                  className=\"ml-4 text-red-500 hover:text-red-700 flex-shrink-0\"\n                >\n                  <svg\n                    className=\"w-5 h-5\"\n                    fill=\"none\"\n                    stroke=\"currentColor\"\n                    viewBox=\"0 0 24 24\"\n                  >\n                    <path\n                      strokeLinecap=\"round\"\n                      strokeLinejoin=\"round\"\n                      strokeWidth={2}\n                      d=\"M6 18L18 6M6 6l12 12\"\n                    />\n                  </svg>\n                </button>\n              </div>\n            ))}\n          </div>\n        </div>\n      )}\n    </div>\n  );\n};\n\nexport default DragDropUpload;\n"
    },
    {
      "path": "fileorganizer/frontend/src/pages/Upload.tsx",
      "mode": "modify",
      "new_content": "import React, { useState } from 'react';\nimport { useSearchParams, useNavigate } from 'react-router-dom';\nimport axios from 'axios';\nimport DragDropUpload from '../components/DragDropUpload';\nimport ProgressBar from '../components/ProgressBar';\nimport LoadingSpinner from '../components/LoadingSpinner';\nimport ErrorDisplay from '../components/ErrorDisplay';\n\ninterface UploadStatus {\n  uploading: boolean;\n  processing: boolean;\n  progress: number;\n  total: number;\n  message: string;\n  error: string | null;\n  jobId: string | null;\n}\n\nconst Upload: React.FC = () => {\n  const [searchParams] = useSearchParams();\n  const packId = searchParams.get('pack');\n  const navigate = useNavigate();\n\n  const [selectedFiles, setSelectedFiles] = useState<File[]>([]);\n  const [status, setStatus] = useState<UploadStatus>({\n    uploading: false,\n    processing: false,\n    progress: 0,\n    total: 0,\n    message: '',\n    error: null,\n    jobId: null\n  });\n\n  const handleFilesSelected = (files: File[]) => {\n    setSelectedFiles(files);\n    setStatus(prev => ({ ...prev, error: null }));\n  };\n\n  const pollJobStatus = async (jobId: string) => {\n    const maxAttempts = 60; // 5 minutes with 5 second intervals\n    let attempts = 0;\n\n    const poll = async () => {\n      try {\n        const response = await axios.get(\n          `http://127.0.0.1:8000/api/v1/batch/job/${jobId}`\n        );\n\n        const jobStatus = response.data;\n\n        setStatus(prev => ({\n          ...prev,\n          progress: jobStatus.progress,\n          total: jobStatus.total,\n          message: `Processing: ${jobStatus.progress}/${jobStatus.total} documents`\n        }));\n\n        if (jobStatus.status === 'completed') {\n          setStatus(prev => ({\n            ...prev,\n            processing: false,\n            message: `[OK] Successfully processed ${jobStatus.total} documents`\n          }));\n\n          // Navigate to triage board after 2 seconds\n          setTimeout(() => {\n            navigate(`/triage?pack=${packId}`);\n          }, 2000);\n          return;\n        }\n\n        if (jobStatus.status === 'failed') {\n          setStatus(prev => ({\n            ...prev,\n            processing: false,\n            error: jobStatus.error || 'Processing failed'\n          }));\n          return;\n        }\n\n        // Continue polling\n        attempts++;\n        if (attempts < maxAttempts) {\n          setTimeout(poll, 5000); // Poll every 5 seconds\n        } else {\n          setStatus(prev => ({\n            ...prev,\n            processing: false,\n            error: 'Processing timeout - please check triage board'\n          }));\n        }\n      } catch (error) {\n        console.error('Failed to poll job status:', error);\n        setStatus(prev => ({\n          ...prev,\n          processing: false,\n          error: 'Failed to check processing status'\n        }));\n      }\n    };\n\n    poll();\n  };\n\n  const uploadFiles = async () => {\n    if (selectedFiles.length === 0) return;\n\n    setStatus({\n      uploading: true,\n      processing: false,\n      progress: 0,\n      total: selectedFiles.length,\n      message: 'Uploading files...',\n      error: null,\n      jobId: null\n    });\n\n    try {\n      // Create FormData with all files\n      const formData = new FormData();\n      selectedFiles.forEach(file => {\n        formData.append('files', file);\n      });\n\n      // Upload batch\n      const response = await axios.post(\n        `http://127.0.0.1:8000/api/v1/batch/upload?pack_id=${packId}`,\n        formData,\n        {\n          headers: { 'Content-Type': 'multipart/form-data' }\n        }\n      );\n\n      const result = response.data;\n\n      // Check for invalid files\n      if (result.invalid_files > 0) {\n        const invalidDetails = result.invalid_details\n          .map((d: any) => `${d.filename}: ${d.reason}`)\n          .join(', ');\n        console.warn('Invalid files:', invalidDetails);\n      }\n\n      // Start processing\n      setStatus({\n        uploading: false,\n        processing: true,\n        progress: 0,\n        total: result.valid_files,\n        message: `Processing ${result.valid_files} documents...`,\n        error: null,\n        jobId: result.job_id\n      });\n\n      // Poll job status\n      pollJobStatus(result.job_id);\n\n    } catch (error: any) {\n      console.error('Upload failed:', error);\n      setStatus({\n        uploading: false,\n        processing: false,\n        progress: 0,\n        total: 0,\n        message: '',\n        error: error.response?.data?.detail || 'Upload failed. Please try again.',\n        jobId: null\n      });\n    }\n  };\n\n  const isProcessing = status.uploading || status.processing;\n\n  return (\n    <div className=\"min-h-screen bg-gray-50 p-8\">\n      <div className=\"max-w-4xl mx-auto\">\n        <h1 className=\"text-3xl font-bold text-gray-800 mb-2\">\n          Upload Documents\n        </h1>\n        <p className=\"text-gray-600 mb-8\">\n          Upload your documents for OCR and classification\n        </p>\n\n        {/* Error Display */}\n        {status.error && (\n          <ErrorDisplay\n            message={status.error}\n            onDismiss={() => setStatus(prev => ({ ...prev, error: null }))}\n          />\n        )}\n\n        {/* Drag & Drop Upload */}\n        <div className=\"bg-white rounded-lg shadow-md p-8 mb-6\">\n          <DragDropUpload\n            onFilesSelected={handleFilesSelected}\n            maxFiles={50}\n            acceptedTypes={['.pdf', '.png', '.jpg', '.jpeg']}\n          />\n\n          {/* Upload Button */}\n          {selectedFiles.length > 0 && !isProcessing && (\n            <button\n              onClick={uploadFiles}\n              className=\"w-full mt-6 bg-blue-600 text-white py-3 rounded-lg font-semibold hover:bg-blue-700 transition-colors\"\n            >\n              Upload & Process {selectedFiles.length} file(s)\n            </button>\n          )}\n        </div>\n\n        {/* Progress Display */}\n        {isProcessing && (\n          <div className=\"bg-white rounded-lg shadow-md p-8 mb-6\">\n            <h3 className=\"text-lg font-semibold mb-4\">\n              {status.uploading ? 'Uploading...' : 'Processing Documents'}\n            </h3>\n\n            {status.uploading ? (\n              <LoadingSpinner message=\"Uploading files to server...\" />\n            ) : (\n              <div>\n                <ProgressBar\n                  current={status.progress}\n                  total={status.total}\n                  label=\"Processing Progress\"\n                />\n                <p className=\"text-sm text-gray-600 mt-4 text-center\">\n                  {status.message}\n                </p>\n              