---
description: Phase 1 - Discovery and Improvement Identification
globs:
  - "**/*.py"
  - "**/*.md"
  - "**/IMPROVEMENTS.md"
alwaysApply: false
---

# Phase 1: Discovery and Improvement Identification

You are performing Phase 1 of the Autopack improvement workflow: **Discovery**.

## Objective

Analyze the codebase to identify potential improvements, bugs, and enhancements. Produce a prioritized list of actionable improvement items.

## Carryover Context Integration (IMP-FEAT-001)

When a `CARRYOVER_CONTEXT` path is provided in the prompt context, incorporate unimplemented improvements from the previous Discovery cycle:

### Reading CARRYOVER_CONTEXT.json

1. **Load the file** and parse its JSON structure
2. **Check `unimplemented_imps`** array - These are improvements that were discovered but not implemented in the previous cycle
3. **For each carryover item**:
   - Preserve the `carryover_from` field (indicates this came from a previous cycle)
   - Assess if the improvement is still relevant to the current codebase
   - Check if dependencies have changed since original discovery
   - Re-evaluate priority based on current state

### Prioritization Rules for Carryover Items

1. **Carryover items get priority boost** - Items waiting from previous cycles should be prioritized:
   - If carryover item was `critical` or `high` priority, maintain or boost priority
   - If carryover item was `medium` priority, consider boosting to `high` if still relevant
   - Mark items that have been carried over multiple times for special attention

2. **Staleness check** - Verify carryover items are still valid:
   - Check if referenced files still exist
   - Check if the issue has been addressed by other changes
   - Check if dependencies are still relevant

3. **Merge with new discoveries** - Combine carryover items with newly discovered improvements:
   - Avoid duplicates (same improvement discovered again)
   - Update carryover items if new information is available
   - Preserve `carryover_from` field for tracking purposes

### Output Format for Carryover Items

When including carryover items in output, add a Historical Context section:

```markdown
### Historical Context
- **Carryover**: Yes (from previous_cycle)
- **Original Discovery**: [date from source_cycle]
- **Status Update**: [any changes since original discovery]
```

## Feedback Loop Context Integration (IMP-LOOP-001)

**CONTEXT INJECTION** (if available):
If `C:\Users\hshk9\OneDrive\Backup\Desktop\DISCOVERY_CONTEXT.json` exists, READ it first and:
1. Prioritize fixing any `recurring_issues` listed - these are issues that keep appearing across projects
2. Focus discovery on `high_value_categories` - these IMP categories have the best historical outcomes
3. Avoid approaches listed in `failed_approaches` - these have failed before and should not be repeated
4. Follow the `recommendations` provided - these are actionable insights from telemetry analysis

### Reading DISCOVERY_CONTEXT.json

1. **Load the file** and parse its JSON structure
2. **Check `recurring_issues`** array - Issues that appear frequently:
   - Each issue has `type`, `occurrences`, and `recommendation`
   - Prioritize creating IMPs that address root causes of recurring issues
   - These should be treated as high-priority discoveries

3. **Check `successful_patterns`** array - Patterns that worked well:
   - Each pattern has `decision_type`, `chosen_option`, and `context`
   - Prefer approaches that match successful patterns
   - Use these as templates for new improvements

4. **Check `high_value_categories`** list - Best-performing IMP categories:
   - Focus discovery efforts on these categories first
   - Examples: TEL (telemetry), LOG (logging), ESC (escalation), PERF (performance)

5. **Check `failed_approaches`** array - Approaches to avoid:
   - Each entry has `approach`, `reason`, and `avoid_in`
   - Do NOT recommend approaches that have previously failed
   - If a similar approach is needed, note the previous failure and propose mitigations

6. **Check `performance_insights`** object - Timing and throughput data:
   - Use to inform realistic effort estimates
   - Identify bottleneck components to prioritize

7. **Follow `recommendations`** list - Actionable insights:
   - These are pre-generated recommendations from the context injector
   - Incorporate these into your discovery prioritization

## Historical Context Integration (IMP-GEN-001)

When telemetry context is provided, use historical data to inform your discovery:

### Reading LEARNING_MEMORY.json

If a `LEARNING_MEMORY` path is provided in the prompt context:

1. **Load the file** and parse its JSON structure
2. **Check `success_patterns`** - These are improvement types that have historically succeeded:
   - Prioritize improvements matching successful patterns
   - Note the `success_rate` and `avg_completion_time` for realistic planning
   - Look for patterns like: category, complexity, file_count ranges

3. **Check `failure_patterns`** - These are improvement types that have historically failed:
   - De-prioritize or flag improvements matching failure patterns
   - Note common `failure_reasons` to avoid similar pitfalls
   - Consider if the failure cause has been addressed before recommending

4. **Review `improvement_outcomes`** - Historical improvement results:
   - Look for similar improvements that were attempted before
   - Check if they succeeded, failed, or were abandoned
   - Use outcomes to refine your recommendations

5. **Examine `wave_history`** - How improvements were grouped in past cycles:
   - Identify successful wave compositions
   - Note wave sizes that worked well vs. those that caused issues

### Reading TELEMETRY_SUMMARY.json

If a `TELEMETRY_SUMMARY` path is provided:

1. **Check `metrics.success_rate`** - Overall system health indicator
2. **Review `completion_time_metrics`** by category:
   - Use `avg_completion_time_minutes` for realistic estimates
   - Note categories with high variance (max >> avg)
3. **Examine `failure_categories`** - Common failure types to avoid
4. **Consider `escalation_frequency`** - Indicates problematic areas

## Discovery Process

### Step 1: Codebase Analysis

Scan the codebase for:
- Code quality issues (lint, type errors, complexity)
- Missing or outdated documentation
- Test coverage gaps
- Performance bottlenecks
- Security vulnerabilities
- Deprecated patterns or libraries

### Step 2: Priority Assessment

For each potential improvement, assess:

1. **Impact** (High/Medium/Low)
   - How much value does this add?
   - How many users/systems are affected?

2. **Effort** (High/Medium/Low)
   - Estimated complexity
   - Number of files affected
   - Dependencies involved

3. **Risk** (High/Medium/Low)
   - Likelihood of introducing bugs
   - Reversibility of changes
   - Test coverage for affected areas

4. **Historical Success** (if telemetry available)
   - Does this match a success pattern? (boost priority)
   - Does this match a failure pattern? (add caution flag)
   - What was the actual completion time for similar work?

### Step 3: Improvement Categorization

Categorize improvements into:
- `bug`: Fixing incorrect behavior
- `feature`: Adding new functionality
- `enhancement`: Improving existing functionality
- `refactor`: Code structure improvements
- `docs`: Documentation updates
- `test`: Test coverage improvements
- `perf`: Performance optimizations
- `security`: Security improvements
- `deps`: Dependency updates

### Step 4: Output Format

Generate improvements in this format:

```markdown
## Improvement: [IMP-XXX-NNN] Title

**Category**: [category]
**Priority**: [critical|high|medium|low]
**Impact**: [High|Medium|Low]
**Effort**: [High|Medium|Low]
**Risk**: [High|Medium|Low]

### Description
[What needs to be done and why]

### Files Affected
- path/to/file1.py
- path/to/file2.py

### Dependencies
- [Other improvements this depends on, if any]

### Historical Context
- [If telemetry available: similar past improvements and their outcomes]
- [Success pattern match: Yes/No]
- [Failure pattern match: Yes/No - with mitigation if Yes]

### Acceptance Criteria
- [ ] Criterion 1
- [ ] Criterion 2
```

## Priority Guidelines

### Critical Priority
- Security vulnerabilities
- Data integrity issues
- Breaking changes blocking other work
- Items matching high-success patterns with urgent need

### High Priority
- User-facing bugs
- Performance issues affecting UX
- Test failures
- Items matching success patterns

### Medium Priority
- Code quality improvements
- Documentation gaps
- Minor enhancements
- Items with mixed historical outcomes

### Low Priority
- Nice-to-have features
- Minor refactoring
- Items matching failure patterns (unless root cause addressed)

## Telemetry-Informed Adjustments

When historical data shows:

| Pattern | Adjustment |
|---------|------------|
| Category has >80% success rate | Boost priority by one level |
| Category has <50% success rate | Add caution flag, require mitigation plan |
| Similar improvement failed before | Note failure reason, require different approach |
| Avg completion time > 2x estimated | Flag as potentially underestimated |
| High escalation frequency | Consider breaking into smaller items |

## Output

At the end of Phase 1, produce:

1. **IMPROVEMENTS.md** - Complete list of discovered improvements
2. **Summary statistics**:
   - Total improvements found
   - Breakdown by category
   - Breakdown by priority
   - Historical pattern matches (if telemetry available)
