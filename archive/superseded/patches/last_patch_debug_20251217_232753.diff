diff --git a/src/backend/search/__init__.py b/src/backend/search/__init__.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/src/backend/search/__init__.py
@@ -0,0 +1,6 @@
+"""Semantic search module using sentence-transformers embeddings."""
+
+from src.backend.search.embedding_service import EmbeddingService
+from src.backend.search.semantic_search import SemanticSearchEngine
+
+__all__ = ["EmbeddingService", "SemanticSearchEngine"]
diff --git a/src/backend/search/embedding_service.py b/src/backend/search/embedding_service.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/src/backend/search/embedding_service.py
@@ -0,0 +1,157 @@
+"""Embedding service using all-mpnet-base-v2 model.
+
+Provides text embedding generation using the sentence-transformers library
+with the all-mpnet-base-v2 model for high-quality semantic representations.
+"""
+
+import logging
+from typing import List, Optional, Union
+import numpy as np
+
+logger = logging.getLogger(__name__)
+
+# Lazy load sentence-transformers to avoid import overhead
+_model = None
+_model_name = "all-mpnet-base-v2"
+
+
+def _get_model():
+    """Lazy load the sentence-transformers model."""
+    global _model
+    if _model is None:
+        try:
+            from sentence_transformers import SentenceTransformer
+            logger.info(f"Loading embedding model: {_model_name}")
+            _model = SentenceTransformer(_model_name)
+            logger.info(f"Embedding model loaded successfully. Dimension: {_model.get_sentence_embedding_dimension()}")
+        except ImportError:
+            logger.error(
+                "sentence-transformers not installed. "
+                "Install with: pip install sentence-transformers"
+            )
+            raise
+    return _model
+
+
+class EmbeddingService:
+    """Service for generating text embeddings using all-mpnet-base-v2.
+    
+    The all-mpnet-base-v2 model provides:
+    - 768-dimensional embeddings
+    - Strong performance on semantic similarity tasks
+    - Good balance of speed and quality
+    
+    Example:
+        >>> service = EmbeddingService()
+        >>> embedding = service.embed_text("Hello world")
+        >>> print(embedding.shape)  # (768,)
+        >>> 
+        >>> embeddings = service.embed_texts(["Hello", "World"])
+        >>> print(embeddings.shape)  # (2, 768)
+    """
+    
+    def __init__(self, model_name: str = "all-mpnet-base-v2"):
+        """Initialize the embedding service.
+        
+        Args:
+            model_name: Name of the sentence-transformers model to use.
+                       Defaults to all-mpnet-base-v2.
+        """
+        global _model_name
+        _model_name = model_name
+        self._model_name = model_name
+    
+    @property
+    def model(self):
+        """Get the underlying sentence-transformers model."""
+        return _get_model()
+    
+    @property
+    def embedding_dimension(self) -> int:
+        """Get the dimension of embeddings produced by this model."""
+        return self.model.get_sentence_embedding_dimension()
+    
+    def embed_text(self, text: str, normalize: bool = True) -> np.ndarray:
+        """Generate embedding for a single text.
+        
+        Args:
+            text: Input text to embed.
+            normalize: Whether to L2-normalize the embedding. Default True.
+        
+        Returns:
+            Numpy array of shape (embedding_dimension,)
+        """
+        embedding = self.model.encode(
+            text,
+            convert_to_numpy=True,
+            normalize_embeddings=normalize
+        )
+        return embedding
+    
+    def embed_texts(
+        self,
+        texts: List[str],
+        normalize: bool = True,
+        batch_size: int = 32,
+        show_progress: bool = False
+    ) -> np.ndarray:
+        """Generate embeddings for multiple texts.
+        
+        Args:
+            texts: List of input texts to embed.
+            normalize: Whether to L2-normalize embeddings. Default True.
+            batch_size: Batch size for encoding. Default 32.
+            show_progress: Whether to show progress bar. Default False.
+        
+        Returns:
+            Numpy array of shape (len(texts), embedding_dimension)
+        """
+        if not texts:
+            return np.array([])
+        
+        embeddings = self.model.encode(
+            texts,
+            convert_to_numpy=True,
+            normalize_embeddings=normalize,
+            batch_size=batch_size,
+            show_progress_bar=show_progress
+        )
+        return embeddings
+    
+    def similarity(self, text1: str, text2: str) -> float:
+        """Compute cosine similarity between two texts.
+        
+        Args:
+            text1: First text.
+            text2: Second text.
+        
+        Returns:
+            Cosine similarity score between -1 and 1.
+        """
+        emb1 = self.embed_text(text1, normalize=True)
+        emb2 = self.embed_text(text2, normalize=True)
+        return float(np.dot(emb1, emb2))
+    
+    def batch_similarity(
+        self,
+        query: str,
+        candidates: List[str]
+    ) -> List[float]:
+        """Compute similarity between a query and multiple candidates.
+        
+        Args:
+            query: Query text.
+            candidates: List of candidate texts to compare against.
+        
+        Returns:
+            List of similarity scores for each candidate.
+        """
+        if not candidates:
+            return []
+        
+        query_emb = self.embed_text(query, normalize=True)
+        candidate_embs = self.embed_texts(candidates, normalize=True)
+        
+        # Compute dot products (equivalent to cosine similarity for normalized vectors)
+        similarities = np.dot(candidate_embs, query_emb)
+        return similarities.tolist()
diff --git a/src/backend/search/semantic_search.py b/src/backend/search/semantic_search.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/src/backend/search/semantic_search.py
@@ -0,0 +1,360 @@
+"""Semantic search engine using embeddings.
+
+Provides semantic search capabilities over document collections
+using the EmbeddingService for vector representations.
+"""
+
+import logging
+from dataclasses import dataclass, field
+from typing import Any, Dict, List, Optional, Tuple
+import numpy as np
+
+from src.backend.search.embedding_service import EmbeddingService
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class SearchResult:
+    """A single search result."""
+    
+    document_id: str
+    score: float
+    text: str
+    metadata: Dict[str, Any] = field(default_factory=dict)
+    
+    def to_dict(self) -> Dict[str, Any]:
+        """Convert to dictionary representation."""
+        return {
+            "document_id": self.document_id,
+            "score": self.score,
+            "text": self.text,
+            "metadata": self.metadata
+        }
+
+
+@dataclass
+class Document:
+    """A document in the search index."""
+    
+    id: str
+    text: str
+    embedding: Optional[np.ndarray] = None
+    metadata: Dict[str, Any] = field(default_factory=dict)
+
+
+class SemanticSearchEngine:
+    """Semantic search engine using all-mpnet-base-v2 embeddings.
+    
+    Provides in-memory semantic search over a collection of documents.
+    For production use with large collections, consider using a vector
+    database like Qdrant, Pinecone, or Milvus.
+    
+    Example:
+        >>> engine = SemanticSearchEngine()
+        >>> engine.add_documents([
+        ...     {"id": "1", "text": "Python programming tutorial"},
+        ...     {"id": "2", "text": "Machine learning basics"},
+        ...     {"id": "3", "text": "Web development with JavaScript"}
+        ... ])
+        >>> results = engine.search("AI and ML", top_k=2)
+        >>> for r in results:
+        ...     print(f"{r.document_id}: {r.score:.3f}")
+    """
+    
+    def __init__(
+        self,
+        embedding_service: Optional[EmbeddingService] = None,
+        similarity_threshold: float = 0.0
+    ):
+        """Initialize the search engine.
+        
+        Args:
+            embedding_service: EmbeddingService instance. Creates new one if None.
+            similarity_threshold: Minimum similarity score for results. Default 0.0.
+        """
+        self._embedding_service = embedding_service or EmbeddingService()
+        self._similarity_threshold = similarity_threshold
+        self._documents: Dict[str, Document] = {}
+        self._embeddings_matrix: Optional[np.ndarray] = None
+        self._document_ids: List[str] = []
+        self._index_dirty = True
+    
+    @property
+    def embedding_service(self) -> EmbeddingService:
+        """Get the embedding service."""
+        return self._embedding_service
+    
+    @property
+    def document_count(self) -> int:
+        """Get the number of indexed documents."""
+        return len(self._documents)
+    
+    def add_document(
+        self,
+        doc_id: str,
+        text: str,
+        metadata: Optional[Dict[str, Any]] = None,
+        embedding: Optional[np.ndarray] = None
+    ) -> None:
+        """Add a single document to the index.
+        
+        Args:
+            doc_id: Unique document identifier.
+            text: Document text content.
+            metadata: Optional metadata dictionary.
+            embedding: Pre-computed embedding. Computed if None.
+        """
+        if embedding is None:
+            embedding = self._embedding_service.embed_text(text)
+        
+        self._documents[doc_id] = Document(
+            id=doc_id,
+            text=text,
+            embedding=embedding,
+            metadata=metadata or {}
+        )
+        self._index_dirty = True
+    
+    def add_documents(
+        self,
+        documents: List[Dict[str, Any]],
+        batch_size: int = 32,
+        show_progress: bool = False
+    ) -> int:
+        """Add multiple documents to the index.
+        
+        Args:
+            documents: List of document dicts with 'id', 'text', and optional 'metadata'.
+            batch_size: Batch size for embedding computation.
+            show_progress: Whether to show progress bar.
+        
+        Returns:
+            Number of documents added.
+        """
+        if not documents:
+            return 0
+        
+        # Extract texts for batch embedding
+        texts = [doc["text"] for doc in documents]
+        
+        # Compute embeddings in batch
+        embeddings = self._embedding_service.embed_texts(
+            texts,
+            batch_size=batch_size,
+            show_progress=show_progress
+        )
+        
+        # Add documents with embeddings
+        for i, doc in enumerate(documents):
+            self._documents[doc["id"]] = Document(
+                id=doc["id"],
+                text=doc["text"],
+                embedding=embeddings[i],
+                metadata=doc.get("metadata", {})
+            )
+        
+        self._index_dirty = True
+        return len(documents)
+    
+    def remove_document(self, doc_id: str) -> bool:
+        """Remove a document from the index.
+        
+        Args:
+            doc_id: Document ID to remove.
+        
+        Returns:
+            True if document was removed, False if not found.
+        """
+        if doc_id in self._documents:
+            del self._documents[doc_id]
+            self._index_dirty = True
+            return True
+        return False
+    
+    def clear(self) -> None:
+        """Clear all documents from the index."""
+        self._documents.clear()
+        self._embeddings_matrix = None
+        self._document_ids = []
+        self._index_dirty = True
+    
+    def _rebuild_index(self) -> None:
+        """Rebuild the embeddings matrix for efficient search."""
+        if not self._documents:
+            self._embeddings_matrix = None
+            self._document_ids = []
+            self._index_dirty = False
+            return
+        
+        self._document_ids = list(self._documents.keys())
+        embeddings = [self._documents[doc_id].embedding for doc_id in self._document_ids]
+        self._embeddings_matrix = np.vstack(embeddings)
+        self._index_dirty = False
+    
+    def search(
+        self,
+        query: str,
+        top_k: int = 10,
+        threshold: Optional[float] = None,
+        filter_metadata: Optional[Dict[str, Any]] = None
+    ) -> List[SearchResult]:
+        """Search for documents similar to the query.
+        
+        Args:
+            query: Search query text.
+            top_k: Maximum number of results to return.
+            threshold: Minimum similarity threshold. Uses instance default if None.
+            filter_metadata: Optional metadata filter (exact match on all keys).
+        
+        Returns:
+            List of SearchResult objects sorted by descending similarity.
+        """
+        if not self._documents:
+            return []
+        
+        # Rebuild index if needed
+        if self._index_dirty:
+            self._rebuild_index()
+        
+        # Compute query embedding
+        query_embedding = self._embedding_service.embed_text(query)
+        
+        # Compute similarities
+        similarities = np.dot(self._embeddings_matrix, query_embedding)
+        
+        # Apply threshold
+        effective_threshold = threshold if threshold is not None else self._similarity_threshold
+        
+        # Get top-k indices
+        if top_k >= len(similarities):
+            top_indices = np.argsort(similarities)[::-1]
+        else:
+            # Use argpartition for efficiency with large collections
+            partition_idx = len(similarities) - top_k
+            top_indices = np.argpartition(similarities, partition_idx)[partition_idx:]
+            top_indices = top_indices[np.argsort(similarities[top_indices])[::-1]]
+        
+        # Build results
+        results = []
+        for idx in top_indices:
+            score = float(similarities[idx])
+            
+            # Apply threshold filter
+            if score < effective_threshold:
+                continue
+            
+            doc_id = self._document_ids[idx]
+            doc = self._documents[doc_id]
+            
+            # Apply metadata filter
+            if filter_metadata:
+                if not all(
+                    doc.metadata.get(k) == v
+                    for k, v in filter_metadata.items()
+                ):
+                    continue
+            
+            results.append(SearchResult(
+                document_id=doc_id,
+                score=score,
+                text=doc.text,
+                metadata=doc.metadata
+            ))
+            
+            if len(results) >= top_k:
+                break
+        
+        return results
+    
+    def search_by_embedding(
+        self,
+        embedding: np.ndarray,
+        top_k: int = 10,
+        threshold: Optional[float] = None
+    ) -> List[SearchResult]:
+        """Search using a pre-computed embedding.
+        
+        Args:
+            embedding: Query embedding vector.
+            top_k: Maximum number of results.
+            threshold: Minimum similarity threshold.
+        
+        Returns:
+            List of SearchResult objects.
+        """
+        if not self._documents:
+            return []
+        
+        if self._index_dirty:
+            self._rebuild_index()
+        
+        # Normalize if needed
+        norm = np.linalg.norm(embedding)
+        if norm > 0:
+            embedding = embedding / norm
+        
+        similarities = np.dot(self._embeddings_matrix, embedding)
+        effective_threshold = threshold if threshold is not None else self._similarity_threshold
+        
+        # Get top-k
+        top_indices = np.argsort(similarities)[::-1][:top_k]
+        
+        results = []
+        for idx in top_indices:
+            score = float(similarities[idx])
+            if score < effective_threshold:
+                continue
+            
+            doc_id = self._document_ids[idx]
+            doc = self._documents[doc_id]
+            
+            results.append(SearchResult(
+                document_id=doc_id,
+                score=score,
+                text=doc.text,
+                metadata=doc.metadata
+            ))
+        
+        return results
+    
+    def get_document(self, doc_id: str) -> Optional[Document]:
+        """Get a document by ID.
+        
+        Args:
+            doc_id: Document ID.
+        
+        Returns:
+            Document object or None if not found.
+        """
+        return self._documents.get(doc_id)
+    
+    def get_similar_documents(
+        self,
+        doc_id: str,
+        top_k: int = 10,
+        include_self: bool = False
+    ) -> List[SearchResult]:
+        """Find documents similar to a given document.
+        
+        Args:
+            doc_id: Source document ID.
+            top_k: Maximum number of results.
+            include_self: Whether to include the source document in results.
+        
+        Returns:
+            List of SearchResult objects.
+        """
+        doc = self._documents.get(doc_id)
+        if doc is None or doc.embedding is None:
+            return []
+        
+        results = self.search_by_embedding(
+            doc.embedding,
+            top_k=top_k + (0 if include_self else 1)
+        )
+        
+        if not include_self:
+            results = [r for r in results if r.document_id != doc_id]
+        
+        return results[:top_k]
diff --git a/src/backend/search/tests/__init__.py b/src/backend/search/tests/__init__.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/src/backend/search/tests/__init__.py
@@ -0,0 +1,1 @@
+"""Tests for semantic search module."""
diff --git a/src/backend/search/tests/test_semantic_search.py b/src/backend/search/tests/test_semantic_search.py
new file mode 100644
index 0000000..1111111
--- /dev/null
+++ b/src/backend/search/tests/test_semantic_search.py
@@ -0,0 +1,250 @@
+"""Tests for semantic search functionality.
+
+Tests the EmbeddingService and SemanticSearchEngine classes.
+"""
+
+import pytest
+import numpy as np
+
+# Skip all tests if sentence-transformers not installed
+pytorch_available = True
+try:
+    import torch
+    from sentence_transformers import SentenceTransformer
+except ImportError:
+    pytorch_available = False
+
+pytestmark = pytest.mark.skipif(
+    not pytorch_available,
+    reason="sentence-transformers not installed"
+)
+
+
+class TestEmbeddingService:
+    """Tests for EmbeddingService."""
+    
+    @pytest.fixture
+    def service(self):
+        """Create embedding service fixture."""
+        from src.backend.search.embedding_service import EmbeddingService
+        return EmbeddingService()
+    
+    def test_embed_text_returns_array(self, service):
+        """Test that embed_text returns numpy array."""
+        embedding = service.embed_text("Hello world")
+        assert isinstance(embedding, np.ndarray)
+    
+    def test_embed_text_dimension(self, service):
+        """Test embedding dimension is 768 for all-mpnet-base-v2."""
+        embedding = service.embed_text("Test text")
+        assert embedding.shape == (768,)
+    
+    def test_embed_text_normalized(self, service):
+        """Test that embeddings are L2 normalized."""
+        embedding = service.embed_text("Test text", normalize=True)
+        norm = np.linalg.norm(embedding)
+        assert abs(norm - 1.0) < 1e-5
+    
+    def test_embed_texts_batch(self, service):
+        """Test batch embedding."""
+        texts = ["Hello", "World", "Test"]
+        embeddings = service.embed_texts(texts)
+        assert embeddings.shape == (3, 768)
+    
+    def test_embed_texts_empty(self, service):
+        """Test empty input returns empty array."""
+        embeddings = service.embed_texts([])
+        assert len(embeddings) == 0
+    
+    def test_similarity_same_text(self, service):
+        """Test similarity of identical texts is ~1.0."""
+        sim = service.similarity("Hello world", "Hello world")
+        assert sim > 0.99
+    
+    def test_similarity_different_texts(self, service):
+        """Test similarity of different texts is lower."""
+        sim = service.similarity(
+            "Python programming language",
+            "Cooking recipes for dinner"
+        )
+        assert sim < 0.5
+    
+    def test_similarity_semantic(self, service):
+        """Test semantic similarity works."""
+        # Similar meaning, different words
+        sim_similar = service.similarity(
+            "The cat sat on the mat",
+            "A feline rested on the rug"
+        )
+        # Completely different topics
+        sim_different = service.similarity(
+            "The cat sat on the mat",
+            "Stock market analysis report"
+        )
+        assert sim_similar > sim_different
+    
+    def test_batch_similarity(self, service):
+        """Test batch similarity computation."""
+        query = "Machine learning"
+        candidates = [
+            "Deep learning neural networks",
+            "Cooking pasta recipes",
+            "Artificial intelligence research"
+        ]
+        scores = service.batch_similarity(query, candidates)
+        assert len(scores) == 3
+        # ML should be more similar to AI/DL than cooking
+        assert scores[0] > scores[1]
+        assert scores[2] > scores[1]
+
+
+class TestSemanticSearchEngine:
+    """Tests for SemanticSearchEngine."""
+    
+    @pytest.fixture
+    def engine(self):
+        """Create search engine fixture."""
+        from src.backend.search.semantic_search import SemanticSearchEngine
+        return SemanticSearchEngine()
+    
+    @pytest.fixture
+    def populated_engine(self, engine):
+        """Create engine with sample documents."""
+        documents = [
+            {"id": "1", "text": "Python programming tutorial for beginners"},
+            {"id": "2", "text": "Machine learning and artificial intelligence"},
+            {"id": "3", "text": "Web development with JavaScript and React"},
+            {"id": "4", "text": "Data science with Python and pandas"},
+            {"id": "5", "text": "Cooking recipes for Italian pasta dishes"},
+        ]
+        engine.add_documents(documents)
+        return engine
+    
+    def test_add_document(self, engine):
+        """Test adding a single document."""
+        engine.add_document("test", "Test document")
+        assert engine.document_count == 1
+    
+    def test_add_documents_batch(self, engine):
+        """Test adding multiple documents."""
+        docs = [
+            {"id": "1", "text": "First document"},
+            {"id": "2", "text": "Second document"},
+        ]
+        count = engine.add_documents(docs)
+        assert count == 2
+        assert engine.document_count == 2
+    
+    def test_remove_document(self, populated_engine):
+        """Test removing a document."""
+        initial_count = populated_engine.document_count
+        removed = populated_engine.remove_document("1")
+        assert removed is True
+        assert populated_engine.document_count == initial_count - 1
+    
+    def test_remove_nonexistent(self, engine):
+        """Test removing nonexistent document returns False."""
+        removed = engine.remove_document("nonexistent")
+        assert removed is False
+    
+    def test_clear(self, populated_engine):
+        """Test clearing all documents."""
+        populated_engine.clear()
+        assert populated_engine.document_count == 0
+    
+    def test_search_returns_results(self, populated_engine):
+        """Test search returns results."""
+        results = populated_engine.search("Python programming")
+        assert len(results) > 0
+    
+    def test_search_relevance(self, populated_engine):
+        """Test search returns relevant results first."""
+        results = populated_engine.search("Python programming", top_k=3)
+        # Python-related docs should rank higher than cooking
+        doc_ids = [r.document_id for r in results]
+        assert "5" not in doc_ids[:2]  # Cooking should not be in top 2
+    
+    def test_search_top_k(self, populated_engine):
+        """Test top_k limits results."""
+        results = populated_engine.search("programming", top_k=2)
+        assert len(results) <= 2
+    
+    def test_search_threshold(self, populated_engine):
+        """Test threshold filters low-scoring results."""
+        results = populated_engine.search(
+            "quantum physics",
+            threshold=0.9  # Very high threshold
+        )
+        # Should return few or no results for unrelated query
+        assert len(results) == 0 or all(r.score >= 0.9 for r in results)
+    
+    def test_search_empty_index(self, engine):
+        """Test search on empty index returns empty list."""
+        results = engine.search("test query")
+        assert results == []
+    
+    def test_search_result_structure(self, populated_engine):
+        """Test search result has correct structure."""
+        results = populated_engine.search("Python", top_k=1)
+        assert len(results) == 1
+        result = results[0]
+        assert hasattr(result, "document_id")
+        assert hasattr(result, "score")
+        assert hasattr(result, "text")
+        assert hasattr(result, "metadata")
+    
+    def test_search_with_metadata(self, engine):
+        """Test search with metadata filter."""
+        engine.add_document(
+            "1", "Python tutorial",
+            metadata={"category": "programming", "level": "beginner"}
+        )
+        engine.add_document(
+            "2", "Python advanced",
+            metadata={"category": "programming", "level": "advanced"}
+        )
+        engine.add_document(
+            "3", "Cooking basics",
+            metadata={"category": "cooking", "level": "beginner"}
+        )
+        
+        # Filter by category
+        results = engine.search(
+            "tutorial",
+            filter_metadata={"category": "programming"}
+        )
+        for r in results:
+            assert r.metadata.get("category") == "programming"
+    
+    def test_get_document(self, populated_engine):
+        """Test retrieving document by ID."""
+        doc = populated_engine.get_document("1")
+        assert doc is not None
+        assert doc.id == "1"
+    
+    def test_get_document_not_found(self, engine):
+        """Test retrieving nonexistent document returns None."""
+        doc = engine.get_document("nonexistent")
+        assert doc is None
+    
+    def test_get_similar_documents(self, populated_engine):
+        """Test finding similar documents."""
+        # Doc 1 is Python tutorial, should be similar to doc 4 (Python data science)
+        results = populated_engine.get_similar_documents("1", top_k=2)
+        assert len(results) > 0
+        # Should not include the source document
+        assert all(r.document_id != "1" for r in results)
+    
+    def test_search_result_to_dict(self, populated_engine):
+        """Test SearchResult.to_dict() method."""
+        results = populated_engine.search("Python", top_k=1)
+        result_dict = results[0].to_dict()
+        assert "document_id" in result_dict
+        assert "score" in result_dict
+        assert "text" in result_dict
+        assert "metadata" in result_dict
+    
+    def test_embedding_dimension_property(self, engine):
+        """Test embedding_service property access."""
+        dim = engine.embedding_service.embedding_dimension
+        assert dim == 768
