"""Anthropic Claude-based Builder and Auditor implementations

Per models.yaml configuration:
- Claude Opus 4.5 for high-risk auditing
- Claude Sonnet 4.5 for progressive strategy auditing
- Complementary to OpenAI models for dual auditing

This module provides Anthropic API integration for when
ModelRouter selects Claude models based on category/quota.
"""

import os
import json
from typing import Dict, List, Optional

try:
    from anthropic import Anthropic
except ImportError:
    # Graceful degradation if anthropic package not installed
    Anthropic = None

from .llm_client import BuilderResult, AuditorResult
from .journal_reader import get_prevention_prompt_injection


class AnthropicBuilderClient:
    """Builder implementation using Anthropic Claude API

    Currently used for:
    - Test generation (claude-sonnet-4-5 per models.yaml)
    - Escalation scenarios when OpenAI quota exhausted
    """

    def __init__(self, api_key: Optional[str] = None):
        """Initialize Anthropic client

        Args:
            api_key: Anthropic API key (defaults to ANTHROPIC_API_KEY env var)
        """
        if Anthropic is None:
            raise ImportError(
                "anthropic package not installed. "
                "Install with: pip install anthropic"
            )

        self.client = Anthropic(api_key=api_key or os.getenv("ANTHROPIC_API_KEY"))

    def execute_phase(
        self,
        phase_spec: Dict,
        file_context: Optional[Dict] = None,
        max_tokens: Optional[int] = None,
        model: str = "claude-sonnet-4-5",
        project_rules: Optional[List] = None,
        run_hints: Optional[List] = None
    ) -> BuilderResult:
        """Execute a phase using Claude

        Args:
            phase_spec: Phase specification
            file_context: Repository file context
            max_tokens: Token budget
            model: Claude model (claude-opus-4-5, claude-sonnet-4-5, etc.)
            project_rules: Persistent learned rules
            run_hints: Within-run hints

        Returns:
            BuilderResult with patch and metadata
        """
        try:
            # Build system prompt
            system_prompt = self._build_system_prompt()

            # Build user prompt
            user_prompt = self._build_user_prompt(
                phase_spec, file_context, project_rules, run_hints
            )

            # Call Anthropic API
            # Use 16384 tokens to avoid truncation of large patches
            response = self.client.messages.create(
                model=model,
                max_tokens=max_tokens or 16384,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}],
                temperature=0.2
            )

            # Extract content
            content = response.content[0].text

            # Parse and extract patch content
            patch_content = ""
            summary = "Generated by Claude"

            try:
                # Try to parse as direct JSON first
                result_json = json.loads(content)
                patch_content = result_json.get("patch_content", "")
                summary = result_json.get("summary", "Generated by Claude")
            except json.JSONDecodeError:
                # Check if wrapped in markdown code fence
                if "```json" in content:
                    # Extract JSON from markdown code block
                    json_start = content.find("```json") + 7
                    json_end = content.find("```", json_start)
                    if json_end > json_start:
                        json_str = content[json_start:json_end].strip()
                        try:
                            result_json = json.loads(json_str)
                            patch_content = result_json.get("patch_content", "")
                            summary = result_json.get("summary", "Generated by Claude")
                        except json.JSONDecodeError:
                            pass

                # If still no patch, try to extract raw diff content
                if not patch_content:
                    patch_content = self._extract_diff_from_text(content)
                    if not patch_content:
                        # Last resort: use entire content
                        patch_content = content

            return BuilderResult(
                success=True,
                patch_content=patch_content,
                builder_messages=[summary],
                tokens_used=response.usage.input_tokens + response.usage.output_tokens,
                model_used=model
            )

        except Exception as e:
            # Return error result
            return BuilderResult(
                success=False,
                patch_content="",
                builder_messages=[f"Builder error: {str(e)}"],
                tokens_used=0,
                model_used=model,
                error=str(e)
            )

    def _extract_diff_from_text(self, text: str) -> str:
        """Extract git diff content from text that may contain explanations.

        Args:
            text: Raw text that may contain diff content

        Returns:
            Extracted diff content or empty string
        """
        lines = text.split('\n')
        diff_lines = []
        in_diff = False

        for line in lines:
            # Start of diff
            if line.startswith('diff --git'):
                in_diff = True
                diff_lines.append(line)
            # Continuation of diff
            elif in_diff:
                # Check if still in diff (various diff markers)
                if (line.startswith(('index ', '---', '+++', '@@', '+', '-', ' ')) or
                    line.startswith('new file mode') or
                    line.startswith('deleted file mode') or
                    line.startswith('similarity index') or
                    line.startswith('rename from') or
                    line.startswith('rename to') or
                    line == ''):
                    diff_lines.append(line)
                # Next diff section
                elif line.startswith('diff --git'):
                    diff_lines.append(line)
                # End of diff (explanatory text or other content)
                else:
                    # Stop if we hit markdown fence or explanatory text
                    if line.startswith('```') or line.startswith('#'):
                        break

        return '\n'.join(diff_lines) if diff_lines else ""

    def _build_system_prompt(self) -> str:
        """Build system prompt for Claude Builder"""
        base_prompt = """You are an expert software engineer working on an autonomous build system.

Your task is to generate code changes (patches) based on phase specifications.

CRITICAL REQUIREMENTS:
1. Output ONLY a raw git diff format patch
2. Do NOT wrap it in JSON, markdown code blocks, or any other format
3. Do NOT add explanatory text before or after the patch
4. Start directly with: diff --git a/path/to/file.py b/path/to/file.py

GIT DIFF FORMAT RULES:
- Each file change MUST start with: diff --git a/PATH b/PATH
- Followed by: index HASH..HASH
- Then: --- a/PATH and +++ b/PATH
- Then: @@ -LINE,COUNT +LINE,COUNT @@ CONTEXT
- Then the actual changes with +/- prefixes
- Use COMPLETE file paths from repository root (e.g., src/backend/api/health.py)
- Do NOT use relative or partial paths (e.g., backend/api/health.py is WRONG)

Requirements:
- Generate clean, production-quality code
- Follow best practices (type hints, docstrings, tests)
- Apply learned rules from project history
- Output ONLY the raw git diff format patch (no JSON, no markdown fences, no explanations)"""

        # Inject prevention rules from debug journal
        try:
            prevention_rules = get_prevention_prompt_injection()
            if prevention_rules:
                base_prompt += "\n\n" + prevention_rules
        except Exception:
            # Gracefully continue if prevention rules can't be loaded
            pass

        return base_prompt

    def _build_user_prompt(
        self,
        phase_spec: Dict,
        file_context: Optional[Dict],
        project_rules: Optional[List],
        run_hints: Optional[List]
    ) -> str:
        """Build user prompt with phase details"""
        prompt_parts = [
            "# Phase Specification",
            f"Description: {phase_spec.get('description', '')}",
            f"Category: {phase_spec.get('task_category', 'general')}",
            f"Complexity: {phase_spec.get('complexity', 'medium')}",
        ]

        if phase_spec.get("acceptance_criteria"):
            prompt_parts.append("\nAcceptance Criteria:")
            for criteria in phase_spec["acceptance_criteria"]:
                prompt_parts.append(f"- {criteria}")

        if project_rules:
            prompt_parts.append("\n# Learned Rules (must follow):")
            for rule in project_rules[:10]:  # Top 10 rules
                # Support both dict-based rules and LearnedRule dataclasses
                if isinstance(rule, dict):
                    text = rule.get("rule_text") or rule.get("constraint") or ""
                else:
                    text = getattr(rule, "constraint", str(rule))
                if text:
                    prompt_parts.append(f"- {text}")

        if run_hints:
            prompt_parts.append("\n# Hints from earlier phases:")
            for hint in run_hints[:5]:  # Recent hints
                # Support both dict-based hints and RunRuleHint dataclasses
                if isinstance(hint, dict):
                    text = hint.get("hint_text", "")
                else:
                    text = getattr(hint, "hint_text", str(hint))
                if text:
                    prompt_parts.append(f"- {text}")

        if file_context:
            prompt_parts.append("\n# Repository Context:")
            # Extract existing_files dict (autonomous_executor returns {"existing_files": {path: content}})
            files = file_context.get("existing_files", file_context)

            # DEBUG: Log types and structure
            import logging
            logger = logging.getLogger(__name__)
            logger.info(f"[DEBUG] file_context type: {type(file_context)}")
            logger.info(f"[DEBUG] file_context keys: {list(file_context.keys()) if isinstance(file_context, dict) else 'NOT A DICT'}")
            logger.info(f"[DEBUG] files type: {type(files)}")
            logger.info(f"[DEBUG] files keys (first 3): {list(files.keys())[:3] if isinstance(files, dict) else 'NOT A DICT'}")

            for file_path, content in list(files.items())[:5]:
                logger.info(f"[DEBUG] Processing file: {file_path}")
                logger.info(f"[DEBUG]   content type: {type(content)}")
                logger.info(f"[DEBUG]   content preview: {str(content)[:100]}")

                prompt_parts.append(f"\n## {file_path}")
                # Show first 500 chars without literal "..." to avoid teaching model bad habits
                if isinstance(content, str):
                    prompt_parts.append(f"```\n{content[:500]}\n```")
                else:
                    # Fallback for unexpected types
                    logger.warning(f"[DEBUG] Content is not a string! Type: {type(content)}, Value: {content}")
                    prompt_parts.append(f"```\n{str(content)[:500]}\n```")

        return "\n".join(prompt_parts)


class AnthropicAuditorClient:
    """Auditor implementation using Anthropic Claude API

    Used for:
    - Primary auditing in progressive strategies (claude-sonnet-4-5)
    - High-risk auditing in best_first strategies (claude-opus-4-5)
    - Dual auditing partner with OpenAI
    """

    def __init__(self, api_key: Optional[str] = None):
        """Initialize Anthropic client

        Args:
            api_key: Anthropic API key (defaults to ANTHROPIC_API_KEY env var)
        """
        if Anthropic is None:
            raise ImportError(
                "anthropic package not installed. "
                "Install with: pip install anthropic"
            )

        self.client = Anthropic(api_key=api_key or os.getenv("ANTHROPIC_API_KEY"))

    def review_patch(
        self,
        patch_content: str,
        phase_spec: Dict,
        file_context: Optional[Dict] = None,
        max_tokens: Optional[int] = None,
        model: str = "claude-sonnet-4-5",
        project_rules: Optional[List] = None,
        run_hints: Optional[List] = None
    ) -> AuditorResult:
        """Review patch using Claude

        Args:
            patch_content: Git diff format patch
            phase_spec: Phase specification
            file_context: Repository context
            max_tokens: Token budget
            model: Claude model
            project_rules: Learned rules
            run_hints: Within-run hints from earlier phases

        Returns:
            AuditorResult with issues found
        """
        try:
            # Build system prompt
            system_prompt = self._build_system_prompt()

            # Build user prompt
            user_prompt = self._build_user_prompt(
                patch_content, phase_spec, file_context, project_rules, run_hints
            )

            # Call Anthropic API
            response = self.client.messages.create(
                model=model,
                max_tokens=max_tokens or 2048,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}],
                temperature=0.1  # Low temperature for consistent auditing
            )

            # Extract content
            content = response.content[0].text

            # Parse JSON response
            try:
                result_json = json.loads(content)
            except json.JSONDecodeError:
                # Fallback: treat as text review
                result_json = {
                    "approved": False,
                    "issues": [{"severity": "major", "description": content}],
                    "summary": "Review completed"
                }

            return AuditorResult(
                approved=result_json.get("approved", False),
                issues=result_json.get("issues", []),
                summary=result_json.get("summary", ""),
                model_used=model,
                total_tokens=response.usage.input_tokens + response.usage.output_tokens,
                metadata={
                    "input_tokens": response.usage.input_tokens,
                    "output_tokens": response.usage.output_tokens,
                    "stop_reason": response.stop_reason
                }
            )

        except Exception as e:
            # Return error result
            return AuditorResult(
                approved=False,
                issues=[{
                    "severity": "critical",
                    "category": "auditor_error",
                    "description": f"Auditor error: {str(e)}"
                }],
                summary=f"Auditor failed: {str(e)}",
                model_used=model,
                total_tokens=0,
                metadata={"error": str(e)}
            )

    def _build_system_prompt(self) -> str:
        """Build system prompt for Claude Auditor"""
        base_prompt = """You are an expert code reviewer for an autonomous build system.

Your task is to review code patches for:
- Security vulnerabilities (OWASP Top 10)
- Type safety issues
- Edge cases and error handling
- Performance problems
- Best practice violations

Output Format (JSON):
{
  "approved": true/false,
  "issues": [
    {
      "severity": "minor|major|critical",
      "category": "security|types|logic|performance|style",
      "description": "Detailed issue description",
      "file_path": "path/to/file.py",
      "line_number": 42
    }
  ],
  "summary": "Brief review summary"
}

Approval Criteria:
- REJECT if any critical or major issues
- APPROVE if only minor issues or no issues"""

        # Inject prevention rules from debug journal
        try:
            prevention_rules = get_prevention_prompt_injection()
            if prevention_rules:
                base_prompt += "\n\n" + prevention_rules
        except Exception:
            # Gracefully continue if prevention rules can't be loaded
            pass

        return base_prompt

    def _build_user_prompt(
        self,
        patch_content: str,
        phase_spec: Dict,
        file_context: Optional[Dict],
        project_rules: Optional[List],
        run_hints: Optional[List] = None
    ) -> str:
        """Build user prompt with patch to review"""
        prompt_parts = [
            "# Patch to Review",
            f"```diff\n{patch_content}\n```",
            f"\n# Phase Context",
            f"Category: {phase_spec.get('task_category', 'general')}",
            f"Description: {phase_spec.get('description', '')}",
        ]

        if project_rules:
            prompt_parts.append("\n# Project Rules (check compliance):")
            for rule in project_rules[:10]:
                prompt_parts.append(f"- {rule.get('rule_text', '')}")

        if run_hints:
            prompt_parts.append("\n# Recent Run Hints:")
            for hint in run_hints[:5]:
                prompt_parts.append(f"- {hint}")

        prompt_parts.append("\nProvide your review as a JSON response.")

        return "\n".join(prompt_parts)
