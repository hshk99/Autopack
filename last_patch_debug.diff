diff --git a/src/research/gatherers/web_scraper.py b/src/research/gatherers/web_scraper.py
index 1111111..2222222 100644
--- a/src/research/gatherers/web_scraper.py
+++ b/src/research/gatherers/web_scraper.py
@@ -1,18 +1,70 @@
-"""
-Web Scraper Module
+"""Web Scraper Module
 
 This module is responsible for scraping web content and interfacing with the content extractor.
+It provides robust web scraping capabilities with rate limiting, robots.txt compliance, and
+comprehensive error handling.
 """
 
 import requests
+import time
 from bs4 import BeautifulSoup
-from typing import List, Dict
+from typing import List, Dict, Optional, Any
+from urllib.parse import urlparse
+from urllib.robotparser import RobotFileParser
 from src.research.gatherers.content_extractor import ContentExtractor
 
 
 class WebScraper:
-    def __init__(self):
+    """Web scraper with rate limiting and robots.txt compliance."""
+
+    def __init__(self, min_seconds_per_domain: float = 1.0):
+        """
+        Initialize the web scraper.
+
+        :param min_seconds_per_domain: Minimum seconds between requests to the same domain.
+        """
         self.extractor = ContentExtractor()
+        self.min_seconds_per_domain = min_seconds_per_domain
+        self.last_request_times: Dict[str, float] = {}
+        self.user_agent = "AutopackResearchBot/1.0"
+
+    def _get_domain(self, url: str) -> str:
+        """Extract domain from URL."""
+        parsed = urlparse(url)
+        return parsed.netloc
+
+    def _enforce_rate_limit(self, domain: str) -> None:
+        """Enforce rate limiting per domain."""
+        if domain in self.last_request_times:
+            elapsed = time.time() - self.last_request_times[domain]
+            if elapsed < self.min_seconds_per_domain:
+                time.sleep(self.min_seconds_per_domain - elapsed)
+        self.last_request_times[domain] = time.time()
+
+    def _allowed_by_robots(self, url: str) -> bool:
+        """Check if URL is allowed by robots.txt."""
+        try:
+            parsed = urlparse(url)
+            robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
+            rp = RobotFileParser()
+            rp.set_url(robots_url)
+            rp.read()
+            return rp.can_fetch(self.user_agent, url)
+        except Exception:
+            # If we can't read robots.txt, assume allowed
+            return True
+
+    def _validate_url(self, url: str) -> None:
+        """Validate URL format and scheme."""
+        if not url:
+            raise ValueError("URL cannot be empty")
+        
+        parsed = urlparse(url)
+        if not parsed.scheme or not parsed.netloc:
+            raise ValueError(f"Invalid URL format: {url}")
+        
+        if parsed.scheme not in ["http", "https"]:
+            raise ValueError(f"Unsupported URL scheme: {parsed.scheme}")
 
     def fetch_html(self, url: str) -> str:
         """
@@ -3,31 +3,60 @@ class WebScraper:
 
         :param url: The URL to fetch content from.
         :return: The HTML content as a string.
+        :raises ValueError: If URL is invalid or content type is unsupported.
+        :raises PermissionError: If robots.txt disallows access.
+        :raises Exception: If HTTP request fails.
         """
+        self._validate_url(url)
+        
+        if not self._allowed_by_robots(url):
+            raise PermissionError(f"Access to {url} disallowed by robots.txt")
+        
+        domain = self._get_domain(url)
+        self._enforce_rate_limit(domain)
+        
         try:
-            response = requests.get(url)
+            headers = {"User-Agent": self.user_agent}
+            response = requests.get(url, headers=headers, timeout=30)
             response.raise_for_status()
+            
+            content_type = response.headers.get("content-type", "")
+            if "text/html" not in content_type.lower():
+                raise ValueError(f"Unsupported content type: {content_type}")
+            
             return response.text
         except requests.RequestException as e:
-            print(f"Error fetching {url}: {e}")
-            return ""
+            raise Exception(f"Error fetching {url}: {e}")
+
+    def fetch_content(self, url: str) -> str:
+        """Alias for fetch_html for backward compatibility."""
+        return self.fetch_html(url)
 
-    def scrape_and_process(self, url: str) -> Dict[str, List[str]]:
+    def parse_content(self, html: str) -> str:
+        """Parse HTML content and extract text."""
+        soup = BeautifulSoup(html, 'html.parser')
+        return soup.get_text(strip=True)
+
+    def scrape_and_process(self, url: str) -> Dict[str, Any]:
         """
         Scrapes a URL and processes the content.
 
         :param url: The URL to scrape.
         :return: A dictionary containing deduplicated, categorized content and identified gaps.
         """
-        html = self.fetch_html(url)
-        if not html:
-            return {"error": "Failed to fetch content"}
-
-        soup = BeautifulSoup(html, 'html.parser')
-        text_content = soup.get_text()
-        processed_content = self.extractor.process_html(text_content)
-        return processed_content
-
-    def scrape_multiple(self, urls: List[str]) -> List[Dict[str, List[str]]]:
+        try:
+            html = self.fetch_html(url)
+            if not html:
+                return {"error": "Failed to fetch content", "url": url}
+
+            soup = BeautifulSoup(html, 'html.parser')
+            text_content = soup.get_text()
+            processed_content = self.extractor.process_html(text_content)
+            processed_content["url"] = url
+            return processed_content
+        except Exception as e:
+            return {"error": str(e), "url": url}
+
+    def scrape_multiple(self, urls: List[str]) -> List[Dict[str, Any]]:
         """
         Scrapes multiple URLs and processes their content.
 
@@ -58,6 +58,6 @@ class WebScraper:
             results.append(result)
         return results
 
-    def aggregate_results(self, results: List[Dict[str, List[str]]]) -> Dict[str, List[str]]:
+    def aggregate_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
         """
         Aggregates results from multiple scrapes.
 
@@ -73,13 +73,29 @@ class WebScraper:
                 "advertisement": [],
                 "other": []
             },
-            "gaps": []
+            "gaps": [],
+            "sources": [],
+            "errors": []
         }
+        
         for result in results:
+            if "error" in result:
+                aggregated["errors"].append({
+                    "url": result.get("url"),
+                    "error": result["error"]
+                })
+                continue
+            
             aggregated["deduplicated"].extend(result.get("deduplicated", []))
+            
             for category, items in result.get("categorized", {}).items():
-                aggregated["categorized"][category].extend(items)
+                if category in aggregated["categorized"]:
+                    aggregated["categorized"][category].extend(items)
+            
             aggregated["gaps"].extend(result.get("gaps", []))
+            
+            if "url" in result:
+                aggregated["sources"].append(result["url"])
 
         # Deduplicate aggregated content
         aggregated["deduplicated"] = list(set(aggregated["deduplicated"]))
@@ -3,6 +3,6 @@ class WebScraper:
 
         return aggregated
 
-    def run(self, urls: List[str]) -> Dict[str, List[str]]:
+    def run(self, urls: List[str]) -> Dict[str, Any]:
         """
         Runs the web scraper on a list of URLs and aggregates the results.
 
@@ -99,4 +99,3 @@ class WebScraper:
         results = self.scrape_multiple(urls)
         aggregated_results = self.aggregate_results(results)
         return aggregated_results
-
diff --git a/src/research/gatherers/content_extractor.py b/src/research/gatherers/content_extractor.py
index 1111111..2222222 100644
--- a/src/research/gatherers/content_extractor.py
+++ b/src/research/gatherers/content_extractor.py
@@ -1,14 +1,29 @@
-"""
-Content Extractor Module
+"""Content Extractor Module
 
 This module is responsible for extracting and processing content from web pages.
 It deduplicates content, categorizes it by type, and identifies gaps.
+Supports HTML, JSON, and plain text extraction with structured output.
 """
 
-from typing import List, Dict, Any
+from typing import List, Dict, Any, Optional
+from dataclasses import dataclass
 import re
+import json
+from bs4 import BeautifulSoup
+from urllib.parse import urljoin, urlparse
+
+
+@dataclass
+class ExtractedContent:
+    """Structured representation of extracted content."""
+    text: str
+    links: List[str]
+    code_blocks: List[str]
+    source_url: Optional[str] = None
 
 
 class ContentExtractor:
+    """Extracts and processes content from various formats."""
+
     def __init__(self):
         self.content_store = []
 
@@ -20,9 +20,152 @@ class ContentExtractor:
         :param html: The HTML content as a string.
         :return: A list of extracted text segments.
         """
-        # Simple regex-based extraction for demonstration purposes
-        text_segments = re.findall(r'>[^<]+<', html)
-        return [segment.strip('<> ') for segment in text_segments]
+        soup = BeautifulSoup(html, 'html.parser')
+        
+        # Remove script and style elements
+        for script in soup(["script", "style"]):
+            script.decompose()
+        
+        # Get text and split into segments
+        text = soup.get_text()
+        lines = (line.strip() for line in text.splitlines())
+        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
+        text_segments = [chunk for chunk in chunks if chunk]
+        
+        return text_segments
+
+    def extract_from_html(self, html: Any) -> str:
+        """
+        Extracts clean text from HTML, removing scripts and styles.
+
+        :param html: The HTML content as a string.
+        :return: Extracted text content.
+        :raises ValueError: If html is None or not a string.
+        """
+        if html is None:
+            raise ValueError("HTML content cannot be None")
+        if not isinstance(html, str):
+            raise ValueError("HTML content must be a string")
+        
+        soup = BeautifulSoup(html, 'html.parser')
+        
+        # Remove script and style elements
+        for script in soup(["script", "style"]):
+            script.decompose()
+        
+        # Check for malformed HTML
+        if not soup.find():
+            raise ValueError("Invalid or malformed HTML")
+        
+        return soup.get_text(strip=True)
+
+    def extract_from_json(self, json_content: str) -> str:
+        """
+        Extracts content from JSON string.
+
+        :param json_content: JSON string.
+        :return: Extracted content value.
+        :raises ValueError: If JSON is invalid.
+        """
+        try:
+            data = json.loads(json_content)
+        except json.JSONDecodeError as e:
+            raise ValueError(f"Invalid JSON: {e}")
+        
+        # Look for 'content' key first, then first string value
+        if isinstance(data, dict):
+            if "content" in data:
+                return str(data["content"])
+            for value in data.values():
+                if isinstance(value, str):
+                    return value
+        
+        return str(data)
+
+    def extract_from_text(self, text: Any) -> str:
+        """
+        Extracts content from plain text.
+
+        :param text: Plain text content.
+        :return: The text content.
+        :raises ValueError: If text is None.
+        """
+        if text is None:
+            raise ValueError("Text content cannot be None")
+        return str(text)
+
+    def extract_links(self, content: Any) -> List[str]:
+        """
+        Extracts links from HTML or text content.
+
+        :param content: HTML or text content.
+        :return: List of absolute URLs.
+        :raises ValueError: If content is not a string.
+        """
+        if not isinstance(content, str):
+            raise ValueError("Content must be a string")
+        
+        links = []
+        
+        # Try parsing as HTML first
+        try:
+            soup = BeautifulSoup(content, 'html.parser')
+            for link in soup.find_all('a', href=True):
+                href = link['href']
+                # Only include absolute URLs
+                if href.startswith('http://') or href.startswith('https://'):
+                    links.append(href)
+        except Exception:
+            pass
+        
+        # Also extract URLs from plain text
+        url_pattern = r'https?://[^\s<>"]+'
+        text_urls = re.findall(url_pattern, content)
+        links.extend(text_urls)
+        
+        # Remove duplicates while preserving order
+        seen = set()
+        unique_links = []
+        for link in links:
+            if link not in seen:
+                seen.add(link)
+                unique_links.append(link)
+        
+        return unique_links
+
+    def extract_code_blocks(self, html: str) -> List[str]:
+        """
+        Extracts code blocks from HTML.
+
+        :param html: HTML content.
+        :return: List of code block contents.
+        """
+        soup = BeautifulSoup(html, 'html.parser')
+        code_blocks = []
+        
+        for code in soup.find_all(['code', 'pre']):
+            code_blocks.append(code.get_text(strip=True))
+        
+        return code_blocks
+
+    def extract(self, content: str, source_url: Optional[str] = None) -> ExtractedContent:
+        """
+        Extracts structured content from HTML.
+
+        :param content: HTML content.
+        :param source_url: Optional source URL for link resolution.
+        :return: ExtractedContent object with text, links, and code blocks.
+        """
+        text = self.extract_from_html(content)
+        links = self.extract_links(content)
+        code_blocks = self.extract_code_blocks(content)
+        
+        return ExtractedContent(
+            text=text,
+            links=links,
+            code_blocks=code_blocks,
+            source_url=source_url
+        )
 
     def deduplicate_content(self, contents: List[str]) -> List[str]:
         """
@@ -46,15 +46,30 @@ class ContentExtractor:
             "advertisement": [],
             "other": []
         }
+        
         for content in contents:
-            if "news" in content.lower():
+            content_lower = content.lower()
+            categorized = False
+            
+            # Check for news indicators
+            if any(keyword in content_lower for keyword in ["news", "breaking", "report", "announced"]):
                 categories["news"].append(content)
-            elif "blog" in content.lower():
+                categorized = True
+            
+            # Check for blog indicators
+            if any(keyword in content_lower for keyword in ["blog", "post", "article", "opinion"]):
                 categories["blog"].append(content)
-            elif "buy now" in content.lower() or "sale" in content.lower():
+                categorized = True
+            
+            # Check for advertisement indicators
+            if any(keyword in content_lower for keyword in ["buy now", "sale", "discount", "offer", "shop"]):
                 categories["advertisement"].append(content)
-            else:
+                categorized = True
+            
+            # If not categorized, put in other
+            if not categorized:
                 categories["other"].append(content)
+        
         return categories
 
     def identify_gaps(self, categorized_content: Dict[str, List[str]]) -> List[str]:
@@ -65,10 +65,14 @@ class ContentExtractor:
         :return: A list of identified gaps.
         """
         gaps = []
-        if not categorized_content["news"]:
+        
+        if not categorized_content.get("news"):
             gaps.append("Missing news content")
-        if not categorized_content["blog"]:
+        if not categorized_content.get("blog"):
             gaps.append("Missing blog content")
+        if len(categorized_content.get("other", [])) > len(categorized_content.get("news", [])) + len(categorized_content.get("blog", [])):
+            gaps.append("High proportion of uncategorized content")
+        
         return gaps
 
     def process_html(self, html: str) -> Dict[str, Any]:
@@ -82,6 +82,7 @@ class ContentExtractor:
         deduplicated = self.deduplicate_content(extracted)
         categorized = self.categorize_content(deduplicated)
         gaps = self.identify_gaps(categorized)
+        
         return {
             "deduplicated": deduplicated,
             "categorized": categorized,
