"""
Governed Apply System for Autopack

Safely applies code patches generated by the Builder to the filesystem.
Uses git apply for patch application with proper error handling.

Enhanced with self-troubleshoot capabilities:
- Post-application file validation (syntax check)
- File integrity checks before/after fallback operations
- Automatic restoration on corruption detection

Per GPT_RESPONSE18: Added symbol preservation and structural similarity validation.
"""

import logging
import re
import hashlib
import ast
from difflib import SequenceMatcher
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Set

from .repair_helpers import YamlRepairHelper, save_repair_debug
from .rollback_manager import RollbackManager
from .config import settings
from .patching.policy import PatchPolicy
from .patching.patch_sanitize import (
    sanitize_patch,
    fix_empty_file_diffs,
    repair_hunk_headers,
)
from .patching.patch_quality import validate_patch_quality
from .patching.apply_engine import (
    execute_git_apply,
    execute_manual_apply,
)
from .exceptions import ValidationError

logger = logging.getLogger(__name__)


# =============================================================================
# VALIDATION FUNCTIONS (per GPT_RESPONSE18 Q5/Q6)
# =============================================================================


def extract_python_symbols(source: str) -> Set[str]:
    """
    Extract top-level symbols from Python source using AST.

    Per GPT_RESPONSE18 Q5: Extract function and class definitions,
    plus uppercase module-level constants.

    Args:
        source: Python source code

    Returns:
        Set of symbol names (functions, classes, CONSTANTS)
    """
    try:
        tree = ast.parse(source)
        names: Set[str] = set()
        for node in tree.body:
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                names.add(node.name)
            elif isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name) and target.id.isupper():
                        names.add(target.id)
        return names
    except SyntaxError:
        return set()


def check_symbol_preservation(
    old_content: str, new_content: str, max_lost_ratio: float
) -> Tuple[bool, str]:
    """
    Check if too many symbols were lost in the patch.

    Per GPT_RESPONSE18 Q5: Reject if >30% of symbols are lost (configurable).

    Args:
        old_content: Original file content
        new_content: New file content after patch
        max_lost_ratio: Maximum ratio of symbols that can be lost (e.g., 0.3)

    Returns:
        Tuple of (is_valid, error_message)
    """
    old_symbols = extract_python_symbols(old_content)
    new_symbols = extract_python_symbols(new_content)
    lost = old_symbols - new_symbols

    if old_symbols:
        lost_ratio = len(lost) / len(old_symbols)
        if lost_ratio > max_lost_ratio:
            lost_names = ", ".join(sorted(lost)[:10])
            if len(lost) > 10:
                lost_names += f"... (+{len(lost) - 10} more)"
            return False, (
                f"symbol_preservation_violation: Lost {len(lost)}/{len(old_symbols)} symbols "
                f"({lost_ratio:.1%} > {max_lost_ratio:.0%} threshold). "
                f"Lost: [{lost_names}]"
            )

    return True, ""


def check_structural_similarity(
    old_content: str, new_content: str, min_ratio: float
) -> Tuple[bool, str]:
    """
    Check if file was drastically rewritten unexpectedly.

    Per GPT_RESPONSE18 Q6: Reject if structural similarity is <60% (configurable)
    for files >=300 lines.

    Args:
        old_content: Original file content
        new_content: New file content after patch
        min_ratio: Minimum similarity ratio required (e.g., 0.6)

    Returns:
        Tuple of (is_valid, error_message)
    """
    ratio = SequenceMatcher(None, old_content, new_content).ratio()
    if ratio < min_ratio:
        return False, (
            f"structural_similarity_violation: Similarity {ratio:.2f} below threshold {min_ratio}. "
            f"File appears to have been drastically rewritten."
        )

    return True, ""


class PatchApplyError(Exception):
    """Raised when patch application fails"""

    pass


class GovernedApplyPath:
    """
    Safely applies patches to the filesystem using git apply.

    This class provides:
    - Safe patch application with validation
    - Automatic cleanup of temporary files
    - Detailed error reporting
    - File verification
    - Workspace isolation (protected paths)
    """

    # Protected paths that Builder should never modify
    # These are Autopack's own source/config directories
    PROTECTED_PATHS = [
        "src/autopack/",  # Autopack core modules
        # CRITICAL: individual core modules that must remain protected even in internal mode
        # (internal mode removes the broad "src/autopack/" prefix)
        "src/autopack/config.py",
        "src/autopack/database.py",
        "src/autopack/models.py",
        "src/autopack/governed_apply.py",
        "src/autopack/autonomous_executor.py",
        "src/autopack/main.py",
        "src/autopack/quality_gate.py",
        "config/",  # Configuration files
        ".autonomous_runs/",  # Run state and logs
        ".git/",  # Git internals
    ]

    # Paths that are always allowed (can override protection if needed)
    ALLOWED_PATHS = [
        # Core maintenance paths that Autopack may update in self-repair runs
        "src/autopack/learned_rules.py",
        "src/autopack/llm_service.py",
        "src/autopack/openai_clients.py",
        "src/autopack/gemini_clients.py",
        "src/autopack/glm_clients.py",
        # Research system deliverables live under src/autopack/research/* and must be writable in project runs.
        "src/autopack/research/",
        # Research CLI is a required deliverable in Chunk 1A (and is safe to allow in project runs).
        "src/autopack/cli/",
        # Research integration deliverables (Chunk 4) are safe, narrow subtrees under src/autopack/.
        "src/autopack/integrations/",
        "src/autopack/phases/",
        "src/autopack/autonomous/",
        "src/autopack/workflow/",
        # Diagnostics parity deliverables (follow-ups): safe internal tooling, narrow subtree.
        "src/autopack/diagnostics/",
        # Dashboard integration for handoff/prompt generation (follow-up): narrow subtree.
        "src/autopack/dashboard/",
        # Research API router integration requires narrow update to main.py (followup-4).
        "src/autopack/main.py",
        "config/models.yaml",
        # BUILD-126: Large file handling modules (Phases E2-I)
        "src/autopack/import_graph.py",
        "src/autopack/scope_refiner.py",
        "src/autopack/risk_scorer.py",
        "src/autopack/context_summarizer.py",
        "src/autopack/quality_gate.py",
    ]

    # Run types that support internal mode
    MAINTENANCE_RUN_TYPES = ["autopack_maintenance", "autopack_upgrade", "self_repair"]

    def __init__(
        self,
        workspace: Path,
        allowed_paths: List[str] = None,
        protected_paths: List[str] = None,
        autopack_internal_mode: bool = False,
        run_type: str = "project_build",
        scope_paths: List[str] = None,
        run_id: Optional[str] = None,
        phase_id: Optional[str] = None,
    ):
        """
        Initialize GovernedApplyPath.

        Args:
            workspace: Path to the workspace root directory
            allowed_paths: Additional paths to allow (overrides protection)
            protected_paths: Additional paths to protect (extends defaults)
            autopack_internal_mode: If True, allows writes to src/autopack/ (requires maintenance run_type)
            run_type: Type of run - "project_build" (default) or "autopack_maintenance"
            scope_paths: Optional list of allowed file paths (scope enforcement - Option C Layer 2)
            run_id: Optional run ID for rollback manager (BUILD-145)
            phase_id: Optional phase ID for rollback manager (BUILD-145)

        Raises:
            ValueError: If autopack_internal_mode=True but run_type is not a maintenance type

        Note on workspace isolation (per GPT_RESPONSE6 recommendations):
        - Normal project runs (project_build): PROTECTED_PATHS enforced as-is
        - Maintenance runs (autopack_maintenance): autopack_internal_mode unlocks src/autopack/
          but still protects .autonomous_runs/, .git/ unless explicitly overridden

        Note on scope enforcement (per GPT_RESPONSE - Option C Layer 2):
        - If scope_paths is provided, ONLY those paths can be modified
        - This is the second validation layer (after context loading)

        Note on rollback (BUILD-145):
        - If executor_rollback_enabled=true and run_id/phase_id provided, creates git savepoints
        - On patch apply failure, automatically rolls back to savepoint
        """
        if isinstance(workspace, str):
            workspace = Path(workspace)
        self.workspace = workspace
        self._file_backups: Dict[str, Tuple[str, str]] = {}  # path -> (hash, content)
        self.run_type = run_type
        self.autopack_internal_mode = autopack_internal_mode
        self.scope_paths = scope_paths or []  # NEW: Store scope paths for validation

        # BUILD-145: Initialize rollback manager if enabled and IDs provided
        self.rollback_manager: Optional[RollbackManager] = None
        if settings.executor_rollback_enabled and run_id and phase_id:
            self.rollback_manager = RollbackManager(workspace, run_id, phase_id)
            logger.info(f"[Rollback] Rollback manager enabled for run={run_id}, phase={phase_id}")

        # [Q7 Implementation] Validate autopack_internal_mode is only used with maintenance runs
        if autopack_internal_mode and run_type not in self.MAINTENANCE_RUN_TYPES:
            raise ValidationError(
                f"autopack_internal_mode=True only allowed for maintenance runs "
                f"(run_type must be one of {self.MAINTENANCE_RUN_TYPES}, got '{run_type}')"
            )

        # Merge default protected paths with any additional ones
        self.protected_paths = list(self.PROTECTED_PATHS)
        if protected_paths:
            self.protected_paths.extend(protected_paths)

        # [Q7 Implementation] In internal mode, unlock src/autopack/ but keep critical paths protected
        if autopack_internal_mode:
            logger.info(
                "[Isolation] autopack_internal_mode enabled - unlocking src/autopack/ for maintenance"
            )
            # Remove src/autopack/ from protection, keep others
            self.protected_paths = [p for p in self.protected_paths if p != "src/autopack/"]

        # Merge default allowed paths with any additional ones
        self.allowed_paths = list(self.ALLOWED_PATHS)
        if allowed_paths:
            for path in allowed_paths:
                normalized = path.replace("\\", "/")
                if not normalized:
                    continue
                target = normalized
                if not target.endswith("/"):
                    suffix = Path(target).suffix
                    if not suffix:  # Treat as directory prefix
                        target += "/"
                self.allowed_paths.append(target)

        # Create PatchPolicy instance for path validation
        self.policy = PatchPolicy(
            protected_paths=self.protected_paths,
            allowed_paths=self.allowed_paths,
            scope_paths=self.scope_paths,
            internal_mode=autopack_internal_mode,
        )

    # =========================================================================
    # WORKSPACE ISOLATION METHODS
    # =========================================================================

    def _is_path_protected(self, file_path: str) -> bool:
        """
        Check if a file path is protected from modification.

        Args:
            file_path: Relative file path to check

        Returns:
            True if path is protected, False otherwise
        """
        # Delegate to PatchPolicy
        return self.policy.is_path_protected(file_path)

    def _extract_justification_from_patch(self, patch_content: str) -> str:
        """
        Extract Builder's justification from patch content (BUILD-127 Phase 2).

        Args:
            patch_content: Patch content to analyze

        Returns:
            Extracted justification string or generic message
        """
        # Look for common comment patterns in patches
        justification_lines = []

        for line in patch_content.split("\n")[:50]:  # Check first 50 lines
            line = line.strip()

            # Diff comments (starting with '#')
            if line.startswith("# ") and len(line) > 3:
                justification_lines.append(line[2:].strip())

            # Git commit message format
            if line.startswith("Subject:") or line.startswith("Summary:"):
                justification_lines.append(line.split(":", 1)[1].strip())

        if justification_lines:
            return " ".join(justification_lines[:3])  # First 3 lines

        return "No justification provided in patch"

    def _validate_patch_paths(self, files: List[str]) -> Tuple[bool, List[str]]:
        """
        Validate that patch does not touch protected directories or violate scope.

        This is a critical workspace isolation check that prevents Builder
        from corrupting Autopack's own source code.

        NEW (Option C - Layer 2): Also enforces scope configuration if present.

        Args:
            files: List of file paths from the patch

        Returns:
            Tuple of (is_valid, list of violations)
        """
        # Use PatchPolicy for validation
        result = self.policy.validate_paths(files)

        # Log violations for observability
        if not result.valid:
            for violation in result.violations:
                if violation.startswith("Protected path:"):
                    file_path = violation.replace("Protected path: ", "")
                    logger.warning(
                        f"[Isolation] BLOCKED: Patch attempts to modify protected path: {file_path}"
                    )
                elif violation.startswith("Outside scope:"):
                    file_path = violation.replace("Outside scope: ", "")
                    logger.warning(
                        f"[Scope] BLOCKED: Patch attempts to modify file outside scope: {file_path}"
                    )

            # Log summary
            scope_count = len([v for v in result.violations if v.startswith("Outside")])

            if scope_count > 0:
                logger.error(f"[Scope] Patch rejected - {scope_count} files outside scope")

            logger.error(
                f"[Isolation] Patch rejected - {len(result.violations)} violations (protected paths + scope)"
            )

        return result.valid, result.violations

    # =========================================================================
    # FILE VALIDATION AND INTEGRITY METHODS (Self-Troubleshoot Enhancement)
    # =========================================================================

    def _compute_file_hash(self, file_path: Path) -> Optional[str]:
        """Compute SHA256 hash of a file for integrity checking."""
        try:
            if file_path.exists():
                with open(file_path, "rb") as f:
                    return hashlib.sha256(f.read()).hexdigest()
        except Exception as e:
            logger.warning(f"Failed to compute hash for {file_path}: {e}")
        return None

    def _backup_files(self, file_paths: List[str]) -> Dict[str, Tuple[str, str]]:
        """
        Create in-memory backups of files before modification.

        Args:
            file_paths: List of relative file paths to backup

        Returns:
            Dict mapping file path to (hash, content) tuple
        """
        backups = {}
        for rel_path in file_paths:
            full_path = self.workspace / rel_path
            if full_path.exists():
                try:
                    with open(full_path, "r", encoding="utf-8") as f:
                        content = f.read()
                    file_hash = hashlib.sha256(content.encode()).hexdigest()
                    backups[rel_path] = (file_hash, content)
                    logger.debug(f"Backed up {rel_path} (hash: {file_hash[:12]}...)")
                except Exception as e:
                    logger.warning(f"Failed to backup {rel_path}: {e}")
        return backups

    def _restore_file(self, rel_path: str, backup: Tuple[str, str]) -> bool:
        """
        Restore a file from backup.

        Args:
            rel_path: Relative file path
            backup: Tuple of (hash, content)

        Returns:
            True if restoration succeeded
        """
        file_hash, content = backup
        full_path = self.workspace / rel_path
        try:
            with open(full_path, "w", encoding="utf-8") as f:
                f.write(content)
            logger.info(f"[Integrity] Restored {rel_path} from backup")
            return True
        except Exception as e:
            logger.error(f"[Integrity] Failed to restore {rel_path}: {e}")
            return False

    def _validate_python_syntax(self, file_path: Path) -> Tuple[bool, Optional[str]]:
        """
        Validate Python file syntax by attempting to compile it.

        Args:
            file_path: Path to Python file

        Returns:
            Tuple of (is_valid, error_message)
        """
        if not file_path.suffix == ".py":
            return True, None

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                source = f.read()
            compile(source, str(file_path), "exec")
            return True, None
        except SyntaxError as e:
            error_msg = f"Line {e.lineno}: {e.msg}"
            return False, error_msg
        except Exception as e:
            return False, str(e)

    def _check_merge_conflict_markers(self, file_path: Path) -> Tuple[bool, Optional[str]]:
        """
        Check if a file contains git merge conflict markers.

        These markers can be left behind by 3-way merge (-3) fallback when patches
        don't apply cleanly. They cause syntax errors and must be detected early.

        Note: We only check for '<<<<<<<' and '>>>>>>>' as these are unique to
        merge conflicts. '=======' alone is commonly used as a section divider
        in code comments (e.g., # =========) and would cause false positives.

        Args:
            file_path: Path to file to check

        Returns:
            Tuple of (has_conflicts, error_message)
        """
        # Only check for unique conflict markers, not '=======' which is used in comments
        conflict_markers = ["<<<<<<<", ">>>>>>>"]
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                for line_num, line in enumerate(f, 1):
                    for marker in conflict_markers:
                        if marker in line:
                            return True, f"Line {line_num}: merge conflict marker '{marker}' found"
            return False, None
        except Exception as e:
            logger.warning(f"Failed to check merge conflicts in {file_path}: {e}")
            return False, None

    def _validate_applied_files(self, files_modified: List[str]) -> Tuple[bool, List[str]]:
        """
        Verify files are syntactically valid after patch application.

        This is a critical self-troubleshoot check that detects corruption
        immediately after any file modification.

        Args:
            files_modified: List of relative file paths that were modified

        Returns:
            Tuple of (all_valid, list_of_corrupted_files)
        """
        corrupted_files = []

        for rel_path in files_modified:
            full_path = self.workspace / rel_path

            if not full_path.exists():
                logger.warning(f"[Validation] File does not exist after patch: {rel_path}")
                continue

            # Check for merge conflict markers (critical - prevents API crashes)
            has_conflicts, conflict_error = self._check_merge_conflict_markers(full_path)
            if has_conflicts:
                logger.error(f"[Validation] MERGE CONFLICTS: {rel_path} - {conflict_error}")
                corrupted_files.append(rel_path)
                continue  # Skip other validations - file is definitely corrupted

            # Validate Python files
            if full_path.suffix == ".py":
                is_valid, error = self._validate_python_syntax(full_path)
                if not is_valid:
                    logger.error(f"[Validation] CORRUPTED: {rel_path} - {error}")
                    corrupted_files.append(rel_path)
                else:
                    logger.debug(f"[Validation] OK: {rel_path}")

            # Validate JSON files
            elif full_path.suffix == ".json":
                try:
                    import json

                    with open(full_path, "r", encoding="utf-8") as f:
                        json.load(f)
                    logger.debug(f"[Validation] OK: {rel_path}")
                except json.JSONDecodeError as e:
                    logger.error(f"[Validation] CORRUPTED: {rel_path} - Invalid JSON: {e}")
                    corrupted_files.append(rel_path)

            # Validate YAML files
            elif full_path.suffix in [".yaml", ".yml"]:
                try:
                    import yaml

                    with open(full_path, "r", encoding="utf-8") as f:
                        content = f.read()
                    # Allow leading comments without explicit document start by prepending '---'
                    stripped = content.lstrip()
                    if stripped.startswith("#") and not stripped.startswith("---"):
                        content = "---\n" + content
                    yaml.safe_load(content)
                    logger.debug(f"[Validation] OK: {rel_path}")
                except yaml.YAMLError as e:
                    logger.error(f"[Validation] CORRUPTED: {rel_path} - Invalid YAML: {e}")
                    corrupted_files.append(rel_path)

        if corrupted_files:
            logger.error(
                f"[Validation] {len(corrupted_files)} files corrupted after patch application"
            )
            return False, corrupted_files

        logger.info(f"[Validation] All {len(files_modified)} modified files validated successfully")
        return True, []

    def _validate_content_changes(
        self,
        files_modified: List[str],
        backups: Dict[str, Tuple[str, str]],
        validation_config: Optional[Dict] = None,
    ) -> Tuple[bool, List[str]]:
        """
        Validate content changes using symbol preservation and structural similarity.

        Per GPT_RESPONSE18 Q5/Q6: Post-apply validation that checks:
        - Python files: symbol preservation (≤30% loss allowed)
        - Large files (≥300 lines): structural similarity (≥60% required)

        Args:
            files_modified: List of relative file paths that were modified
            backups: Dict mapping file path to (hash, content) tuple
            validation_config: Optional config dict with thresholds

        Returns:
            Tuple of (all_valid, list of files with issues)
        """
        # Load validation config from models.yaml or use defaults
        if validation_config is None:
            try:
                import yaml

                config_path = Path(__file__).parent.parent.parent / "config" / "models.yaml"
                if config_path.exists():
                    with open(config_path) as f:
                        models_config = yaml.safe_load(f)
                        validation_config = models_config.get("validation", {})
                else:
                    validation_config = {}
            except Exception as e:
                logger.debug(f"[Validation] Could not load validation config: {e}")
                validation_config = {}

        # Get thresholds from config
        symbol_config = validation_config.get("symbol_preservation", {})
        symbol_enabled = symbol_config.get("enabled", True)
        max_lost_ratio = symbol_config.get("max_lost_ratio", 0.3)

        similarity_config = validation_config.get("structural_similarity", {})
        similarity_enabled = similarity_config.get("enabled", True)
        min_ratio = similarity_config.get("min_ratio", 0.6)
        min_lines_for_check = similarity_config.get("min_lines_for_check", 300)

        problem_files = []

        for rel_path in files_modified:
            full_path = self.workspace / rel_path

            # Skip if file doesn't exist (was deleted) or no backup
            if not full_path.exists() or rel_path not in backups:
                continue

            # Get old content from backup
            _, old_content = backups[rel_path]

            # Read new content
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    new_content = f.read()
            except Exception as e:
                logger.warning(f"[Validation] Failed to read {rel_path}: {e}")
                continue

            old_line_count = old_content.count("\n") + 1

            # Check 1: Symbol preservation for Python files
            if symbol_enabled and full_path.suffix == ".py":
                is_valid, error = check_symbol_preservation(
                    old_content, new_content, max_lost_ratio
                )
                if not is_valid:
                    logger.warning(f"[Validation] SYMBOL_LOSS: {rel_path} - {error}")
                    problem_files.append(rel_path)
                    continue  # Skip further checks for this file

            # Check 2: Structural similarity for large files
            if similarity_enabled and old_line_count >= min_lines_for_check:
                is_valid, error = check_structural_similarity(old_content, new_content, min_ratio)
                if not is_valid:
                    logger.warning(f"[Validation] SIMILARITY_LOW: {rel_path} - {error}")
                    problem_files.append(rel_path)
                    continue

        if problem_files:
            logger.warning(
                f"[Validation] {len(problem_files)} files have content validation issues: "
                f"{', '.join(problem_files[:5])}"
                + (f" (+{len(problem_files) - 5} more)" if len(problem_files) > 5 else "")
            )
            return False, problem_files

        return True, []

    def _restore_corrupted_files(
        self, corrupted_files: List[str], backups: Dict[str, Tuple[str, str]]
    ) -> Tuple[int, int]:
        """
        Attempt to restore corrupted files from backup.

        Args:
            corrupted_files: List of corrupted file paths
            backups: Dict of backups from _backup_files()

        Returns:
            Tuple of (restored_count, failed_count)
        """
        restored = 0
        failed = 0

        for rel_path in corrupted_files:
            if rel_path in backups:
                if self._restore_file(rel_path, backups[rel_path]):
                    restored += 1
                else:
                    failed += 1
            else:
                # No backup available - file was new, just delete it
                full_path = self.workspace / rel_path
                try:
                    if self._is_path_protected(rel_path):
                        logger.error(
                            "[Integrity] Refusing to delete protected file %s even though it appears 'new' "
                            "(no backup available). Treating as failure.",
                            rel_path,
                        )
                        failed += 1
                        continue
                    full_path.unlink()
                    logger.info(f"[Integrity] Removed corrupted new file: {rel_path}")
                    restored += 1
                except Exception as e:
                    logger.error(f"[Integrity] Failed to remove {rel_path}: {e}")
                    failed += 1

        return restored, failed

    # =========================================================================
    # PATCH REPAIR METHODS
    # =========================================================================

    def _repair_hunk_headers(self, patch_content: str) -> str:
        """Repair hunk headers (delegates to patch_sanitize module)."""
        return repair_hunk_headers(patch_content, workspace_path=str(self.workspace))

    def _fix_empty_file_diffs(self, patch_content: str) -> str:
        """Fix empty file diffs (delegates to patch_sanitize module)."""
        return fix_empty_file_diffs(patch_content)

    def _remove_existing_files_for_new_patches(self, patch_content: str) -> str:
        """
        Handle existing files that patches try to create as 'new file'.

        When a patch marks a file as 'new file mode' but the file already exists,
        DO NOT delete the file (this is dangerous and can cause silent data loss if the
        patch is later rejected). Instead, fail fast and require the patch to be emitted
        as a modification instead of a new-file create.

        Args:
            patch_content: Patch content

        Returns:
            Unchanged patch content. (No side effects.)
        """
        lines = patch_content.split("\n")
        i = 0

        while i < len(lines):
            line = lines[i]

            if line.startswith("diff --git"):
                # Extract file path: diff --git a/path b/path
                parts = line.split()
                if len(parts) >= 4:
                    file_path = parts[3][2:]  # Remove 'b/' prefix
                    full_path = self.workspace / file_path

                    # Check if next lines indicate new file mode
                    if i + 1 < len(lines) and lines[i + 1].startswith("new file mode"):
                        if full_path.exists():
                            # File exists but patch wants to create it - treat as an error.
                            # This prevents accidental deletion of critical modules (e.g. src/autopack/config.py).
                            if self._is_path_protected(file_path):
                                raise PatchApplyError(
                                    f"Unsafe patch: attempts to create protected file as new when it already exists: {file_path}. "
                                    f"Refuse to delete/replace protected files."
                                )
                            raise PatchApplyError(
                                f"Unsafe patch: attempts to create existing file as new: {file_path}. "
                                f"Emit this change as a modification instead of 'new file mode'."
                            )

            i += 1

        return patch_content

    def _validate_patch_quality(self, patch_content: str) -> List[str]:
        """
        Validate patch quality to detect LLM truncation/abbreviation issues.

        Delegates to patch_quality module.

        Returns:
            List of validation error messages (empty if valid)
        """
        result = validate_patch_quality(patch_content, strict_mode=False)
        return [issue.message for issue in result.issues]

    def _validate_patch_context(self, patch_content: str) -> List[str]:
        """
        BUILD-045: Validate that patch hunk context lines match actual file content.

        This detects goal drift where LLM generates patches for the wrong file state,
        preventing git apply failures due to context mismatches.

        Returns:
            List of validation error messages (empty if context matches)
        """
        import re

        errors = []

        # Parse patch to extract file paths and hunks
        current_file = None
        current_hunks = []

        lines = patch_content.split("\n")
        i = 0
        while i < len(lines):
            line = lines[i]

            # Extract file path from diff header
            if line.startswith("diff --git"):
                # Save previous file's hunks for validation
                if current_file and current_hunks:
                    file_errors = self._validate_file_hunks(current_file, current_hunks)
                    errors.extend(file_errors)

                # Extract new file path (e.g., "diff --git a/src/file.py b/src/file.py")
                parts = line.split()
                if len(parts) >= 4:
                    # Remove a/ or b/ prefix
                    current_file = parts[3].lstrip("b/")
                    current_hunks = []

            # Parse hunk header (e.g., "@@ -10,5 +12,6 @@")
            elif line.startswith("@@"):
                match = re.match(r"^@@\s+-(\d+),(\d+)\s+\+(\d+),(\d+)\s+@@", line)
                if match:
                    old_start = int(match.group(1))
                    old_count = int(match.group(2))

                    # Extract context lines from this hunk
                    hunk_lines = []
                    j = i + 1
                    while (
                        j < len(lines)
                        and not lines[j].startswith("@@")
                        and not lines[j].startswith("diff --git")
                    ):
                        hunk_lines.append(lines[j])
                        j += 1

                    # Extract context lines (lines without + or - prefix, or lines with - prefix)
                    context_lines = []
                    for hunk_line in hunk_lines:
                        if hunk_line.startswith(" ") or hunk_line.startswith("-"):
                            # Remove the prefix to get actual line content
                            context_lines.append(hunk_line[1:] if hunk_line else "")

                    if context_lines:
                        current_hunks.append(
                            {
                                "start_line": old_start,
                                "count": old_count,
                                "context": context_lines[
                                    :5
                                ],  # First 5 context lines for validation
                            }
                        )

            i += 1

        # Validate last file's hunks
        if current_file and current_hunks:
            file_errors = self._validate_file_hunks(current_file, current_hunks)
            errors.extend(file_errors)

        return errors

    def _validate_file_hunks(self, file_path: str, hunks: List[Dict]) -> List[str]:
        """
        Validate that hunk context lines match actual file content.

        Args:
            file_path: Relative path to file
            hunks: List of hunk dictionaries with start_line, count, and context

        Returns:
            List of validation error messages
        """
        errors = []

        # Check if file exists
        full_path = self.workspace / file_path
        if not full_path.exists():
            # File doesn't exist yet - this is a new file, skip validation
            return errors

        try:
            # Read actual file content
            with open(full_path, "r", encoding="utf-8", errors="ignore") as f:
                actual_lines = f.readlines()

            # Validate each hunk
            for hunk in hunks:
                start_line = hunk["start_line"]
                context = hunk["context"]

                # Check if start_line is within file bounds
                if start_line < 1 or start_line > len(actual_lines):
                    errors.append(
                        f"{file_path}: Hunk starts at line {start_line} but file only has {len(actual_lines)} lines"
                    )
                    continue

                # Check if context lines match (allowing for minor whitespace differences)
                for i, context_line in enumerate(context[:3]):  # Check first 3 context lines
                    actual_line_num = start_line + i - 1  # 0-indexed
                    if actual_line_num < 0 or actual_line_num >= len(actual_lines):
                        continue

                    actual_line = actual_lines[actual_line_num].rstrip("\n")
                    context_line_normalized = context_line.rstrip()
                    actual_line_normalized = actual_line.rstrip()

                    # Compare normalized lines (ignore trailing whitespace)
                    if context_line_normalized != actual_line_normalized:
                        # Allow minor differences (e.g., tabs vs spaces) for first line
                        if (
                            i == 0
                            and context_line_normalized.strip() == actual_line_normalized.strip()
                        ):
                            continue

                        errors.append(
                            f"{file_path}:{start_line + i}: Context mismatch - "
                            f"expected '{context_line_normalized}' but found '{actual_line_normalized}'"
                        )
                        break  # Don't flood with errors for same hunk

        except Exception as e:
            # Don't fail validation if we can't read the file - let git apply handle it
            logger.debug(f"[BUILD-045] Could not validate {file_path}: {e}")

        return errors

    def _validate_pack_schema_in_patch(self, patch_content: str) -> List[str]:
        """
        If the patch touches pack YAML files, validate them against the pack schema.
        """
        errors: List[str] = []

        # Find files in patch
        file_paths = []
        for line in patch_content.splitlines():
            if line.startswith("diff --git"):
                parts = line.split()
                if len(parts) >= 4:
                    file_path = parts[3][2:]  # strip leading b/
                    file_paths.append(file_path)

        pack_paths = [
            p for p in file_paths if p.endswith((".yaml", ".yml")) and "backend/packs/" in p
        ]

        if not pack_paths:
            return errors

        # Extract new content for each pack and validate
        for path in pack_paths:
            new_content = self._extract_new_content_from_patch(patch_content, path)
            if not new_content:
                continue
            errors.extend(self._validate_pack_schema(path, new_content))

        return errors

    def _validate_pack_schema(self, file_path: str, content: str) -> List[str]:
        """
        Validate a country pack YAML against a minimal schema:
        - Required top-level keys: name, description, version, country, domain, categories, checklists, official_sources
        - categories: non-empty list, no duplicate names, each with name/description/examples
        - checklists: non-empty list, each with name/required_documents
        - official_sources: non-empty list
        """
        import yaml

        required_keys = [
            "name",
            "description",
            "version",
            "country",
            "domain",
            "categories",
            "checklists",
            "official_sources",
        ]
        errors: List[str] = []

        try:
            content_to_load = content
            stripped = content.lstrip()
            # Always provide a document start for validation to avoid PyYAML expecting one.
            if not stripped.startswith("---"):
                content_to_load = "---\n" + content
            data = yaml.safe_load(content_to_load)
        except Exception as e:
            errors.append(f"Pack schema: YAML parse failed for {file_path}: {e}")
            return errors

        if not isinstance(data, dict):
            errors.append(f"Pack schema: Expected mapping at top level for {file_path}")
            return errors

        for key in required_keys:
            if key not in data or data.get(key) in ("", None):
                errors.append(f"Pack schema: Missing required key '{key}' in {file_path}")

        categories = data.get("categories", [])
        if not isinstance(categories, list) or not categories:
            errors.append(f"Pack schema: 'categories' must be a non-empty list in {file_path}")
        else:
            seen = set()
            for cat in categories:
                if not isinstance(cat, dict):
                    errors.append(f"Pack schema: category entry is not a mapping in {file_path}")
                    continue
                name = cat.get("name")
                if not name or not isinstance(name, str):
                    errors.append(f"Pack schema: category missing 'name' in {file_path}")
                elif name in seen:
                    errors.append(f"Pack schema: duplicate category name '{name}' in {file_path}")
                else:
                    seen.add(name)
                if not cat.get("description"):
                    errors.append(
                        f"Pack schema: category '{name or '?'}' missing description in {file_path}"
                    )
                examples = cat.get("examples", [])
                if not isinstance(examples, list) or not examples:
                    errors.append(
                        f"Pack schema: category '{name or '?'}' missing examples list in {file_path}"
                    )

        checklists = data.get("checklists", [])
        if not isinstance(checklists, list) or not checklists:
            errors.append(f"Pack schema: 'checklists' must be a non-empty list in {file_path}")
        else:
            for cl in checklists:
                if not isinstance(cl, dict):
                    errors.append(f"Pack schema: checklist entry is not a mapping in {file_path}")
                    continue
                if not cl.get("name"):
                    errors.append(f"Pack schema: checklist missing 'name' in {file_path}")
                reqs = cl.get("required_documents", [])
                if not isinstance(reqs, list) or not reqs:
                    errors.append(
                        f"Pack schema: checklist '{cl.get('name', '?')}' missing required_documents list in {file_path}"
                    )

        sources = data.get("official_sources", [])
        if not isinstance(sources, list) or not sources:
            errors.append(
                f"Pack schema: 'official_sources' must be a non-empty list in {file_path}"
            )

        return errors

    def _attempt_yaml_repair_in_patch(
        self, patch_content: str, validation_errors: List[str]
    ) -> Tuple[Optional[str], str]:
        """
        Attempt to repair YAML content within a patch.

        Extracts YAML files from the patch, attempts repair using YamlRepairHelper,
        and reconstructs the patch with repaired content.

        Args:
            patch_content: The full patch content
            validation_errors: List of validation errors (used to identify which files need repair)

        Returns:
            Tuple of (repaired_patch, repair_method) or (None, "failed")
        """
        yaml_repair = YamlRepairHelper()

        # Extract YAML file paths from validation errors
        yaml_files_to_repair = set()
        for error in validation_errors:
            # Extract file path from error message like "YAML file 'path/to/file.yaml' ..."
            match = re.search(r"YAML file '([^']+)'", error)
            if match:
                yaml_files_to_repair.add(match.group(1))

        if not yaml_files_to_repair:
            return None, "no_yaml_files_identified"

        logger.info(
            f"[YamlRepair] Attempting repair of {len(yaml_files_to_repair)} YAML files: {yaml_files_to_repair}"
        )

        # Parse the patch to extract file contents
        repaired_files = {}
        for file_path in yaml_files_to_repair:
            new_content = self._extract_new_content_from_patch(patch_content, file_path)
            if not new_content:
                logger.warning(f"[YamlRepair] Could not extract content for {file_path}")
                continue

            # Get old content if file exists
            full_path = self.workspace / file_path
            old_content = ""
            if full_path.exists():
                try:
                    old_content = full_path.read_text(encoding="utf-8")
                except Exception as e:
                    logger.warning(f"[YamlRepair] Could not read existing {file_path}: {e}")

            # Attempt repair
            error_msg = f"YAML validation failed for {file_path}"
            repaired, method = yaml_repair.attempt_repair(
                old_content, new_content, error_msg, file_path
            )

            if repaired:
                repaired_files[file_path] = (new_content, repaired, method)
                # Save debug info
                save_repair_debug(
                    file_path=file_path,
                    original=old_content,
                    attempted=new_content,
                    repaired=repaired,
                    error=error_msg,
                    method=method,
                )
            else:
                logger.warning(f"[YamlRepair] Could not repair {file_path}")
                save_repair_debug(
                    file_path=file_path,
                    original=old_content,
                    attempted=new_content,
                    repaired=None,
                    error=error_msg,
                    method=method,
                )

        if not repaired_files:
            return None, "all_repairs_failed"

        # Reconstruct the patch with repaired content
        new_patch = patch_content
        for file_path, (old_new, repaired_content, method) in repaired_files.items():
            new_patch = self._replace_file_content_in_patch(
                new_patch, file_path, old_new, repaired_content
            )

        repair_methods = "+".join(m for _, _, m in repaired_files.values())
        return new_patch, f"yaml_repair:{repair_methods}"

    def _extract_new_content_from_patch(self, patch_content: str, file_path: str) -> Optional[str]:
        """Extract the new file content for a specific file from a patch."""
        lines = patch_content.split("\n")
        content_lines = []
        in_target_file = False
        in_hunk = False

        for i, line in enumerate(lines):
            if line.startswith("diff --git"):
                # Check if this is our target file
                if f"b/{file_path}" in line or line.endswith(f" b/{file_path}"):
                    in_target_file = True
                    in_hunk = False
                    content_lines = []
                else:
                    # If we were in target file, we're done
                    if in_target_file and content_lines:
                        break
                    in_target_file = False
                continue

            if not in_target_file:
                continue

            if line.startswith("@@"):
                in_hunk = True
                continue

            if in_hunk:
                # Collect added lines (strip the '+' prefix)
                if line.startswith("+") and not line.startswith("+++"):
                    content_lines.append(line[1:])
                # For context lines in new file mode, also include them
                elif line.startswith(" "):
                    content_lines.append(line[1:])

        if content_lines:
            return "\n".join(content_lines)
        return None

    def _replace_file_content_in_patch(
        self, patch_content: str, file_path: str, old_content: str, new_content: str
    ) -> str:
        """Replace the content of a specific file in a patch with repaired content."""
        lines = patch_content.split("\n")
        result = []
        skip_until_next_diff = False

        i = 0
        while i < len(lines):
            line = lines[i]

            if line.startswith("diff --git"):
                # Check if this is our target file
                if f"b/{file_path}" in line or line.endswith(f" b/{file_path}"):
                    skip_until_next_diff = False
                    result.append(line)
                    i += 1
                    # Collect metadata lines until hunk
                    while (
                        i < len(lines)
                        and not lines[i].startswith("@@")
                        and not lines[i].startswith("diff --git")
                    ):
                        result.append(lines[i])
                        i += 1
                    # Now write the repaired content as a new hunk
                    if i < len(lines) and lines[i].startswith("@@"):
                        # Create proper hunk header for the repaired content
                        new_lines = new_content.split("\n")
                        result.append(f"@@ -0,0 +1,{len(new_lines)} @@")
                        for content_line in new_lines:
                            result.append(f"+{content_line}")
                        # Skip the old hunk content
                        i += 1
                        while i < len(lines) and not lines[i].startswith("diff --git"):
                            i += 1
                    continue
                else:
                    skip_until_next_diff = False

            if not skip_until_next_diff:
                result.append(line)
            i += 1

        return "\n".join(result)

    def _sanitize_patch(self, patch_content: str) -> str:
        """Sanitize patch content (delegates to patch_sanitize module)."""
        return sanitize_patch(patch_content)

    def apply_patch(
        self, patch_content: str, *, full_file_mode: bool = False
    ) -> Tuple[bool, Optional[str]]:
        """
        Apply a patch to the filesystem.

        Args:
            patch_content: The patch content to apply (git diff format)
            full_file_mode: If True, allows direct write fallback for complete file contents.
                           If False (diff mode), skips direct write fallback and fails fast.
                           Per GPT_RESPONSE15: direct write only works for full-file mode.

        Returns:
            Tuple of (success: bool, error_message: Optional[str])
            - (True, None) if patch applied successfully
            - (False, error_message) if patch failed with error details
        """
        if not patch_content or not patch_content.strip():
            logger.warning("Empty patch content provided")
            return True, None  # Empty patch is technically successful

        # BUILD-129 NDJSON mode: When the Builder uses NDJSON, operations are applied directly to disk
        # inside the LLM client. The executor still threads a synthetic diff-like header through the
        # pipeline so deliverables validation can see the touched paths:
        #
        #   # NDJSON Operations Applied (N files)
        #   diff --git a/<path> b/<path>
        #   +++ b/<path>
        #
        # This is intentionally NOT a complete git diff and must NOT be fed into git apply.
        # Treat it as "already applied", but still enforce scope/protected-path constraints by
        # validating the referenced paths.
        stripped = patch_content.lstrip()
        if stripped.startswith("# NDJSON Operations Applied"):
            try:
                files_to_modify = self._extract_files_from_patch(patch_content)
                is_valid, violations = self._validate_patch_paths(files_to_modify)
                if not is_valid:
                    # BUILD-127 Phase 2: preserve governance behavior for protected-path violations
                    protected_path_violations = [
                        v.replace("Protected path: ", "")
                        for v in violations
                        if v.startswith("Protected path:")
                    ]
                    if protected_path_violations:
                        from .governance_requests import create_protected_path_error

                        error_msg = create_protected_path_error(
                            violated_paths=protected_path_violations,
                            justification=self._extract_justification_from_patch(patch_content),
                        )
                        logger.warning(
                            f"[Governance] Protected path violation: {len(protected_path_violations)} paths"
                        )
                        return False, error_msg
                    error_msg = f"Patch rejected - violations: {', '.join(violations)}"
                    logger.error(f"[Isolation] {error_msg}")
                    return False, error_msg
                logger.info(
                    "[NDJSON] Detected synthetic NDJSON patch header; skipping git apply (already applied)"
                )
                return True, None
            except Exception as e:
                return False, f"ndjson_synthetic_patch_validation_failed: {e}"

        # BUILD-145: Track savepoint creation status for rollback
        savepoint_created = False

        try:
            # Sanitize patch to fix common LLM output issues
            patch_content = self._sanitize_patch(patch_content)
            # Remove existing files that conflict with new file patches
            patch_content = self._remove_existing_files_for_new_patches(patch_content)
            # Repair incorrect line numbers and counts in hunk headers
            patch_content = self._repair_hunk_headers(patch_content)
            # Normalize line endings to LF and ensure trailing newline
            patch_content = patch_content.replace("\r\n", "\n").replace("\r", "\n")
            if not patch_content.endswith("\n"):
                patch_content += "\n"

            # [Self-Troubleshoot] Backup files before modification
            files_to_modify = self._extract_files_from_patch(patch_content)

            # [Workspace Isolation] Check for protected path violations BEFORE applying
            is_valid, violations = self._validate_patch_paths(files_to_modify)
            if not is_valid:
                # BUILD-127 Phase 2: Return structured error for governance flow
                protected_path_violations = [
                    v.replace("Protected path: ", "")
                    for v in violations
                    if v.startswith("Protected path:")
                ]

                if protected_path_violations:
                    # Structured error for governance handling
                    from .governance_requests import create_protected_path_error

                    error_msg = create_protected_path_error(
                        violated_paths=protected_path_violations,
                        justification=self._extract_justification_from_patch(patch_content),
                    )
                    logger.warning(
                        f"[Governance] Protected path violation: {len(protected_path_violations)} paths"
                    )
                    return False, error_msg
                else:
                    # Non-protected violations (scope, etc.) - regular error
                    error_msg = f"Patch rejected - violations: {', '.join(violations)}"
                    logger.error(f"[Isolation] {error_msg}")
                    return False, error_msg

            backups = self._backup_files(files_to_modify)
            logger.debug(f"[Integrity] Backed up {len(backups)} existing files before patch")

            # BUILD-145: Create git savepoint before applying patch (if rollback enabled)
            if self.rollback_manager:
                success, error = self.rollback_manager.create_savepoint()
                if success:
                    savepoint_created = True
                else:
                    logger.warning(
                        f"[Rollback] Failed to create savepoint: {error} - proceeding without rollback"
                    )

            # Validate patch for common LLM truncation issues
            validation_errors = self._validate_patch_quality(patch_content)
            if validation_errors:
                # Check if any errors are YAML-related - attempt repair
                yaml_errors = [e for e in validation_errors if "YAML" in e or "yaml" in e]
                if yaml_errors and full_file_mode:
                    logger.info(
                        f"[YamlRepair] Detected {len(yaml_errors)} YAML validation errors, attempting repair..."
                    )
                    repaired_patch, repair_method = self._attempt_yaml_repair_in_patch(
                        patch_content, validation_errors
                    )
                    if repaired_patch:
                        logger.info(f"[YamlRepair] Repair succeeded via {repair_method}")
                        patch_content = repaired_patch
                        # Re-validate after repair
                        validation_errors = self._validate_patch_quality(patch_content)
                    # Pack schema validation for country packs (after structural validation passes)
                    if not validation_errors:
                        validation_errors.extend(self._validate_pack_schema_in_patch(patch_content))
                        if not validation_errors:
                            logger.info("[YamlRepair] Repaired patch passed validation")

                if validation_errors:
                    error_details = "\n".join(f"  - {err}" for err in validation_errors)
                    error_msg = f"Patch validation failed - LLM generated incomplete/truncated patch:\n{error_details}"
                    logger.error(error_msg)
                    logger.error(f"Patch content:\n{patch_content[:500]}...")
                    raise PatchApplyError(error_msg)

            # Write patch to a temporary file
            patch_file = self.workspace / "temp_patch.diff"
            logger.info(f"Writing patch to {patch_file}")

            with open(patch_file, "w", encoding="utf-8") as f:
                f.write(patch_content)

            # Also save a debug copy
            debug_patch_file = self.workspace / "last_patch_debug.diff"
            with open(debug_patch_file, "w", encoding="utf-8") as f:
                f.write(patch_content)

            # BUILD-045: Validate patch context matches actual file state before applying
            # This prevents git apply failures due to goal drift / context mismatch
            context_errors = self._validate_patch_context(patch_content)
            if context_errors:
                error_details = "\n".join(f"  - {err}" for err in context_errors)
                error_msg = f"Patch context validation failed - hunks don't match actual file state:\n{error_details}"
                logger.error(f"[BUILD-045] {error_msg}")
                logger.warning(
                    "[BUILD-045] This typically indicates goal drift - LLM generated patch for wrong file state"
                )
                # Don't fail immediately - let git apply attempt and provide feedback
                # This allows 3-way merge to potentially resolve minor context mismatches
                logger.info(
                    "[BUILD-045] Proceeding with git apply - 3-way merge may resolve context differences"
                )

            # Delegate to apply engine for git apply execution
            apply_result = execute_git_apply(
                patch_content=patch_content,
                workspace=Path(self.workspace),
                check_only=False,
                reverse=False,
            )

            if not apply_result.success:
                # Git apply failed - try manual apply fallback if in full_file_mode
                if not full_file_mode:
                    logger.error(
                        "Git apply failed for diff-mode patch. Direct write fallback skipped (only works for full-file mode)."
                    )
                    return (
                        False,
                        f"diff_mode_patch_failed: {apply_result.message}",
                    )

                # Try manual apply (only works for new files)
                logger.warning(
                    "Git apply failed, attempting manual apply fallback (new-file patches only)..."
                )
                manual_result = execute_manual_apply(
                    patch_content=patch_content,
                    workspace=Path(self.workspace),
                    target_files=None,
                )

                if not manual_result.success:
                    logger.error(f"Manual apply also failed: {manual_result.message}")
                    return False, f"git_apply_and_manual_apply_failed: {manual_result.message}"

                # Manual apply succeeded
                files_changed = manual_result.files_modified
                logger.info(f"Manual apply succeeded - {len(files_changed)} files written")
                for f in files_changed:
                    logger.info(f"  - {f}")

                # Validate files after manual apply
                all_valid, corrupted = self._validate_applied_files(files_changed)
                if not all_valid:
                    logger.error(
                        f"[Integrity] Manual apply corrupted {len(corrupted)} files - restoring"
                    )
                    restored, failed = self._restore_corrupted_files(corrupted, backups)
                    return (
                        False,
                        f"Manual apply corrupted {len(corrupted)} files (restored {restored})",
                    )

                # Validate content changes (symbol preservation, structural similarity)
                # Note: For new files, backups will be empty, so validation will skip them (expected)
                content_valid, problem_files = self._validate_content_changes(
                    files_changed, backups
                )
                if not content_valid:
                    logger.warning(
                        f"[Validation] Content validation issues in {len(problem_files)} files. "
                        "Patch applied but may have unintended changes."
                    )

                return True, None

            # Git apply succeeded
            files_changed = apply_result.files_modified
            logger.info(f"Patch applied successfully - {len(files_changed)} files modified")
            for file_path in files_changed:
                logger.info(f"  - {file_path}")

            # [Self-Troubleshoot] Validate files after git apply
            all_valid, corrupted = self._validate_applied_files(files_changed)
            if not all_valid:
                logger.error(f"[Integrity] Git apply corrupted {len(corrupted)} files - restoring")

                # BUILD-145: Rollback to savepoint on validation failure
                if savepoint_created and self.rollback_manager:
                    rollback_success, rollback_error = self.rollback_manager.rollback_to_savepoint(
                        f"Post-apply validation failed: {len(corrupted)} corrupted files"
                    )
                    if rollback_success:
                        logger.info("[Rollback] Successfully rolled back to pre-patch state")
                        return (
                            False,
                            f"Patch corrupted {len(corrupted)} files - rolled back to savepoint",
                        )
                    else:
                        logger.error(f"[Rollback] Rollback failed: {rollback_error}")

                # Fallback to file-level restore if rollback not enabled or failed
                restored, failed = self._restore_corrupted_files(corrupted, backups)
                return (
                    False,
                    f"Patch corrupted {len(corrupted)} files (restored {restored}, failed {failed})",
                )

            # [GPT_RESPONSE18] Validate content changes (symbol preservation, structural similarity)
            content_valid, problem_files = self._validate_content_changes(files_changed, backups)
            if not content_valid:
                logger.warning(
                    f"[Validation] Content validation issues in {len(problem_files)} files. "
                    "Patch applied but may have unintended changes."
                )
                # Note: We don't fail the patch here, just log warnings.
                # The caller can check logs for DATA_INTEGRITY warnings.
                # Per GPT_RESPONSE18: These are advisory checks in Phase 1.

            # BUILD-145: Cleanup savepoint on successful apply
            if savepoint_created and self.rollback_manager:
                self.rollback_manager.cleanup_savepoint()

            return True, None

        except Exception as e:
            error_msg = str(e)
            logger.error(f"Exception during patch application: {error_msg}")

            # BUILD-145: Rollback to savepoint on exception
            if savepoint_created and self.rollback_manager:
                rollback_success, rollback_error = self.rollback_manager.rollback_to_savepoint(
                    f"Exception during patch apply: {error_msg}"
                )
                if rollback_success:
                    logger.info(
                        "[Rollback] Successfully rolled back to pre-patch state after exception"
                    )
                else:
                    logger.error(f"[Rollback] Rollback failed: {rollback_error}")

            # Clean up temp file if it exists
            patch_file = self.workspace / "temp_patch.diff"
            if patch_file.exists():
                patch_file.unlink()
            return False, error_msg

    def _extract_files_from_patch(self, patch_content: str) -> List[str]:
        """
        Extract list of files modified from patch content.

        Args:
            patch_content: Git diff/patch content

        Returns:
            List of file paths that were modified
        """
        files = []
        for line in patch_content.split("\n"):
            # Look for diff --git a/path b/path lines
            if line.startswith("diff --git"):
                parts = line.split()
                if len(parts) >= 4:
                    # Extract file path from 'a/path/to/file'
                    file_path = parts[2][2:]  # Remove 'a/' prefix
                    files.append(file_path)
            # Also look for +++ b/path lines as backup
            elif line.startswith("+++") and not line.startswith("+++ /dev/null"):
                file_path = line[6:].strip()  # Remove '+++ b/'
                if file_path and file_path not in files:
                    files.append(file_path)

        return files

    def parse_patch_stats(self, patch_content: str) -> Tuple[List[str], int, int]:
        """
        Parse patch to extract statistics.

        Args:
            patch_content: Git diff/patch content

        Returns:
            Tuple of (files_changed, lines_added, lines_removed)
        """
        files_changed = self._extract_files_from_patch(patch_content)

        lines_added = 0
        lines_removed = 0

        for line in patch_content.split("\n"):
            # Count additions (lines starting with + but not +++)
            if line.startswith("+") and not line.startswith("+++"):
                lines_added += 1
            # Count removals (lines starting with - but not ---)
            elif line.startswith("-") and not line.startswith("---"):
                lines_removed += 1

        return files_changed, lines_added, lines_removed


# Backwards-compatible helper for call sites that import `parse_patch_stats` from this module.
# Note: `autopack.backlog_maintenance.parse_patch_stats` returns a DiffStats dataclass; many executor
# call sites want a simple tuple.
def parse_patch_stats(patch_content: str) -> Tuple[List[str], int, int]:
    """
    Parse patch to extract statistics.

    Returns:
        Tuple of (files_changed, lines_added, lines_removed)
    """
    files_changed: List[str] = []
    seen = set()
    for line in (patch_content or "").split("\n"):
        if line.startswith("diff --git"):
            parts = line.split()
            if len(parts) >= 4 and parts[2].startswith("a/"):
                p = parts[2][2:]
                if p and p not in seen:
                    seen.add(p)
                    files_changed.append(p)
        elif line.startswith("+++") and not line.startswith("+++ /dev/null"):
            # Handles both "+++ b/<path>" and (rare) malformed variants by stripping common prefix
            p = line.replace("+++ b/", "", 1).strip()
            if p and p not in seen:
                seen.add(p)
                files_changed.append(p)

    lines_added = 0
    lines_removed = 0
    for line in (patch_content or "").split("\n"):
        if line.startswith("+") and not line.startswith("+++"):
            lines_added += 1
        elif line.startswith("-") and not line.startswith("---"):
            lines_removed += 1

    return files_changed, lines_added, lines_removed
